{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Data Loaders Using Langchain"
      ],
      "metadata": {
        "id": "861IuYFY-sqx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BzxyedmC-p9Z"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from dotenv import load_dotenv, find_dotenv\n",
        "_ = load_dotenv(find_dotenv())\n",
        "openai_api_key = os.environ[\"OPENAI_API_KEY\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z6qA78Qy-p9d"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "chatModel = ChatOpenAI(model=\"gpt-3.5-turbo-0125\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2moO7a3v-p9d"
      },
      "outputs": [],
      "source": [
        "messages = [\n",
        "    (\"system\", \"You are an helpful assistant.\"),\n",
        "    (\"human\", \"tell me about genaiwithprince\"),\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bb9XQhQR-p9e"
      },
      "outputs": [],
      "source": [
        "resoponse=chatModel.invoke(messages)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iZEA_D8r-p9e",
        "outputId": "154e6816-503b-400a-8192-500516c780a3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "I'm sorry, but I couldn't find any specific information about \"genaiwithprince.\" It is possible that it may be a username or a personal identifier that is not widely known or publicly available. If you can provide more context or details, I would be happy to try to help you further.\n"
          ]
        }
      ],
      "source": [
        "print(resoponse.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yJ4wb5lp-p9g",
        "outputId": "14f9816b-1ce8-4097-d41a-4ceb3be55968"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain-community\n",
            "  Downloading langchain_community-0.3.14-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (2.0.36)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (3.11.11)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting httpx-sse<0.5.0,>=0.4.0 (from langchain-community)\n",
            "  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: langchain<0.4.0,>=0.3.14 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (0.3.14)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.29 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (0.3.29)\n",
            "Requirement already satisfied: langsmith<0.3,>=0.1.125 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (0.2.10)\n",
            "Requirement already satisfied: numpy<2,>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (1.26.4)\n",
            "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community)\n",
            "  Downloading pydantic_settings-2.7.1-py3-none-any.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (9.0.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (4.0.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (24.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.18.3)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading marshmallow-3.25.0-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.3 in /usr/local/lib/python3.10/dist-packages (from langchain<0.4.0,>=0.3.14->langchain-community) (0.3.4)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.10/dist-packages (from langchain<0.4.0,>=0.3.14->langchain-community) (2.10.4)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.29->langchain-community) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.29->langchain-community) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.29->langchain-community) (4.12.2)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.3,>=0.1.125->langchain-community) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.3,>=0.1.125->langchain-community) (3.10.13)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.3,>=0.1.125->langchain-community) (1.0.0)\n",
            "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain-community)\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (2024.12.14)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.1.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain-community) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain-community) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain-community) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.29->langchain-community) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<0.4.0,>=0.3.14->langchain-community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<0.4.0,>=0.3.14->langchain-community) (2.27.2)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain-community) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain-community) (1.2.2)\n",
            "Downloading langchain_community-0.3.14-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m29.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
            "Downloading pydantic_settings-2.7.1-py3-none-any.whl (29 kB)\n",
            "Downloading marshmallow-3.25.0-py3-none-any.whl (49 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.5/49.5 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: python-dotenv, mypy-extensions, marshmallow, httpx-sse, typing-inspect, pydantic-settings, dataclasses-json, langchain-community\n",
            "Successfully installed dataclasses-json-0.6.7 httpx-sse-0.4.0 langchain-community-0.3.14 marshmallow-3.25.0 mypy-extensions-1.0.0 pydantic-settings-2.7.1 python-dotenv-1.0.1 typing-inspect-0.9.0\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement langcahin (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for langcahin\u001b[0m\u001b[31m\n",
            "\u001b[0mCollecting langchain-huggingface\n",
            "  Downloading langchain_huggingface-0.1.2-py3-none-any.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from langchain-huggingface) (0.27.1)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.15 in /usr/local/lib/python3.10/dist-packages (from langchain-huggingface) (0.3.29)\n",
            "Requirement already satisfied: sentence-transformers>=2.6.0 in /usr/local/lib/python3.10/dist-packages (from langchain-huggingface) (3.3.1)\n",
            "Requirement already satisfied: tokenizers>=0.19.1 in /usr/local/lib/python3.10/dist-packages (from langchain-huggingface) (0.21.0)\n",
            "Requirement already satisfied: transformers>=4.39.0 in /usr/local/lib/python3.10/dist-packages (from langchain-huggingface) (4.47.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->langchain-huggingface) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->langchain-huggingface) (2024.10.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->langchain-huggingface) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->langchain-huggingface) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->langchain-huggingface) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->langchain-huggingface) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->langchain-huggingface) (4.12.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.15->langchain-huggingface) (1.33)\n",
            "Requirement already satisfied: langsmith<0.3,>=0.1.125 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.15->langchain-huggingface) (0.2.10)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.5.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.15->langchain-huggingface) (2.10.4)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.15->langchain-huggingface) (9.0.0)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=2.6.0->langchain-huggingface) (2.5.1+cu121)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=2.6.0->langchain-huggingface) (1.6.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=2.6.0->langchain-huggingface) (1.13.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=2.6.0->langchain-huggingface) (11.1.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.39.0->langchain-huggingface) (1.26.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.39.0->langchain-huggingface) (2024.11.6)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.39.0->langchain-huggingface) (0.5.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.15->langchain-huggingface) (3.0.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.3,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain-huggingface) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.3,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain-huggingface) (3.10.13)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.3,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain-huggingface) (1.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4.0,>=0.3.15->langchain-huggingface) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4.0,>=0.3.15->langchain-huggingface) (2.27.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.23.0->langchain-huggingface) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.23.0->langchain-huggingface) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.23.0->langchain-huggingface) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.23.0->langchain-huggingface) (2024.12.14)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (3.1.5)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (1.3.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers>=2.6.0->langchain-huggingface) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers>=2.6.0->langchain-huggingface) (3.5.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain-huggingface) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain-huggingface) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain-huggingface) (0.14.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (3.0.2)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain-huggingface) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain-huggingface) (1.2.2)\n",
            "Downloading langchain_huggingface-0.1.2-py3-none-any.whl (21 kB)\n",
            "Installing collected packages: langchain-huggingface\n",
            "Successfully installed langchain-huggingface-0.1.2\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (0.27.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2024.10.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2024.12.14)\n"
          ]
        }
      ],
      "source": [
        "# !pip install langchain-community==0.2.10\n",
        "\n",
        "!pip install -U langchain-community\n",
        "!pip install langcahin\n",
        "!pip install langchain-huggingface\n",
        "!pip install huggingface_hub"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2CAqDrQH-p9h"
      },
      "source": [
        "# Text File"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "QL71OgFs-p9j"
      },
      "outputs": [],
      "source": [
        "from langchain_community.document_loaders import TextLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "YvCVKfXX-p9k"
      },
      "outputs": [],
      "source": [
        "loader = TextLoader(\"/content/sample.txt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "HEKhrtK4-p9k"
      },
      "outputs": [],
      "source": [
        "loaded_data=loader.load()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "_4K2aFh4-p9l"
      },
      "outputs": [],
      "source": [
        "page_content=loaded_data[0].page_content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a8mVNOXx-p9m",
        "outputId": "154b7f69-96d3-44d7-93be-fe1f24659e0d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "India, often described as a land of Unity in Diversity, stands as a testament to the harmonious coexistence of various cultures, languages, religions, and traditions. From the snowy peaks of the Himalayas in the north to the tropical shores of Kerala in the south, and from the bustling streets of Mumbai in the west to the serene tea gardens of Assam in the east, the diversity is evident not just geographically but culturally and socially as well.The country is home to 22 officially recognized languages and over 1,600 dialects, showcasing its rich linguistic heritage. Festivals like Diwali, Eid, Christmas, and Vaisakhi are celebrated with equal fervor, reflecting the country's pluralistic ethos. Each state has its own unique cuisine, attire, dance forms, and music, yet the overarching sense of belonging to one nation binds everyone together. India's democratic framework ensures that every individual, irrespective of their background, has a voice. The Constitution itself is a symbol of this unity, laying down the fundamental rights and duties that uphold the principles of justice, liberty, equality, and fraternity. In essence, India's strength lies in its ability to embrace diversity while fostering a sense of unity and national pride. It's this intricate blend of varied identities woven together that makes India truly incredible.\n"
          ]
        }
      ],
      "source": [
        "print(page_content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TpeulDRM-p9m"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v3moVtK--p9n"
      },
      "source": [
        "# Csv file"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_dLeIptB-p9n"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "a-c6SmqF-p9n"
      },
      "outputs": [],
      "source": [
        "\n",
        "from langchain_community.document_loaders import CSVLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "hGdPD6Z4-p9n"
      },
      "outputs": [],
      "source": [
        "loader = CSVLoader('/content/Iris.csv')\n",
        "loaded_data = loader.load()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QNqUxSDe-p9n",
        "outputId": "782585b9-7eb2-4cb4-c123-44c02debe59d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(metadata={'source': '/content/Iris.csv', 'row': 0}, page_content='Id: 1\\nSepalLengthCm: 5.1\\nSepalWidthCm: 3.5\\nPetalLengthCm: 1.4\\nPetalWidthCm: 0.2\\nSpecies: Iris-setosa'),\n",
              " Document(metadata={'source': '/content/Iris.csv', 'row': 1}, page_content='Id: 2\\nSepalLengthCm: 4.9\\nSepalWidthCm: 3.0\\nPetalLengthCm: 1.4\\nPetalWidthCm: 0.2\\nSpecies: Iris-setosa'),\n",
              " Document(metadata={'source': '/content/Iris.csv', 'row': 2}, page_content='Id: 3\\nSepalLengthCm: 4.7\\nSepalWidthCm: 3.2\\nPetalLengthCm: 1.3\\nPetalWidthCm: 0.2\\nSpecies: Iris-setosa'),\n",
              " Document(metadata={'source': '/content/Iris.csv', 'row': 3}, page_content='Id: 4\\nSepalLengthCm: 4.6\\nSepalWidthCm: 3.1\\nPetalLengthCm: 1.5\\nPetalWidthCm: 0.2\\nSpecies: Iris-setosa'),\n",
              " Document(metadata={'source': '/content/Iris.csv', 'row': 4}, page_content='Id: 5\\nSepalLengthCm: 5.0\\nSepalWidthCm: 3.6\\nPetalLengthCm: 1.4\\nPetalWidthCm: 0.2\\nSpecies: Iris-setosa'),\n",
              " Document(metadata={'source': '/content/Iris.csv', 'row': 5}, page_content='Id: 6\\nSepalLengthCm: 5.4\\nSepalWidthCm: 3.9\\nPetalLengthCm: 1.7\\nPetalWidthCm: 0.4\\nSpecies: Iris-setosa'),\n",
              " Document(metadata={'source': '/content/Iris.csv', 'row': 6}, page_content='Id: 7\\nSepalLengthCm: 4.6\\nSepalWidthCm: 3.4\\nPetalLengthCm: 1.4\\nPetalWidthCm: 0.3\\nSpecies: Iris-setosa'),\n",
              " Document(metadata={'source': '/content/Iris.csv', 'row': 7}, page_content='Id: 8\\nSepalLengthCm: 5.0\\nSepalWidthCm: 3.4\\nPetalLengthCm: 1.5\\nPetalWidthCm: 0.2\\nSpecies: Iris-setosa'),\n",
              " Document(metadata={'source': '/content/Iris.csv', 'row': 8}, page_content='Id: 9\\nSepalLengthCm: 4.4\\nSepalWidthCm: 2.9\\nPetalLengthCm: 1.4\\nPetalWidthCm: 0.2\\nSpecies: Iris-setosa'),\n",
              " Document(metadata={'source': '/content/Iris.csv', 'row': 9}, page_content='Id: 10\\nSepalLengthCm: 4.9\\nSepalWidthCm: 3.1\\nPetalLengthCm: 1.5\\nPetalWidthCm: 0.1\\nSpecies: Iris-setosa'),\n",
              " Document(metadata={'source': '/content/Iris.csv', 'row': 10}, page_content='Id: 11\\nSepalLengthCm: 5.4\\nSepalWidthCm: 3.7\\nPetalLengthCm: 1.5\\nPetalWidthCm: 0.2\\nSpecies: Iris-setosa'),\n",
              " Document(metadata={'source': '/content/Iris.csv', 'row': 11}, page_content='Id: 12\\nSepalLengthCm: 4.8\\nSepalWidthCm: 3.4\\nPetalLengthCm: 1.6\\nPetalWidthCm: 0.2\\nSpecies: Iris-setosa'),\n",
              " Document(metadata={'source': '/content/Iris.csv', 'row': 12}, page_content='Id: 13\\nSepalLengthCm: 4.8\\nSepalWidthCm: 3.0\\nPetalLengthCm: 1.4\\nPetalWidthCm: 0.1\\nSpecies: Iris-setosa'),\n",
              " Document(metadata={'source': '/content/Iris.csv', 'row': 13}, page_content='Id: 14\\nSepalLengthCm: 4.3\\nSepalWidthCm: 3.0\\nPetalLengthCm: 1.1\\nPetalWidthCm: 0.1\\nSpecies: Iris-setosa'),\n",
              " Document(metadata={'source': '/content/Iris.csv', 'row': 14}, page_content='Id: 15\\nSepalLengthCm: 5.8\\nSepalWidthCm: 4.0\\nPetalLengthCm: 1.2\\nPetalWidthCm: 0.2\\nSpecies: Iris-setosa'),\n",
              " Document(metadata={'source': '/content/Iris.csv', 'row': 15}, page_content='Id: 16\\nSepalLengthCm: 5.7\\nSepalWidthCm: 4.4\\nPetalLengthCm: 1.5\\nPetalWidthCm: 0.4\\nSpecies: Iris-setosa'),\n",
              " Document(metadata={'source': '/content/Iris.csv', 'row': 16}, page_content='Id: 17\\nSepalLengthCm: 5.4\\nSepalWidthCm: 3.9\\nPetalLengthCm: 1.3\\nPetalWidthCm: 0.4\\nSpecies: Iris-setosa'),\n",
              " Document(metadata={'source': '/content/Iris.csv', 'row': 17}, page_content='Id: 18\\nSepalLengthCm: 5.1\\nSepalWidthCm: 3.5\\nPetalLengthCm: 1.4\\nPetalWidthCm: 0.3\\nSpecies: Iris-setosa'),\n",
              " Document(metadata={'source': '/content/Iris.csv', 'row': 18}, page_content='Id: 19\\nSepalLengthCm: 5.7\\nSepalWidthCm: 3.8\\nPetalLengthCm: 1.7\\nPetalWidthCm: 0.3\\nSpecies: Iris-setosa'),\n",
              " Document(metadata={'source': '/content/Iris.csv', 'row': 19}, page_content='Id: 20\\nSepalLengthCm: 5.1\\nSepalWidthCm: 3.8\\nPetalLengthCm: 1.5\\nPetalWidthCm: 0.3\\nSpecies: Iris-setosa'),\n",
              " Document(metadata={'source': '/content/Iris.csv', 'row': 20}, page_content='Id: 21\\nSepalLengthCm: 5.4\\nSepalWidthCm: 3.4\\nPetalLengthCm: 1.7\\nPetalWidthCm: 0.2\\nSpecies: Iris-setosa'),\n",
              " Document(metadata={'source': '/content/Iris.csv', 'row': 21}, page_content='Id: 22\\nSepalLengthCm: 5.1\\nSepalWidthCm: 3.7\\nPetalLengthCm: 1.5\\nPetalWidthCm: 0.4\\nSpecies: Iris-setosa'),\n",
              " Document(metadata={'source': '/content/Iris.csv', 'row': 22}, page_content='Id: 23\\nSepalLengthCm: 4.6\\nSepalWidthCm: 3.6\\nPetalLengthCm: 1.0\\nPetalWidthCm: 0.2\\nSpecies: Iris-setosa'),\n",
              " Document(metadata={'source': '/content/Iris.csv', 'row': 23}, page_content='Id: 24\\nSepalLengthCm: 5.1\\nSepalWidthCm: 3.3\\nPetalLengthCm: 1.7\\nPetalWidthCm: 0.5\\nSpecies: Iris-setosa'),\n",
              " Document(metadata={'source': '/content/Iris.csv', 'row': 24}, page_content='Id: 25\\nSepalLengthCm: 4.8\\nSepalWidthCm: 3.4\\nPetalLengthCm: 1.9\\nPetalWidthCm: 0.2\\nSpecies: Iris-setosa'),\n",
              " Document(metadata={'source': '/content/Iris.csv', 'row': 25}, page_content='Id: 26\\nSepalLengthCm: 5.0\\nSepalWidthCm: 3.0\\nPetalLengthCm: 1.6\\nPetalWidthCm: 0.2\\nSpecies: Iris-setosa'),\n",
              " Document(metadata={'source': '/content/Iris.csv', 'row': 26}, page_content='Id: 27\\nSepalLengthCm: 5.0\\nSepalWidthCm: 3.4\\nPetalLengthCm: 1.6\\nPetalWidthCm: 0.4\\nSpecies: Iris-setosa'),\n",
              " Document(metadata={'source': '/content/Iris.csv', 'row': 27}, page_content='Id: 28\\nSepalLengthCm: 5.2\\nSepalWidthCm: 3.5\\nPetalLengthCm: 1.5\\nPetalWidthCm: 0.2\\nSpecies: Iris-setosa'),\n",
              " Document(metadata={'source': '/content/Iris.csv', 'row': 28}, page_content='Id: 29\\nSepalLengthCm: 5.2\\nSepalWidthCm: 3.4\\nPetalLengthCm: 1.4\\nPetalWidthCm: 0.2\\nSpecies: Iris-setosa'),\n",
              " Document(metadata={'source': '/content/Iris.csv', 'row': 29}, page_content='Id: 30\\nSepalLengthCm: 4.7\\nSepalWidthCm: 3.2\\nPetalLengthCm: 1.6\\nPetalWidthCm: 0.2\\nSpecies: Iris-setosa'),\n",
              " Document(metadata={'source': '/content/Iris.csv', 'row': 30}, page_content='Id: 31\\nSepalLengthCm: 4.8\\nSepalWidthCm: 3.1\\nPetalLengthCm: 1.6\\nPetalWidthCm: 0.2\\nSpecies: Iris-setosa'),\n",
              " Document(metadata={'source': '/content/Iris.csv', 'row': 31}, page_content='Id: 32\\nSepalLengthCm: 5.4\\nSepalWidthCm: 3.4\\nPetalLengthCm: 1.5\\nPetalWidthCm: 0.4\\nSpecies: Iris-setosa'),\n",
              " Document(metadata={'source': '/content/Iris.csv', 'row': 32}, page_content='Id: 33\\nSepalLengthCm: 5.2\\nSepalWidthCm: 4.1\\nPetalLengthCm: 1.5\\nPetalWidthCm: 0.1\\nSpecies: Iris-setosa'),\n",
              " Document(metadata={'source': '/content/Iris.csv', 'row': 33}, page_content='Id: 34\\nSepalLengthCm: 5.5\\nSepalWidthCm: 4.2\\nPetalLengthCm: 1.4\\nPetalWidthCm: 0.2\\nSpecies: Iris-setosa'),\n",
              " Document(metadata={'source': '/content/Iris.csv', 'row': 34}, page_content='Id: 35\\nSepalLengthCm: 4.9\\nSepalWidthCm: 3.1\\nPetalLengthCm: 1.5\\nPetalWidthCm: 0.1\\nSpecies: Iris-setosa'),\n",
              " Document(metadata={'source': '/content/Iris.csv', 'row': 35}, page_content='Id: 36\\nSepalLengthCm: 5.0\\nSepalWidthCm: 3.2\\nPetalLengthCm: 1.2\\nPetalWidthCm: 0.2\\nSpecies: Iris-setosa'),\n",
              " Document(metadata={'source': '/content/Iris.csv', 'row': 36}, page_content='Id: 37\\nSepalLengthCm: 5.5\\nSepalWidthCm: 3.5\\nPetalLengthCm: 1.3\\nPetalWidthCm: 0.2\\nSpecies: Iris-setosa'),\n",
              " Document(metadata={'source': '/content/Iris.csv', 'row': 37}, page_content='Id: 38\\nSepalLengthCm: 4.9\\nSepalWidthCm: 3.1\\nPetalLengthCm: 1.5\\nPetalWidthCm: 0.1\\nSpecies: Iris-setosa'),\n",
              " Document(metadata={'source': '/content/Iris.csv', 'row': 38}, page_content='Id: 39\\nSepalLengthCm: 4.4\\nSepalWidthCm: 3.0\\nPetalLengthCm: 1.3\\nPetalWidthCm: 0.2\\nSpecies: Iris-setosa'),\n",
              " Document(metadata={'source': '/content/Iris.csv', 'row': 39}, page_content='Id: 40\\nSepalLengthCm: 5.1\\nSepalWidthCm: 3.4\\nPetalLengthCm: 1.5\\nPetalWidthCm: 0.2\\nSpecies: Iris-setosa'),\n",
              " Document(metadata={'source': '/content/Iris.csv', 'row': 40}, page_content='Id: 41\\nSepalLengthCm: 5.0\\nSepalWidthCm: 3.5\\nPetalLengthCm: 1.3\\nPetalWidthCm: 0.3\\nSpecies: Iris-setosa'),\n",
              " Document(metadata={'source': '/content/Iris.csv', 'row': 41}, page_content='Id: 42\\nSepalLengthCm: 4.5\\nSepalWidthCm: 2.3\\nPetalLengthCm: 1.3\\nPetalWidthCm: 0.3\\nSpecies: Iris-setosa'),\n",
              " Document(metadata={'source': '/content/Iris.csv', 'row': 42}, page_content='Id: 43\\nSepalLengthCm: 4.4\\nSepalWidthCm: 3.2\\nPetalLengthCm: 1.3\\nPetalWidthCm: 0.2\\nSpecies: Iris-setosa'),\n",
              " Document(metadata={'source': '/content/Iris.csv', 'row': 43}, page_content='Id: 44\\nSepalLengthCm: 5.0\\nSepalWidthCm: 3.5\\nPetalLengthCm: 1.6\\nPetalWidthCm: 0.6\\nSpecies: Iris-setosa'),\n",
              " Document(metadata={'source': '/content/Iris.csv', 'row': 44}, page_content='Id: 45\\nSepalLengthCm: 5.1\\nSepalWidthCm: 3.8\\nPetalLengthCm: 1.9\\nPetalWidthCm: 0.4\\nSpecies: Iris-setosa'),\n",
              " Document(metadata={'source': '/content/Iris.csv', 'row': 45}, page_content='Id: 46\\nSepalLengthCm: 4.8\\nSepalWidthCm: 3.0\\nPetalLengthCm: 1.4\\nPetalWidthCm: 0.3\\nSpecies: Iris-setosa'),\n",
              " Document(metadata={'source': '/content/Iris.csv', 'row': 46}, page_content='Id: 47\\nSepalLengthCm: 5.1\\nSepalWidthCm: 3.8\\nPetalLengthCm: 1.6\\nPetalWidthCm: 0.2\\nSpecies: Iris-setosa'),\n",
              " Document(metadata={'source': '/content/Iris.csv', 'row': 47}, page_content='Id: 48\\nSepalLengthCm: 4.6\\nSepalWidthCm: 3.2\\nPetalLengthCm: 1.4\\nPetalWidthCm: 0.2\\nSpecies: Iris-setosa'),\n",
              " Document(metadata={'source': '/content/Iris.csv', 'row': 48}, page_content='Id: 49\\nSepalLengthCm: 5.3\\nSepalWidthCm: 3.7\\nPetalLengthCm: 1.5\\nPetalWidthCm: 0.2\\nSpecies: Iris-setosa'),\n",
              " Document(metadata={'source': '/content/Iris.csv', 'row': 49}, page_content='Id: 50\\nSepalLengthCm: 5.0\\nSepalWidthCm: 3.3\\nPetalLengthCm: 1.4\\nPetalWidthCm: 0.2\\nSpecies: Iris-setosa'),\n",
              " Document(metadata={'source': '/content/Iris.csv', 'row': 50}, page_content='Id: 51\\nSepalLengthCm: 7.0\\nSepalWidthCm: 3.2\\nPetalLengthCm: 4.7\\nPetalWidthCm: 1.4\\nSpecies: Iris-versicolor'),\n",
              " Document(metadata={'source': '/content/Iris.csv', 'row': 51}, page_content='Id: 52\\nSepalLengthCm: 6.4\\nSepalWidthCm: 3.2\\nPetalLengthCm: 4.5\\nPetalWidthCm: 1.5\\nSpecies: Iris-versicolor'),\n",
              " Document(metadata={'source': '/content/Iris.csv', 'row': 52}, page_content='Id: 53\\nSepalLengthCm: 6.9\\nSepalWidthCm: 3.1\\nPetalLengthCm: 4.9\\nPetalWidthCm: 1.5\\nSpecies: Iris-versicolor'),\n",
              " Document(metadata={'source': '/content/Iris.csv', 'row': 53}, page_content='Id: 54\\nSepalLengthCm: 5.5\\nSepalWidthCm: 2.3\\nPetalLengthCm: 4.0\\nPetalWidthCm: 1.3\\nSpecies: Iris-versicolor'),\n",
              " Document(metadata={'source': '/content/Iris.csv', 'row': 54}, page_content='Id: 55\\nSepalLengthCm: 6.5\\nSepalWidthCm: 2.8\\nPetalLengthCm: 4.6\\nPetalWidthCm: 1.5\\nSpecies: Iris-versicolor'),\n",
              " Document(metadata={'source': '/content/Iris.csv', 'row': 55}, page_content='Id: 56\\nSepalLengthCm: 5.7\\nSepalWidthCm: 2.8\\nPetalLengthCm: 4.5\\nPetalWidthCm: 1.3\\nSpecies: Iris-versicolor'),\n",
              " Document(metadata={'source': '/content/Iris.csv', 'row': 56}, page_content='Id: 57\\nSepalLengthCm: 6.3\\nSepalWidthCm: 3.3\\nPetalLengthCm: 4.7\\nPetalWidthCm: 1.6\\nSpecies: Iris-versicolor'),\n",
              " Document(metadata={'source': '/content/Iris.csv', 'row': 57}, page_content='Id: 58\\nSepalLengthCm: 4.9\\nSepalWidthCm: 2.4\\nPetalLengthCm: 3.3\\nPetalWidthCm: 1.0\\nSpecies: Iris-versicolor'),\n",
              " Document(metadata={'source': '/content/Iris.csv', 'row': 58}, page_content='Id: 59\\nSepalLengthCm: 6.6\\nSepalWidthCm: 2.9\\nPetalLengthCm: 4.6\\nPetalWidthCm: 1.3\\nSpecies: Iris-versicolor'),\n",
              " Document(metadata={'source': '/content/Iris.csv', 'row': 59}, page_content='Id: 60\\nSepalLengthCm: 5.2\\nSepalWidthCm: 2.7\\nPetalLengthCm: 3.9\\nPetalWidthCm: 1.4\\nSpecies: Iris-versicolor'),\n",
              " Document(metadata={'source': '/content/Iris.csv', 'row': 60}, page_content='Id: 61\\nSepalLengthCm: 5.0\\nSepalWidthCm: 2.0\\nPetalLengthCm: 3.5\\nPetalWidthCm: 1.0\\nSpecies: Iris-versicolor'),\n",
              " Document(metadata={'source': '/content/Iris.csv', 'row': 61}, page_content='Id: 62\\nSepalLengthCm: 5.9\\nSepalWidthCm: 3.0\\nPetalLengthCm: 4.2\\nPetalWidthCm: 1.5\\nSpecies: Iris-versicolor'),\n",
              " Document(metadata={'source': '/content/Iris.csv', 'row': 62}, page_content='Id: 63\\nSepalLengthCm: 6.0\\nSepalWidthCm: 2.2\\nPetalLengthCm: 4.0\\nPetalWidthCm: 1.0\\nSpecies: Iris-versicolor'),\n",
              " Document(metadata={'source': '/content/Iris.csv', 'row': 63}, page_content='Id: 64\\nSepalLengthCm: 6.1\\nSepalWidthCm: 2.9\\nPetalLengthCm: 4.7\\nPetalWidthCm: 1.4\\nSpecies: Iris-versicolor'),\n",
              " Document(metadata={'source': '/content/Iris.csv', 'row': 64}, page_content='Id: 65\\nSepalLengthCm: 5.6\\nSepalWidthCm: 2.9\\nPetalLengthCm: 3.6\\nPetalWidthCm: 1.3\\nSpecies: Iris-versicolor'),\n",
              " Document(metadata={'source': '/content/Iris.csv', 'row': 65}, page_content='Id: 66\\nSepalLengthCm: 6.7\\nSepalWidthCm: 3.1\\nPetalLengthCm: 4.4\\nPetalWidthCm: 1.4\\nSpecies: Iris-versicolor'),\n",
              " Document(metadata={'source': '/content/Iris.csv', 'row': 66}, page_content='Id: 67\\nSepalLengthCm: 5.6\\nSepalWidthCm: 3.0\\nPetalLengthCm: 4.5\\nPetalWidthCm: 1.5\\nSpecies: Iris-versicolor'),\n",
              " Document(metadata={'source': '/content/Iris.csv', 'row': 67}, page_content='Id: 68\\nSepalLengthCm: 5.8\\nSepalWidthCm: 2.7\\nPetalLengthCm: 4.1\\nPetalWidthCm: 1.0\\nSpecies: Iris-versicolor'),\n",
              " Document(metadata={'source': '/content/Iris.csv', 'row': 68}, page_content='Id: 69\\nSepalLengthCm: 6.2\\nSepalWidthCm: 2.2\\nPetalLengthCm: 4.5\\nPetalWidthCm: 1.5\\nSpecies: Iris-versicolor'),\n",
              " Document(metadata={'source': '/content/Iris.csv', 'row': 69}, page_content='Id: 70\\nSepalLengthCm: 5.6\\nSepalWidthCm: 2.5\\nPetalLengthCm: 3.9\\nPetalWidthCm: 1.1\\nSpecies: Iris-versicolor'),\n",
              " Document(metadata={'source': '/content/Iris.csv', 'row': 70}, page_content='Id: 71\\nSepalLengthCm: 5.9\\nSepalWidthCm: 3.2\\nPetalLengthCm: 4.8\\nPetalWidthCm: 1.8\\nSpecies: Iris-versicolor'),\n",
              " Document(metadata={'source': '/content/Iris.csv', 'row': 71}, page_content='Id: 72\\nSepalLengthCm: 6.1\\nSepalWidthCm: 2.8\\nPetalLengthCm: 4.0\\nPetalWidthCm: 1.3\\nSpecies: Iris-versicolor'),\n",
              " Document(metadata={'source': '/content/Iris.csv', 'row': 72}, page_content='Id: 73\\nSepalLengthCm: 6.3\\nSepalWidthCm: 2.5\\nPetalLengthCm: 4.9\\nPetalWidthCm: 1.5\\nSpecies: Iris-versicolor'),\n",
              " Document(metadata={'source': '/content/Iris.csv', 'row': 73}, page_content='Id: 74\\nSepalLengthCm: 6.1\\nSepalWidthCm: 2.8\\nPetalLengthCm: 4.7\\nPetalWidthCm: 1.2\\nSpecies: Iris-versicolor'),\n",
              " Document(metadata={'source': '/content/Iris.csv', 'row': 74}, page_content='Id: 75\\nSepalLengthCm: 6.4\\nSepalWidthCm: 2.9\\nPetalLengthCm: 4.3\\nPetalWidthCm: 1.3\\nSpecies: Iris-versicolor'),\n",
              " Document(metadata={'source': '/content/Iris.csv', 'row': 75}, page_content='Id: 76\\nSepalLengthCm: 6.6\\nSepalWidthCm: 3.0\\nPetalLengthCm: 4.4\\nPetalWidthCm: 1.4\\nSpecies: Iris-versicolor'),\n",
              " Document(metadata={'source': '/content/Iris.csv', 'row': 76}, page_content='Id: 77\\nSepalLengthCm: 6.8\\nSepalWidthCm: 2.8\\nPetalLengthCm: 4.8\\nPetalWidthCm: 1.4\\nSpecies: Iris-versicolor'),\n",
              " Document(metadata={'source': '/content/Iris.csv', 'row': 77}, page_content='Id: 78\\nSepalLengthCm: 6.7\\nSepalWidthCm: 3.0\\nPetalLengthCm: 5.0\\nPetalWidthCm: 1.7\\nSpecies: Iris-versicolor'),\n",
              " Document(metadata={'source': '/content/Iris.csv', 'row': 78}, page_content='Id: 79\\nSepalLengthCm: 6.0\\nSepalWidthCm: 2.9\\nPetalLengthCm: 4.5\\nPetalWidthCm: 1.5\\nSpecies: Iris-versicolor'),\n",
              " Document(metadata={'source': '/content/Iris.csv', 'row': 79}, page_content='Id: 80\\nSepalLengthCm: 5.7\\nSepalWidthCm: 2.6\\nPetalLengthCm: 3.5\\nPetalWidthCm: 1.0\\nSpecies: Iris-versicolor'),\n",
              " Document(metadata={'source': '/content/Iris.csv', 'row': 80}, page_content='Id: 81\\nSepalLengthCm: 5.5\\nSepalWidthCm: 2.4\\nPetalLengthCm: 3.8\\nPetalWidthCm: 1.1\\nSpecies: Iris-versicolor'),\n",
              " Document(metadata={'source': '/content/Iris.csv', 'row': 81}, page_content='Id: 82\\nSepalLengthCm: 5.5\\nSepalWidthCm: 2.4\\nPetalLengthCm: 3.7\\nPetalWidthCm: 1.0\\nSpecies: Iris-versicolor'),\n",
              " Document(metadata={'source': '/content/Iris.csv', 'row': 82}, page_content='Id: 83\\nSepalLengthCm: 5.8\\nSepalWidthCm: 2.7\\nPetalLengthCm: 3.9\\nPetalWidthCm: 1.2\\nSpecies: Iris-versicolor'),\n",
              " Document(metadata={'source': '/content/Iris.csv', 'row': 83}, page_content='Id: 84\\nSepalLengthCm: 6.0\\nSepalWidthCm: 2.7\\nPetalLengthCm: 5.1\\nPetalWidthCm: 1.6\\nSpecies: Iris-versicolor'),\n",
              " Document(metadata={'source': '/content/Iris.csv', 'row': 84}, page_content='Id: 85\\nSepalLengthCm: 5.4\\nSepalWidthCm: 3.0\\nPetalLengthCm: 4.5\\nPetalWidthCm: 1.5\\nSpecies: Iris-versicolor'),\n",
              " Document(metadata={'source': '/content/Iris.csv', 'row': 85}, page_content='Id: 86\\nSepalLengthCm: 6.0\\nSepalWidthCm: 3.4\\nPetalLengthCm: 4.5\\nPetalWidthCm: 1.6\\nSpecies: Iris-versicolor'),\n",
              " Document(metadata={'source': '/content/Iris.csv', 'row': 86}, page_content='Id: 87\\nSepalLengthCm: 6.7\\nSepalWidthCm: 3.1\\nPetalLengthCm: 4.7\\nPetalWidthCm: 1.5\\nSpecies: Iris-versicolor'),\n",
              " Document(metadata={'source': '/content/Iris.csv', 'row': 87}, page_content='Id: 88\\nSepalLengthCm: 6.3\\nSepalWidthCm: 2.3\\nPetalLengthCm: 4.4\\nPetalWidthCm: 1.3\\nSpecies: Iris-versicolor'),\n",
              " Document(metadata={'source': '/content/Iris.csv', 'row': 88}, page_content='Id: 89\\nSepalLengthCm: 5.6\\nSepalWidthCm: 3.0\\nPetalLengthCm: 4.1\\nPetalWidthCm: 1.3\\nSpecies: Iris-versicolor'),\n",
              " Document(metadata={'source': '/content/Iris.csv', 'row': 89}, page_content='Id: 90\\nSepalLengthCm: 5.5\\nSepalWidthCm: 2.5\\nPetalLengthCm: 4.0\\nPetalWidthCm: 1.3\\nSpecies: Iris-versicolor'),\n",
              " Document(metadata={'source': '/content/Iris.csv', 'row': 90}, page_content='Id: 91\\nSepalLengthCm: 5.5\\nSepalWidthCm: 2.6\\nPetalLengthCm: 4.4\\nPetalWidthCm: 1.2\\nSpecies: Iris-versicolor'),\n",
              " Document(metadata={'source': '/content/Iris.csv', 'row': 91}, page_content='Id: 92\\nSepalLengthCm: 6.1\\nSepalWidthCm: 3.0\\nPetalLengthCm: 4.6\\nPetalWidthCm: 1.4\\nSpecies: Iris-versicolor'),\n",
              " Document(metadata={'source': '/content/Iris.csv', 'row': 92}, page_content='Id: 93\\nSepalLengthCm: 5.8\\nSepalWidthCm: 2.6\\nPetalLengthCm: 4.0\\nPetalWidthCm: 1.2\\nSpecies: Iris-versicolor'),\n",
              " Document(metadata={'source': '/content/Iris.csv', 'row': 93}, page_content='Id: 94\\nSepalLengthCm: 5.0\\nSepalWidthCm: 2.3\\nPetalLengthCm: 3.3\\nPetalWidthCm: 1.0\\nSpecies: Iris-versicolor'),\n",
              " Document(metadata={'source': '/content/Iris.csv', 'row': 94}, page_content='Id: 95\\nSepalLengthCm: 5.6\\nSepalWidthCm: 2.7\\nPetalLengthCm: 4.2\\nPetalWidthCm: 1.3\\nSpecies: Iris-versicolor'),\n",
              " Document(metadata={'source': '/content/Iris.csv', 'row': 95}, page_content='Id: 96\\nSepalLengthCm: 5.7\\nSepalWidthCm: 3.0\\nPetalLengthCm: 4.2\\nPetalWidthCm: 1.2\\nSpecies: Iris-versicolor'),\n",
              " Document(metadata={'source': '/content/Iris.csv', 'row': 96}, page_content='Id: 97\\nSepalLengthCm: 5.7\\nSepalWidthCm: 2.9\\nPetalLengthCm: 4.2\\nPetalWidthCm: 1.3\\nSpecies: Iris-versicolor'),\n",
              " Document(metadata={'source': '/content/Iris.csv', 'row': 97}, page_content='Id: 98\\nSepalLengthCm: 6.2\\nSepalWidthCm: 2.9\\nPetalLengthCm: 4.3\\nPetalWidthCm: 1.3\\nSpecies: Iris-versicolor'),\n",
              " Document(metadata={'source': '/content/Iris.csv', 'row': 98}, page_content='Id: 99\\nSepalLengthCm: 5.1\\nSepalWidthCm: 2.5\\nPetalLengthCm: 3.0\\nPetalWidthCm: 1.1\\nSpecies: Iris-versicolor'),\n",
              " Document(metadata={'source': '/content/Iris.csv', 'row': 99}, page_content='Id: 100\\nSepalLengthCm: 5.7\\nSepalWidthCm: 2.8\\nPetalLengthCm: 4.1\\nPetalWidthCm: 1.3\\nSpecies: Iris-versicolor'),\n",
              " Document(metadata={'source': '/content/Iris.csv', 'row': 100}, page_content='Id: 101\\nSepalLengthCm: 6.3\\nSepalWidthCm: 3.3\\nPetalLengthCm: 6.0\\nPetalWidthCm: 2.5\\nSpecies: Iris-virginica'),\n",
              " Document(metadata={'source': '/content/Iris.csv', 'row': 101}, page_content='Id: 102\\nSepalLengthCm: 5.8\\nSepalWidthCm: 2.7\\nPetalLengthCm: 5.1\\nPetalWidthCm: 1.9\\nSpecies: Iris-virginica'),\n",
              " Document(metadata={'source': '/content/Iris.csv', 'row': 102}, page_content='Id: 103\\nSepalLengthCm: 7.1\\nSepalWidthCm: 3.0\\nPetalLengthCm: 5.9\\nPetalWidthCm: 2.1\\nSpecies: Iris-virginica'),\n",
              " Document(metadata={'source': '/content/Iris.csv', 'row': 103}, page_content='Id: 104\\nSepalLengthCm: 6.3\\nSepalWidthCm: 2.9\\nPetalLengthCm: 5.6\\nPetalWidthCm: 1.8\\nSpecies: Iris-virginica'),\n",
              " Document(metadata={'source': '/content/Iris.csv', 'row': 104}, page_content='Id: 105\\nSepalLengthCm: 6.5\\nSepalWidthCm: 3.0\\nPetalLengthCm: 5.8\\nPetalWidthCm: 2.2\\nSpecies: Iris-virginica'),\n",
              " Document(metadata={'source': '/content/Iris.csv', 'row': 105}, page_content='Id: 106\\nSepalLengthCm: 7.6\\nSepalWidthCm: 3.0\\nPetalLengthCm: 6.6\\nPetalWidthCm: 2.1\\nSpecies: Iris-virginica'),\n",
              " Document(metadata={'source': '/content/Iris.csv', 'row': 106}, page_content='Id: 107\\nSepalLengthCm: 4.9\\nSepalWidthCm: 2.5\\nPetalLengthCm: 4.5\\nPetalWidthCm: 1.7\\nSpecies: Iris-virginica'),\n",
              " Document(metadata={'source': '/content/Iris.csv', 'row': 107}, page_content='Id: 108\\nSepalLengthCm: 7.3\\nSepalWidthCm: 2.9\\nPetalLengthCm: 6.3\\nPetalWidthCm: 1.8\\nSpecies: Iris-virginica'),\n",
              " Document(metadata={'source': '/content/Iris.csv', 'row': 108}, page_content='Id: 109\\nSepalLengthCm: 6.7\\nSepalWidthCm: 2.5\\nPetalLengthCm: 5.8\\nPetalWidthCm: 1.8\\nSpecies: Iris-virginica'),\n",
              " Document(metadata={'source': '/content/Iris.csv', 'row': 109}, page_content='Id: 110\\nSepalLengthCm: 7.2\\nSepalWidthCm: 3.6\\nPetalLengthCm: 6.1\\nPetalWidthCm: 2.5\\nSpecies: Iris-virginica'),\n",
              " Document(metadata={'source': '/content/Iris.csv', 'row': 110}, page_content='Id: 111\\nSepalLengthCm: 6.5\\nSepalWidthCm: 3.2\\nPetalLengthCm: 5.1\\nPetalWidthCm: 2.0\\nSpecies: Iris-virginica'),\n",
              " Document(metadata={'source': '/content/Iris.csv', 'row': 111}, page_content='Id: 112\\nSepalLengthCm: 6.4\\nSepalWidthCm: 2.7\\nPetalLengthCm: 5.3\\nPetalWidthCm: 1.9\\nSpecies: Iris-virginica'),\n",
              " Document(metadata={'source': '/content/Iris.csv', 'row': 112}, page_content='Id: 113\\nSepalLengthCm: 6.8\\nSepalWidthCm: 3.0\\nPetalLengthCm: 5.5\\nPetalWidthCm: 2.1\\nSpecies: Iris-virginica'),\n",
              " Document(metadata={'source': '/content/Iris.csv', 'row': 113}, page_content='Id: 114\\nSepalLengthCm: 5.7\\nSepalWidthCm: 2.5\\nPetalLengthCm: 5.0\\nPetalWidthCm: 2.0\\nSpecies: Iris-virginica'),\n",
              " Document(metadata={'source': '/content/Iris.csv', 'row': 114}, page_content='Id: 115\\nSepalLengthCm: 5.8\\nSepalWidthCm: 2.8\\nPetalLengthCm: 5.1\\nPetalWidthCm: 2.4\\nSpecies: Iris-virginica'),\n",
              " Document(metadata={'source': '/content/Iris.csv', 'row': 115}, page_content='Id: 116\\nSepalLengthCm: 6.4\\nSepalWidthCm: 3.2\\nPetalLengthCm: 5.3\\nPetalWidthCm: 2.3\\nSpecies: Iris-virginica'),\n",
              " Document(metadata={'source': '/content/Iris.csv', 'row': 116}, page_content='Id: 117\\nSepalLengthCm: 6.5\\nSepalWidthCm: 3.0\\nPetalLengthCm: 5.5\\nPetalWidthCm: 1.8\\nSpecies: Iris-virginica'),\n",
              " Document(metadata={'source': '/content/Iris.csv', 'row': 117}, page_content='Id: 118\\nSepalLengthCm: 7.7\\nSepalWidthCm: 3.8\\nPetalLengthCm: 6.7\\nPetalWidthCm: 2.2\\nSpecies: Iris-virginica'),\n",
              " Document(metadata={'source': '/content/Iris.csv', 'row': 118}, page_content='Id: 119\\nSepalLengthCm: 7.7\\nSepalWidthCm: 2.6\\nPetalLengthCm: 6.9\\nPetalWidthCm: 2.3\\nSpecies: Iris-virginica'),\n",
              " Document(metadata={'source': '/content/Iris.csv', 'row': 119}, page_content='Id: 120\\nSepalLengthCm: 6.0\\nSepalWidthCm: 2.2\\nPetalLengthCm: 5.0\\nPetalWidthCm: 1.5\\nSpecies: Iris-virginica'),\n",
              " Document(metadata={'source': '/content/Iris.csv', 'row': 120}, page_content='Id: 121\\nSepalLengthCm: 6.9\\nSepalWidthCm: 3.2\\nPetalLengthCm: 5.7\\nPetalWidthCm: 2.3\\nSpecies: Iris-virginica'),\n",
              " Document(metadata={'source': '/content/Iris.csv', 'row': 121}, page_content='Id: 122\\nSepalLengthCm: 5.6\\nSepalWidthCm: 2.8\\nPetalLengthCm: 4.9\\nPetalWidthCm: 2.0\\nSpecies: Iris-virginica'),\n",
              " Document(metadata={'source': '/content/Iris.csv', 'row': 122}, page_content='Id: 123\\nSepalLengthCm: 7.7\\nSepalWidthCm: 2.8\\nPetalLengthCm: 6.7\\nPetalWidthCm: 2.0\\nSpecies: Iris-virginica'),\n",
              " Document(metadata={'source': '/content/Iris.csv', 'row': 123}, page_content='Id: 124\\nSepalLengthCm: 6.3\\nSepalWidthCm: 2.7\\nPetalLengthCm: 4.9\\nPetalWidthCm: 1.8\\nSpecies: Iris-virginica'),\n",
              " Document(metadata={'source': '/content/Iris.csv', 'row': 124}, page_content='Id: 125\\nSepalLengthCm: 6.7\\nSepalWidthCm: 3.3\\nPetalLengthCm: 5.7\\nPetalWidthCm: 2.1\\nSpecies: Iris-virginica'),\n",
              " Document(metadata={'source': '/content/Iris.csv', 'row': 125}, page_content='Id: 126\\nSepalLengthCm: 7.2\\nSepalWidthCm: 3.2\\nPetalLengthCm: 6.0\\nPetalWidthCm: 1.8\\nSpecies: Iris-virginica'),\n",
              " Document(metadata={'source': '/content/Iris.csv', 'row': 126}, page_content='Id: 127\\nSepalLengthCm: 6.2\\nSepalWidthCm: 2.8\\nPetalLengthCm: 4.8\\nPetalWidthCm: 1.8\\nSpecies: Iris-virginica'),\n",
              " Document(metadata={'source': '/content/Iris.csv', 'row': 127}, page_content='Id: 128\\nSepalLengthCm: 6.1\\nSepalWidthCm: 3.0\\nPetalLengthCm: 4.9\\nPetalWidthCm: 1.8\\nSpecies: Iris-virginica'),\n",
              " Document(metadata={'source': '/content/Iris.csv', 'row': 128}, page_content='Id: 129\\nSepalLengthCm: 6.4\\nSepalWidthCm: 2.8\\nPetalLengthCm: 5.6\\nPetalWidthCm: 2.1\\nSpecies: Iris-virginica'),\n",
              " Document(metadata={'source': '/content/Iris.csv', 'row': 129}, page_content='Id: 130\\nSepalLengthCm: 7.2\\nSepalWidthCm: 3.0\\nPetalLengthCm: 5.8\\nPetalWidthCm: 1.6\\nSpecies: Iris-virginica'),\n",
              " Document(metadata={'source': '/content/Iris.csv', 'row': 130}, page_content='Id: 131\\nSepalLengthCm: 7.4\\nSepalWidthCm: 2.8\\nPetalLengthCm: 6.1\\nPetalWidthCm: 1.9\\nSpecies: Iris-virginica'),\n",
              " Document(metadata={'source': '/content/Iris.csv', 'row': 131}, page_content='Id: 132\\nSepalLengthCm: 7.9\\nSepalWidthCm: 3.8\\nPetalLengthCm: 6.4\\nPetalWidthCm: 2.0\\nSpecies: Iris-virginica'),\n",
              " Document(metadata={'source': '/content/Iris.csv', 'row': 132}, page_content='Id: 133\\nSepalLengthCm: 6.4\\nSepalWidthCm: 2.8\\nPetalLengthCm: 5.6\\nPetalWidthCm: 2.2\\nSpecies: Iris-virginica'),\n",
              " Document(metadata={'source': '/content/Iris.csv', 'row': 133}, page_content='Id: 134\\nSepalLengthCm: 6.3\\nSepalWidthCm: 2.8\\nPetalLengthCm: 5.1\\nPetalWidthCm: 1.5\\nSpecies: Iris-virginica'),\n",
              " Document(metadata={'source': '/content/Iris.csv', 'row': 134}, page_content='Id: 135\\nSepalLengthCm: 6.1\\nSepalWidthCm: 2.6\\nPetalLengthCm: 5.6\\nPetalWidthCm: 1.4\\nSpecies: Iris-virginica'),\n",
              " Document(metadata={'source': '/content/Iris.csv', 'row': 135}, page_content='Id: 136\\nSepalLengthCm: 7.7\\nSepalWidthCm: 3.0\\nPetalLengthCm: 6.1\\nPetalWidthCm: 2.3\\nSpecies: Iris-virginica'),\n",
              " Document(metadata={'source': '/content/Iris.csv', 'row': 136}, page_content='Id: 137\\nSepalLengthCm: 6.3\\nSepalWidthCm: 3.4\\nPetalLengthCm: 5.6\\nPetalWidthCm: 2.4\\nSpecies: Iris-virginica'),\n",
              " Document(metadata={'source': '/content/Iris.csv', 'row': 137}, page_content='Id: 138\\nSepalLengthCm: 6.4\\nSepalWidthCm: 3.1\\nPetalLengthCm: 5.5\\nPetalWidthCm: 1.8\\nSpecies: Iris-virginica'),\n",
              " Document(metadata={'source': '/content/Iris.csv', 'row': 138}, page_content='Id: 139\\nSepalLengthCm: 6.0\\nSepalWidthCm: 3.0\\nPetalLengthCm: 4.8\\nPetalWidthCm: 1.8\\nSpecies: Iris-virginica'),\n",
              " Document(metadata={'source': '/content/Iris.csv', 'row': 139}, page_content='Id: 140\\nSepalLengthCm: 6.9\\nSepalWidthCm: 3.1\\nPetalLengthCm: 5.4\\nPetalWidthCm: 2.1\\nSpecies: Iris-virginica'),\n",
              " Document(metadata={'source': '/content/Iris.csv', 'row': 140}, page_content='Id: 141\\nSepalLengthCm: 6.7\\nSepalWidthCm: 3.1\\nPetalLengthCm: 5.6\\nPetalWidthCm: 2.4\\nSpecies: Iris-virginica'),\n",
              " Document(metadata={'source': '/content/Iris.csv', 'row': 141}, page_content='Id: 142\\nSepalLengthCm: 6.9\\nSepalWidthCm: 3.1\\nPetalLengthCm: 5.1\\nPetalWidthCm: 2.3\\nSpecies: Iris-virginica'),\n",
              " Document(metadata={'source': '/content/Iris.csv', 'row': 142}, page_content='Id: 143\\nSepalLengthCm: 5.8\\nSepalWidthCm: 2.7\\nPetalLengthCm: 5.1\\nPetalWidthCm: 1.9\\nSpecies: Iris-virginica'),\n",
              " Document(metadata={'source': '/content/Iris.csv', 'row': 143}, page_content='Id: 144\\nSepalLengthCm: 6.8\\nSepalWidthCm: 3.2\\nPetalLengthCm: 5.9\\nPetalWidthCm: 2.3\\nSpecies: Iris-virginica'),\n",
              " Document(metadata={'source': '/content/Iris.csv', 'row': 144}, page_content='Id: 145\\nSepalLengthCm: 6.7\\nSepalWidthCm: 3.3\\nPetalLengthCm: 5.7\\nPetalWidthCm: 2.5\\nSpecies: Iris-virginica'),\n",
              " Document(metadata={'source': '/content/Iris.csv', 'row': 145}, page_content='Id: 146\\nSepalLengthCm: 6.7\\nSepalWidthCm: 3.0\\nPetalLengthCm: 5.2\\nPetalWidthCm: 2.3\\nSpecies: Iris-virginica'),\n",
              " Document(metadata={'source': '/content/Iris.csv', 'row': 146}, page_content='Id: 147\\nSepalLengthCm: 6.3\\nSepalWidthCm: 2.5\\nPetalLengthCm: 5.0\\nPetalWidthCm: 1.9\\nSpecies: Iris-virginica'),\n",
              " Document(metadata={'source': '/content/Iris.csv', 'row': 147}, page_content='Id: 148\\nSepalLengthCm: 6.5\\nSepalWidthCm: 3.0\\nPetalLengthCm: 5.2\\nPetalWidthCm: 2.0\\nSpecies: Iris-virginica'),\n",
              " Document(metadata={'source': '/content/Iris.csv', 'row': 148}, page_content='Id: 149\\nSepalLengthCm: 6.2\\nSepalWidthCm: 3.4\\nPetalLengthCm: 5.4\\nPetalWidthCm: 2.3\\nSpecies: Iris-virginica'),\n",
              " Document(metadata={'source': '/content/Iris.csv', 'row': 149}, page_content='Id: 150\\nSepalLengthCm: 5.9\\nSepalWidthCm: 3.0\\nPetalLengthCm: 5.1\\nPetalWidthCm: 1.8\\nSpecies: Iris-virginica')]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "loaded_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "mjVLD49Z-p9o"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iJFEPMip-p9o"
      },
      "source": [
        "# HTML"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8olZBGlP-p9o"
      },
      "source": [
        "pip install unstructured==0.15.0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install unstructured"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-AMGlpIhAzlo",
        "outputId": "1ad83f26-819d-44ea-849b-0476a2b7becd"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting unstructured\n",
            "  Using cached unstructured-0.16.12-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: chardet in /usr/local/lib/python3.10/dist-packages (from unstructured) (5.2.0)\n",
            "Collecting filetype (from unstructured)\n",
            "  Using cached filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting python-magic (from unstructured)\n",
            "  Using cached python_magic-0.4.27-py2.py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from unstructured) (5.3.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from unstructured) (3.9.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from unstructured) (2.32.3)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from unstructured) (4.12.3)\n",
            "Collecting emoji (from unstructured)\n",
            "  Using cached emoji-2.14.0-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: dataclasses-json in /usr/local/lib/python3.10/dist-packages (from unstructured) (0.6.7)\n",
            "Collecting python-iso639 (from unstructured)\n",
            "  Using cached python_iso639-2024.10.22-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting langdetect (from unstructured)\n",
            "  Using cached langdetect-1.0.9.tar.gz (981 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy<2 in /usr/local/lib/python3.10/dist-packages (from unstructured) (1.26.4)\n",
            "Collecting rapidfuzz (from unstructured)\n",
            "  Downloading rapidfuzz-3.11.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Collecting backoff (from unstructured)\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from unstructured) (4.12.2)\n",
            "Collecting unstructured-client (from unstructured)\n",
            "  Downloading unstructured_client-0.28.1-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from unstructured) (1.17.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from unstructured) (4.67.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from unstructured) (5.9.5)\n",
            "Collecting python-oxmsg (from unstructured)\n",
            "  Downloading python_oxmsg-0.0.1-py3-none-any.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: html5lib in /usr/local/lib/python3.10/dist-packages (from unstructured) (1.1)\n",
            "Collecting ndjson (from unstructured)\n",
            "  Downloading ndjson-0.3.1-py2.py3-none-any.whl.metadata (3.2 kB)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->unstructured) (2.6)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json->unstructured) (3.25.0)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json->unstructured) (0.9.0)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.10/dist-packages (from html5lib->unstructured) (1.17.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from html5lib->unstructured) (0.5.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->unstructured) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->unstructured) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->unstructured) (2024.11.6)\n",
            "Collecting olefile (from python-oxmsg->unstructured)\n",
            "  Downloading olefile-0.47-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->unstructured) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->unstructured) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->unstructured) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->unstructured) (2024.12.14)\n",
            "Collecting aiofiles>=24.1.0 (from unstructured-client->unstructured)\n",
            "  Downloading aiofiles-24.1.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: cryptography>=3.1 in /usr/local/lib/python3.10/dist-packages (from unstructured-client->unstructured) (43.0.3)\n",
            "Requirement already satisfied: eval-type-backport<0.3.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from unstructured-client->unstructured) (0.2.2)\n",
            "Requirement already satisfied: httpx>=0.27.0 in /usr/local/lib/python3.10/dist-packages (from unstructured-client->unstructured) (0.28.1)\n",
            "Collecting jsonpath-python<2.0.0,>=1.0.6 (from unstructured-client->unstructured)\n",
            "  Downloading jsonpath_python-1.0.6-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: nest-asyncio>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from unstructured-client->unstructured) (1.6.0)\n",
            "Collecting pydantic<2.10.0,>=2.9.2 (from unstructured-client->unstructured)\n",
            "  Downloading pydantic-2.9.2-py3-none-any.whl.metadata (149 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.4/149.4 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pypdf>=4.0 (from unstructured-client->unstructured)\n",
            "  Downloading pypdf-5.1.0-py3-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from unstructured-client->unstructured) (2.8.2)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from unstructured-client->unstructured) (1.0.0)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=3.1->unstructured-client->unstructured) (1.17.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->unstructured-client->unstructured) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->unstructured-client->unstructured) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx>=0.27.0->unstructured-client->unstructured) (0.14.0)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->unstructured) (24.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<2.10.0,>=2.9.2->unstructured-client->unstructured) (0.7.0)\n",
            "Collecting pydantic-core==2.23.4 (from pydantic<2.10.0,>=2.9.2->unstructured-client->unstructured)\n",
            "  Downloading pydantic_core-2.23.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json->unstructured) (1.0.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=3.1->unstructured-client->unstructured) (2.22)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio->httpx>=0.27.0->unstructured-client->unstructured) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx>=0.27.0->unstructured-client->unstructured) (1.2.2)\n",
            "Downloading unstructured-0.16.12-py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m45.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading emoji-2.14.0-py3-none-any.whl (586 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m586.9/586.9 kB\u001b[0m \u001b[31m33.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
            "Downloading ndjson-0.3.1-py2.py3-none-any.whl (5.3 kB)\n",
            "Downloading python_iso639-2024.10.22-py3-none-any.whl (274 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m274.9/274.9 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_magic-0.4.27-py2.py3-none-any.whl (13 kB)\n",
            "Downloading python_oxmsg-0.0.1-py3-none-any.whl (31 kB)\n",
            "Downloading rapidfuzz-3.11.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m37.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading unstructured_client-0.28.1-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.9/62.9 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiofiles-24.1.0-py3-none-any.whl (15 kB)\n",
            "Downloading jsonpath_python-1.0.6-py3-none-any.whl (7.6 kB)\n",
            "Downloading pydantic-2.9.2-py3-none-any.whl (434 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m434.9/434.9 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydantic_core-2.23.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m29.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pypdf-5.1.0-py3-none-any.whl (297 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m298.0/298.0 kB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading olefile-0.47-py2.py3-none-any.whl (114 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.6/114.6 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: langdetect\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993222 sha256=8118c81ea3c4827e045a452c59ea62d939e84f67276ec1655e529eefb8926cbe\n",
            "  Stored in directory: /root/.cache/pip/wheels/95/03/7d/59ea870c70ce4e5a370638b5462a7711ab78fba2f655d05106\n",
            "Successfully built langdetect\n",
            "Installing collected packages: ndjson, filetype, rapidfuzz, python-magic, python-iso639, pypdf, pydantic-core, olefile, langdetect, jsonpath-python, emoji, backoff, aiofiles, python-oxmsg, pydantic, unstructured-client, unstructured\n",
            "  Attempting uninstall: pydantic-core\n",
            "    Found existing installation: pydantic_core 2.27.2\n",
            "    Uninstalling pydantic_core-2.27.2:\n",
            "      Successfully uninstalled pydantic_core-2.27.2\n",
            "  Attempting uninstall: pydantic\n",
            "    Found existing installation: pydantic 2.10.4\n",
            "    Uninstalling pydantic-2.10.4:\n",
            "      Successfully uninstalled pydantic-2.10.4\n",
            "Successfully installed aiofiles-24.1.0 backoff-2.2.1 emoji-2.14.0 filetype-1.2.0 jsonpath-python-1.0.6 langdetect-1.0.9 ndjson-0.3.1 olefile-0.47 pydantic-2.9.2 pydantic-core-2.23.4 pypdf-5.1.0 python-iso639-2024.10.22 python-magic-0.4.27 python-oxmsg-0.0.1 rapidfuzz-3.11.0 unstructured-0.16.12 unstructured-client-0.28.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "tj9w2X3q-p9p"
      },
      "outputs": [],
      "source": [
        "from langchain_community.document_loaders import UnstructuredHTMLLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "vEYB-tqZ-p9p"
      },
      "outputs": [],
      "source": [
        "loader = UnstructuredHTMLLoader('/content/sample.html')\n",
        "\n",
        "loaded_data = loader.load()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "id": "NJ4rtKJq-p9p",
        "outputId": "d9b08a3a-4197-42a7-83bc-195ca186285a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Posted on October 5, 2023\\n\\nAI Accelera\\n\\nNo Comments\\n\\nThe new book “100 AI Startups” by Julio Colomer presents 100 new Artificial Intelligence companies that have earned at least $500,000 in their first year of existence and are radically changing how things are done in over 30 sectors, including banking, insurance, health, education, legal, logistics, marketing, sales, customer service, and even in public administration.\\n\\nThese 100 startups from Silicon Valley show how the fastest-growing area among the new Artificial Intelligence companies are the new LLM Apps (LLM Apps), which are changing every sector and areas of the company.\\n\\nIt’s not science fiction. There are already startups with LLM Applications that have earned more than $500,000 before their first year of existence by doing this:\\n\\nImproving the effectiveness of surgeons.\\n\\nCustomer service in banks.\\n\\nTutoring for students.\\n\\nLegal recommendations.\\n\\nStock analysis.\\n\\nDelivering physiotherapy sessions.\\n\\nFinding new clients.\\n\\nImmigration management.\\n\\nAudits.\\n\\nManaging contracts with Public Administration.\\n\\nManaging construction projects.\\n\\nEvent organization.\\n\\nManaging defaults.\\n\\nAnswering (yes, answering) surveys.\\n\\nGenerating news.\\n\\nNegotiating a sales contract.\\n\\nValuing a company.\\n\\nTaking notes in meetings.\\n\\n“Once again the future is being invented in Silicon Valley. And this time with an intensity not seen since the first wave of the Internet,” comments Julio Colomer.\\n\\n“In this wave, it’s not just venture capitalists, accelerators, and startups that are involved. All the major tech companies, from Microsoft to Google, including Apple, Facebook, Tesla, and Salesforce, have jumped in with multimillion-dollar investments in Generative Artificial Intelligence. We haven’t seen such unanimous interest in the last twenty years.”\\n\\nBeyond the initial interest generated by the launch of ChatGPT, little is known by the general public about how AI can be used to make money, be more productive, or live better.\\n\\n“The importance of this book is that it helps visualize what can already be done with the new Generative Artificial Intelligence. This is valuable not only for entrepreneurs looking for ideas but for executives and professionals from all sectors who want to advance in their professional careers.”\\n\\n“This book confirms the changes we announced in the previous book, Keys to Artificial Intelligence:\\n\\nFirst, that the new Artificial Intelligence inherits from Machine Learning and Data Science but is not the same. One is mistaken to think that the professional skills needed to build the new LLM Applications are the same as those needed for data analysis or a prediction model.\\n\\nSecond, that today the key is the LLM Applications, which ride on the backs of the Foundation LLMs (ChatGPT, Llama2, etc.) combining them with external data sources and APIs.\\n\\nThird, that the development of LLM Applications is experiencing exponential growth in the United States and will soon reach other countries: every company wants to have LLM Applications. This opens an excellent employment opportunity for those who enter this new field.”\\n\\nYou can see the table of contents of the book “100 AI Startups” here.\\n\\nThe books “100 AI Startups” and “Keys to Artificial Intelligence” can be found on Amazon.\\n\\n“We are in a new world. LLM Applications use methods and tools that were practically unknown three years ago: embeddings, vector databases, orchestration frameworks, LLM cache, prompt engineering, etc.”\\n\\nThe AI Accelerator (aceleradoraAI.com), with a presence in Spain, Latin America, and the United States, offers courses to learn how to create LLM Applications and other related services such as:\\n\\nDevelopment of LLM Applications for companies.\\n\\nDevelopment of LLM Applications for startups.\\n\\nArtificial Intelligence consulting services for companies.\\n\\nArtificial Intelligence training services for companies.\\n\\nStaff selection to form AI Teams in the company.\\n\\nTechnical support for entrepreneurs wanting to create an AI startup.\\n\\nAI Test Field: connecting entrepreneurs and junior developers.\\n\\nFor more information, visit the AI Accelera website.\\n\\nFor each of the 100 startups analyzed in the book “100 IA Startups“, it details:\\n\\nProblem solved.\\n\\nProposed solution.\\n\\nTechnical analysis.\\n\\nIdentifying data of the company, its website, and its founders.\\n\\nThe startup presented in the book use LLM Applications to solve the described problems. If you are interested in learning more about LLM Applications or Artificial Intelligence applied to the company, here are some interesting links:\\n\\nCourses and Bootcamps to learn to create LLM Applications.\\n\\nDevelopment of LLM Applications for your company.\\n\\nDevelopment of LLM Applications for a new startup.\\n\\nArtificial Intelligence consulting for businesses.\\n\\nIn-Company custom Artificial Intelligence training.\\n\\nSelection of Artificial Intelligence professionals, LLM Applications, Machine Learning, and Data Science for businesses.\\n\\nExperimental projects of LLM Applications: contact between entrepreneurs and junior developers.\\n\\nWe are looking for entrepreneurs interested in creating startups based on LLM Applications. We provide technical assistance and monitoring.\\n\\nWe are looking for collaborators: Do you work in a training center, a consultancy/advisory service, or in a support center for entrepreneurs? We are interested in talking to you.\\n\\nPost Tags: AI, artificial intelligence\\n\\nRelated Posts\\n\\nReal Case: Your new doctor is an Artificial Intelligence Agent. (2023)\\n\\nReal Case: Your child’s new tutor is an Artificial Intelligence Agent. (2023)\\n\\nReal Case: An Artificial Intelligence Agent takes over End-to-End testing for software applications. (2023)\\n\\nReal Case: An Artificial Intelligence Agent to find opportunities to contract with the Government. (2023)\\n\\nGo to the Main Page of our Blog'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "loaded_data[0].page_content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oz1NnI7K-p9p",
        "outputId": "71a648a1-df38-49d7-cc8e-d79f2202242d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Posted on October 5, 2023\n",
            "\n",
            "AI Accelera\n",
            "\n",
            "No Comments\n",
            "\n",
            "The new book “100 AI Startups” by Julio Colomer presents 100 new Artificial Intelligence companies that have earned at least $500,000 in their first year of existence and are radically changing how things are done in over 30 sectors, including banking, insurance, health, education, legal, logistics, marketing, sales, customer service, and even in public administration.\n",
            "\n",
            "These 100 startups from Silicon Valley show how the fastest-growing area among the new Artificial Intelligence companies are the new LLM Apps (LLM Apps), which are changing every sector and areas of the company.\n",
            "\n",
            "It’s not science fiction. There are already startups with LLM Applications that have earned more than $500,000 before their first year of existence by doing this:\n",
            "\n",
            "Improving the effectiveness of surgeons.\n",
            "\n",
            "Customer service in banks.\n",
            "\n",
            "Tutoring for students.\n",
            "\n",
            "Legal recommendations.\n",
            "\n",
            "Stock analysis.\n",
            "\n",
            "Delivering physiotherapy sessions.\n",
            "\n",
            "Finding new clients.\n",
            "\n",
            "Immigration management.\n",
            "\n",
            "Audits.\n",
            "\n",
            "Managing contracts with Public Administration.\n",
            "\n",
            "Managing construction projects.\n",
            "\n",
            "Event organization.\n",
            "\n",
            "Managing defaults.\n",
            "\n",
            "Answering (yes, answering) surveys.\n",
            "\n",
            "Generating news.\n",
            "\n",
            "Negotiating a sales contract.\n",
            "\n",
            "Valuing a company.\n",
            "\n",
            "Taking notes in meetings.\n",
            "\n",
            "“Once again the future is being invented in Silicon Valley. And this time with an intensity not seen since the first wave of the Internet,” comments Julio Colomer.\n",
            "\n",
            "“In this wave, it’s not just venture capitalists, accelerators, and startups that are involved. All the major tech companies, from Microsoft to Google, including Apple, Facebook, Tesla, and Salesforce, have jumped in with multimillion-dollar investments in Generative Artificial Intelligence. We haven’t seen such unanimous interest in the last twenty years.”\n",
            "\n",
            "Beyond the initial interest generated by the launch of ChatGPT, little is known by the general public about how AI can be used to make money, be more productive, or live better.\n",
            "\n",
            "“The importance of this book is that it helps visualize what can already be done with the new Generative Artificial Intelligence. This is valuable not only for entrepreneurs looking for ideas but for executives and professionals from all sectors who want to advance in their professional careers.”\n",
            "\n",
            "“This book confirms the changes we announced in the previous book, Keys to Artificial Intelligence:\n",
            "\n",
            "First, that the new Artificial Intelligence inherits from Machine Learning and Data Science but is not the same. One is mistaken to think that the professional skills needed to build the new LLM Applications are the same as those needed for data analysis or a prediction model.\n",
            "\n",
            "Second, that today the key is the LLM Applications, which ride on the backs of the Foundation LLMs (ChatGPT, Llama2, etc.) combining them with external data sources and APIs.\n",
            "\n",
            "Third, that the development of LLM Applications is experiencing exponential growth in the United States and will soon reach other countries: every company wants to have LLM Applications. This opens an excellent employment opportunity for those who enter this new field.”\n",
            "\n",
            "You can see the table of contents of the book “100 AI Startups” here.\n",
            "\n",
            "The books “100 AI Startups” and “Keys to Artificial Intelligence” can be found on Amazon.\n",
            "\n",
            "“We are in a new world. LLM Applications use methods and tools that were practically unknown three years ago: embeddings, vector databases, orchestration frameworks, LLM cache, prompt engineering, etc.”\n",
            "\n",
            "The AI Accelerator (aceleradoraAI.com), with a presence in Spain, Latin America, and the United States, offers courses to learn how to create LLM Applications and other related services such as:\n",
            "\n",
            "Development of LLM Applications for companies.\n",
            "\n",
            "Development of LLM Applications for startups.\n",
            "\n",
            "Artificial Intelligence consulting services for companies.\n",
            "\n",
            "Artificial Intelligence training services for companies.\n",
            "\n",
            "Staff selection to form AI Teams in the company.\n",
            "\n",
            "Technical support for entrepreneurs wanting to create an AI startup.\n",
            "\n",
            "AI Test Field: connecting entrepreneurs and junior developers.\n",
            "\n",
            "For more information, visit the AI Accelera website.\n",
            "\n",
            "For each of the 100 startups analyzed in the book “100 IA Startups“, it details:\n",
            "\n",
            "Problem solved.\n",
            "\n",
            "Proposed solution.\n",
            "\n",
            "Technical analysis.\n",
            "\n",
            "Identifying data of the company, its website, and its founders.\n",
            "\n",
            "The startup presented in the book use LLM Applications to solve the described problems. If you are interested in learning more about LLM Applications or Artificial Intelligence applied to the company, here are some interesting links:\n",
            "\n",
            "Courses and Bootcamps to learn to create LLM Applications.\n",
            "\n",
            "Development of LLM Applications for your company.\n",
            "\n",
            "Development of LLM Applications for a new startup.\n",
            "\n",
            "Artificial Intelligence consulting for businesses.\n",
            "\n",
            "In-Company custom Artificial Intelligence training.\n",
            "\n",
            "Selection of Artificial Intelligence professionals, LLM Applications, Machine Learning, and Data Science for businesses.\n",
            "\n",
            "Experimental projects of LLM Applications: contact between entrepreneurs and junior developers.\n",
            "\n",
            "We are looking for entrepreneurs interested in creating startups based on LLM Applications. We provide technical assistance and monitoring.\n",
            "\n",
            "We are looking for collaborators: Do you work in a training center, a consultancy/advisory service, or in a support center for entrepreneurs? We are interested in talking to you.\n",
            "\n",
            "Post Tags: AI, artificial intelligence\n",
            "\n",
            "Related Posts\n",
            "\n",
            "Real Case: Your new doctor is an Artificial Intelligence Agent. (2023)\n",
            "\n",
            "Real Case: Your child’s new tutor is an Artificial Intelligence Agent. (2023)\n",
            "\n",
            "Real Case: An Artificial Intelligence Agent takes over End-to-End testing for software applications. (2023)\n",
            "\n",
            "Real Case: An Artificial Intelligence Agent to find opportunities to contract with the Government. (2023)\n",
            "\n",
            "Go to the Main Page of our Blog\n"
          ]
        }
      ],
      "source": [
        "print(loaded_data[0].page_content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GpImcd3E-p9q"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AUJJF1vC-p9q"
      },
      "source": [
        "# PDF"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wdCQereg-p9q"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "Zg9eaPUJ-p9q"
      },
      "outputs": [],
      "source": [
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "loader = PyPDFLoader('/content/NLPconceptsResources.pdf')\n",
        "loaded_data = loader.load_and_split()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tFrDDUE_-p9r",
        "outputId": "a8c0f225-0984-49df-82fd-5bc42cbfdb42"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(metadata={'source': '/content/NLPconceptsResources.pdf', 'page': 0}, page_content='Basic NLP Concept s :\\n1\\nBasic NLP Concepts : \\nWhy We Need Text Preprocessing:\\n\\ue072\\ue094\\x00Improve Data Quality: Cleans and removes noise from raw text data, making it more \\nconsistent.\\n\\ue073\\ue094\\x00Reduce Dimensionality: Simplifies the text, making it easier for models to process and \\nanalyze.\\n\\ue074\\ue094\\x00Enhance Model Accuracy: Helps models focus on relevant information, improving \\nprediction accuracy.\\n\\ue075\\ue094\\x00Standardize Input: Ensures uniformity in text (e.g., lowercase, base forms), reducing \\nvariability.\\nWhen to Perform Text Preprocessing:\\n\\ue072\\ue094\\x00Before Model Training: Prepares text data to be fed into machine learning models.\\n\\ue073\\ue094\\x00Before Feature Extraction: Ensures that extracted features (like word vectors) are \\nmeaningful.\\n\\ue074\\ue094\\x00In Real-Time Systems: Preprocesses text on-the-fly in applications like chatbots.\\n\\ue075\\ue094\\x00During Data Exploration: Cleans text data before analysis to gain better insights.\\nNatural Language Processing (NLP): A Comprehensive Overview\\nCommon NLP Use Cases\\n\\ue072\\ue094\\x00Language Translation\\ue092 Automatically translating text from one language to another.\\n\\ue073\\ue094\\x00Speech Recognition\\ue092 Converting spoken language into text.\\n\\ue074\\ue094\\x00Hiring and Recruitment\\ue092 Analyzing resumes and job descriptions.\\n\\ue075\\ue094\\x00Chatbots\\ue092 Facilitating automated conversations with users.\\n\\ue076\\ue094\\x00Sentiment Analysis\\ue092 Determining the sentiment (positive, negative, neutral) expressed in \\ntext.'),\n",
              " Document(metadata={'source': '/content/NLPconceptsResources.pdf', 'page': 1}, page_content=\"Basic NLP Concept s :\\n2\\nSearch Engine Results\\nSearch engines use NLP to suggest relevant results based on user behavior and intent. For \\nexample, Google predicts what you'll type and shows relevant outcomes like a calculator for \\nmath equations or flight status for flight numbers.\\nLanguage Translation\\nNLP powers translation tools that convert text and voice formats between languages, with \\nroots going back to early machine translation in the 1950s.\\nSemantic Search\\nNLP-driven semantic search improves customer experience by understanding the context of \\nqueries and suggesting relevant products.\"),\n",
              " Document(metadata={'source': '/content/NLPconceptsResources.pdf', 'page': 2}, page_content='Basic NLP Concept s :\\n3\\nSentiment Analysis\\nOftentimes, when businesses need help understanding their customer needs, they turn \\nto\\xa0sentiment analysis.\\nSentiment analysis (also known as opinion mining) is an NLP strategy that can determine \\nwhether the meaning behind data is positive, negative, or neutral. For instance, if an unhappy \\nclient sends an email which mentions the terms “errorˮ and “not worth the priceˮ, then their \\nopinion would be automatically tagged as one with negative sentiment.\\nAutocomplete & Autocorrect\\nAutocomplete and autocorrect use NLP to predict and correct text as you type, improving \\ntyping accuracy and speed.\\nSpellcheck\\nSpellcheck notifies users of spelling errors and corrects them automatically, a common NLP \\napplication.\\nEmail Filters'),\n",
              " Document(metadata={'source': '/content/NLPconceptsResources.pdf', 'page': 3}, page_content='Basic NLP Concept s :\\n4\\nNLP enhances email filters, categorizing emails into primary, social, or promotions folders \\nbased on content.\\nChatbots\\nNLP-powered chatbots provide quick, automated responses in customer service, helping \\nbusinesses manage inquiries efficiently.\\nSmart Assistants\\nSmart assistants like Siri and Alexa use NLP for voice recognition and respond to everyday \\nqueries, even assisting with shopping.'),\n",
              " Document(metadata={'source': '/content/NLPconceptsResources.pdf', 'page': 4}, page_content='Basic NLP Concept s :\\n5\\nSocial Media Monitoring\\nNLP helps monitor social media by filtering comments and analyzing sentiment to understand \\ncustomer reactions.\\nCustomer Service Automation\\nNLP automates customer service by responding to simple questions and routing support \\ntickets to the right agents.\\nOptical Character Recognition (OCR)\\nOCR uses NLP to convert text from scanned documents or images into machine-readable \\nformats, useful for tasks like translation.'),\n",
              " Document(metadata={'source': '/content/NLPconceptsResources.pdf', 'page': 5}, page_content='Basic NLP Concept s :\\n6\\nSpeech Recognition\\nSpeech recognition converts human speech into text, enabling voice search and other \\napplications.\\nNatural Language Generation\\nNLP generates natural-sounding text or speech from data, used in smart assistants and \\ncustomer service bots.\\nSyllabus : \\nText Cleaning\\nRemoving Punctuation\\nRemoving Special Characters\\nRemoving Numbers\\nRemoving Extra Whitespaces\\nLowercasing Text\\nTokenization'),\n",
              " Document(metadata={'source': '/content/NLPconceptsResources.pdf', 'page': 6}, page_content='Basic NLP Concept s :\\n7\\nWord Tokenization\\nSentence Tokenization\\nSubword Tokenization\\nCharacter Tokenization\\nStop Words Removal\\nIdentifying Stop Words\\nRemoving Stop Words from Tokenized Text\\nStemming and Lemmatization\\nStemming \\nLemmatization\\nText Normalization\\nConverting Text to Lowercase\\nRemoving Accents and Diacritics\\nText Segmentation \\ue081Optional)\\nSentence Segmentation\\nTopic Segmentation\\nChunking\\nN\\ue088Gram Generation \\ue081Optional)\\nUnigrams\\nBigrams\\nTrigrams\\nHigher-order N\\ue088Grams\\nVectorization \\ue081Text to Numerical Form)\\nOHE (one Hot encoding )\\nBag of Words \\ue081BoW\\ue082\\nTF\\ue088IDF \\ue081Term Frequency-Inverse Document Frequency)\\nWord Embedding\\nWord2Vec'),\n",
              " Document(metadata={'source': '/content/NLPconceptsResources.pdf', 'page': 7}, page_content='Basic NLP Concept s :\\n8\\nGloVe \\ue081Global Vectors for Word Representation)\\nWhy We Need Text Preprocessing:\\n\\ue072\\ue094\\x00Improve Data Quality: Cleans and removes noise from raw text data, making it more \\nconsistent.\\n\\ue073\\ue094\\x00Reduce Dimensionality: Simplifies the text, making it easier for models to process and \\nanalyze.\\n\\ue074\\ue094\\x00Enhance Model Accuracy: Helps models focus on relevant information, improving \\nprediction accuracy.\\n\\ue075\\ue094\\x00Standardize Input: Ensures uniformity in text (e.g., lowercase, base forms), reducing \\nvariability.\\nWhen to Perform Text Preprocessing:\\n\\ue072\\ue094\\x00Before Model Training: Prepares text data to be fed into machine learning models.\\n\\ue073\\ue094\\x00Before Feature Extraction: Ensures that extracted features (like word vectors) are \\nmeaningful.\\n\\ue074\\ue094\\x00In Real-Time Systems: Preprocesses text on-the-fly in applications like chatbots.\\n\\ue075\\ue094\\x00During Data Exploration: Cleans text data before analysis to gain better insights.\\nKey NLP Terminology\\nCorpus:\\nExplanation: A large collection of text documents used for training NLP models. Itʼs \\nessentially the dataset containing all the text that will be analyzed.\\nDocument:'),\n",
              " Document(metadata={'source': '/content/NLPconceptsResources.pdf', 'page': 8}, page_content='Basic NLP Concept s :\\n9\\nExplanation: A single piece of text within a corpus. It can be an article, a paragraph, or \\neven a sentence, depending on how the data is structured.\\nVocabulary:\\nExplanation: The set of all unique words present in the corpus. Itʼs the dictionary of \\nterms that the model will learn from.\\nToken:\\nExplanation: The smallest unit of text, usually a word or punctuation mark, that results \\nfrom the tokenization process. Tokens are the building blocks for text analysis.\\nFrequency:\\nExplanation: The number of times a particular token (word) appears in a document or \\ncorpus. Itʼs used to understand how common or rare certain words are in the text.\\nN\\ue088Grams:\\nExplanation: A sequence of \\'n\\' consecutive words or tokens in a text. For example, in \\nthe sentence \"I love NLP,\" bigrams \\ue0812-grams) would be \"I love\" and \"love NLP.\"\\nUnigram:\\nExplanation: A single word or token in a text. For example, in \"I love NLP,\" the unigrams \\nare \"I,\" \"love,\" and \"NLP.\"\\nBigram:\\nExplanation: A pair of consecutive words or tokens. For example, in \"I love NLP,\" the \\nbigrams are \"I love\" and \"love NLP.\"\\nTrigram:\\nExplanation: A sequence of three consecutive words or tokens. For example, in \"I love \\nlearning NLP,\" the trigram is \"I love learning.\"'),\n",
              " Document(metadata={'source': '/content/NLPconceptsResources.pdf', 'page': 9}, page_content=\"Basic NLP Concept s :\\n10\\n1. Punctuation Removal\\nWhy Itʼs Important:\\nPunctuation marks like commas, periods, and exclamation points often don't add much \\nvalue in text analysis, especially when we're only interested in the content words. \\nRemoving them helps in standardizing the text and reducing noise.\\nPython Code:\\nThe issue in your code arises because string.punctuation contains individual punctuation \\ncharacters (e.g., ., ,, !), and you need to check if each character in a word is a punctuation \\nmark. If your goal is to remove punctuation from each word in filtered_tokens, you need to \\niterate over each word and remove punctuation from it. Here's how you can do it:\\nimport string\\n# Example filtered tokens\\nfiltered_tokens = ['Hello!', 'This', 'is', 'a', 'test.', 'Let\\\\'s', 'rem\\nove', 'punctuation!']\\n# Remove punctuation from each word\\npunctuation_removed = [''.join(char for char in word if char not in str\\ning.punctuation) for word in filtered_tokens]\"),\n",
              " Document(metadata={'source': '/content/NLPconceptsResources.pdf', 'page': 10}, page_content='Basic NLP Concept s :\\n11\\n# Remove empty strings if any word consisted only of punctuation\\npunctuation_removed = [word for word in punctuation_removed if word]\\nprint(\"Without Punctuation:\", punctuation_removed)\\nRemoving all irrelevant characters (Numbers and Punctuation) \\nRemove numbers if they are not relevant to your analyses \\ue0810\\ue0899\\ue082. And punctuation also will be \\nremove. Punctuation is basically the set of symbols \\ue083!ˮ#$%&ʼ()*+,-./:;\\ue1de?\\ue087\\ue083\\\\]^_`\\ue085|\\ue086\\ue0a3\\ue084\\ue092\\nResult: All numeric and punctuation has been replaced with space ʼ ‘ .\\nimport re\\ndef clean_text(text):\\n    # Remove all characters except alphabets and spaces\\n    cleaned_text = re.sub(r\\'[^A-Za-z\\\\s]\\', \\'\\', text)\\n    return cleaned_text'),\n",
              " Document(metadata={'source': '/content/NLPconceptsResources.pdf', 'page': 11}, page_content='Basic NLP Concept s :\\n12\\nparagraph = \"Here\\'s a paragraph with numbers 123 and special character\\ns! Like @#$%.\"\\ncleaned_paragraph = clean_text(paragraph)\\nprint(cleaned_paragraph)\\n3. Whitespace Removal\\nWhy Itʼs Important:\\nExtra spaces and tabs can create inconsistencies in text processing. Removing them \\nensures that the text is clean and uniform, preventing potential errors in analysis.\\nPython Code:\\npythonCopy code\\ntext_with_whitespace = \"  This   is   an example     sentence with    e\\nxtra spaces.  \"\\n# Remove extra whitespace\\ncleaned_text = \" \".join(text_with_whitespace.split())\\nprint(\"Cleaned Text:\", cleaned_text)\\nOutput:\\nvbnetCopy code\\nCleaned Text: This is an example sentence with extra spaces.\\nSummary\\nStop Words Removal: Reduces data size and enhances focus on meaningful words.\\nPunctuation Removal: Eliminates noise, making the text more consistent.\\nWhitespace Removal: Ensures clean and uniform text, preventing processing errors.'),\n",
              " Document(metadata={'source': '/content/NLPconceptsResources.pdf', 'page': 12}, page_content='Basic NLP Concept s :\\n13\\n Tokenization —\\nTokenization is the process of splitting the given text into smaller pieces called tokens. Words, \\nnumbers, punctuation marks, and others can be considered as tokens. We will use Natural \\nlanguage tool kit (nltk) library for tokenization.\\n1. Tokenization\\nDefinition\\n\\ue092 The process of breaking down text into smaller units called tokens (words, phrases, or \\nsymbols).\\nWhen to Use\\n\\ue092 At the beginning of the preprocessing pipeline to facilitate further analysis.\\nAdvantages\\nSimplifies text analysis by creating manageable units.\\nEssential for subsequent steps like POS tagging and NER.\\nDisadvantages:\\nCan lead to loss of context if not done carefully ( splitting contractions).\\nMay not handle multilingual text effectively.'),\n",
              " Document(metadata={'source': '/content/NLPconceptsResources.pdf', 'page': 13}, page_content='Basic NLP Concept s :\\n14\\nResult: As we can see the string has been changed into tokens, that has \\nbeen stored in the form of ‘list of stringʼ .\\nimport nltk\\nfrom nltk.tokenize import word_tokenize\\n# Download the necessary NLTK resources (if you haven\\'t already)\\nnltk.download(\\'punkt\\')\\n# Example sentence\\nsentence = \"Natural Language Processing with NLTK is fun and exciting!\"\\n# Tokenize the sentence\\ntokens = word_tokenize(sentence)\\n# Print the tokens'),\n",
              " Document(metadata={'source': '/content/NLPconceptsResources.pdf', 'page': 14}, page_content='Basic NLP Concept s :\\n15\\nprint(tokens)\\n1. Word Tokenization\\nDefinition: Splits text into individual words.\\nExample: \"Hello, world!\" → [\"Hello\", \"world\"]\\nCode Example \\ue081Python):\\nfrom nltk.tokenize import word_tokenize\\ntext = \"Hello, world!\"\\ntokens = word_tokenize(text)\\nprint(tokens)  # Output: [\\'Hello\\', \\',\\', \\'world\\', \\'!\\']\\n2. Sentence Tokenization\\nDefinition: Splits text into sentences.\\nExample: \"Hello, world! How are you?\" → [\"Hello, world!\", \"How are you?\"]\\nCode Example \\ue081Python):\\nfrom nltk.tokenize import sent_tokenize\\ntext = \"Hello, world! How are you?\"\\nsentences = sent_tokenize(text)\\nprint(sentences)  # Output: [\\'Hello, world!\\', \\'How are you?\\']\\n3. Character Tokenization\\nDefinition: Splits text into individual characters.\\nExample: \"Hello\" → [\"H\", \"e\", \"l\", \"l\", \"o\"]'),\n",
              " Document(metadata={'source': '/content/NLPconceptsResources.pdf', 'page': 15}, page_content='Basic NLP Concept s :\\n16\\nCode Example \\ue081Python):\\npythonCopy code\\ntext = \"Hello\"\\ntokens = list(text)\\nprint(tokens)  # Output: [\\'H\\', \\'e\\', \\'l\\', \\'l\\', \\'o\\']\\n4. Subword Tokenization\\nDefinition: Splits text into subword units, useful for handling rare or unknown words.\\nExample: \"unbelievable\" → [\"un\", \"believable\"]\\nCode Example \\ue081Python with SentencePiece):\\nimport sentencepiece as spm\\nsp = spm.SentencePieceProcessor(model_file=\\'m.model\\')\\ntext = \"unbelievable\"\\ntokens = sp.encode_as_pieces(text)\\nprint(tokens)  # Output example: [\\' ▁ un\\', \\'believable\\']\\n5. Whitespace Tokenization\\nDefinition: Splits text based on whitespace.\\nExample: \"Hello world\" → [\"Hello\", \"world\"]\\nCode Example \\ue081Python):\\ntext = \"Hello world\"\\ntokens = text.split()\\nprint(tokens)  # Output: [\\'Hello\\', \\'world\\']\\nRemoving Stopwords'),\n",
              " Document(metadata={'source': '/content/NLPconceptsResources.pdf', 'page': 16}, page_content='Basic NLP Concept s :\\n17\\n“Stopwordsˮ are the most common words in a language like “theˮ, “aˮ, “meˮ, “isˮ, “toˮ, “allˮ,. \\nThese words do not carry important meaning and are usually removed from texts. It is possible \\nto remove stopwords using Natural Language Toolkit (nltk). You also may check the list of \\nstopwords by using following code.\\nStop Words Removal\\nDefinition\\n\\ue092 The process of removing common words that do not add significant meaning to the text (e.g., \\n\"the,\" \"is,\" \"and\").\\nWhen to Use\\n\\ue092 After tokenization to reduce noise in the data.\\nAdvantages\\n:\\nReduces dimensionality of the dataset.\\nEnhances the focus on meaningful words.\\nDisadvantages\\n:\\nMay remove important context in certain applications (e.g., sentiment analysis).\\nRequires customization based on the specific use case and language.'),\n",
              " Document(metadata={'source': '/content/NLPconceptsResources.pdf', 'page': 17}, page_content='Basic NLP Concept s :\\n18\\n1. Stop Words Removal\\nWhy Itʼs Important:\\nStop words like \"is,\" \"the,\" and \"and\" are common words that usually don\\'t contribute much \\nto the meaning of the text. Removing them reduces the size of the data and helps the \\nmodel focus on more significant words.\\nPython Code:\\nimport nltk\\nfrom nltk.corpus import stopwords\\nfrom nltk.tokenize import word_tokenize\\n# Sample text\\ntext = \"This is an example sentence, demonstrating the removal of stop \\nwords.\"\\n# Tokenize the text\\ntokens = word_tokenize(text)\\n# Remove stop words\\nstop_words = set(stopwords.words(\\'english\\'))\\nfiltered_tokens = [word for word in tokens if word.lower() not in stop_\\nwords]\\nprint(\"Filtered Tokens:\", filtered_tokens)\\n Stemming and Lemmatization :\\nStemming\\xa0usually refers to a crude process that chops off the ends of words in the hope of \\nachieving this goal correctly most of the time, and often includes the removal of derivational \\nunits (the obtained element is known as the stem).\\n\\xa0lemmatization\\xa0consists in doing things properly with the use of a vocabulary and \\nmorphological analysis of words, to return the base or dictionary form of a word, which is \\nknown as the lemma.'),\n",
              " Document(metadata={'source': '/content/NLPconceptsResources.pdf', 'page': 18}, page_content='Basic NLP Concept s :\\n19\\nStemming\\nDefinition\\n\\ue092 Reduces words to their root form by removing suffixes ( \"running\" to \"run\").\\nWhen to Use\\n\\ue092 When a rough approximation of word forms is sufficient.\\nAdvantages\\nSimple and computationally efficient.\\nHelps in standardizing words to a common base form.\\nDisadvantages\\n:\\nCan produce non-words ( \"universe\" to \"univers\").\\nMay lose some meaning or context.'),\n",
              " Document(metadata={'source': '/content/NLPconceptsResources.pdf', 'page': 19}, page_content='Basic NLP Concept s :\\n20\\nStemming\\nimport nltk\\nfrom nltk.stem import PorterStemmer\\n# Initialize the Porter Stemmer\\nstemmer = PorterStemmer()\\n# Example words\\nwords = [\"running\", \"runner\", \"ran\", \"easily\", \"fairly\"]\\n# Apply stemming\\nstemmed_words = [stemmer.stem(word) for word in words]\\nprint(\"Stemmed Words:\", stemmed_words)\\nLemmatization\\nDefinition:\\nReduces words to their base or dictionary form, considering the context (e.g., \"better\" to \\n\"good\").\\nWhen to Use\\n\\ue092 When accurate word forms are required for analysis.\\nAdvantages\\n:\\nProvides more meaningful and contextually appropriate results than stemming.\\nPreserves grammatical correctness.\\nDisadvantages\\n:'),\n",
              " Document(metadata={'source': '/content/NLPconceptsResources.pdf', 'page': 20}, page_content='Basic NLP Concept s :\\n21\\nMore computationally intensive than stemming.\\nRequires a comprehensive dictionary and context understanding.\\nLemmatization:\\nResult: Now here we can see it finds the root word, like ‘troublingʼ to \\n‘troubleʼ, ‘tookʼ to ‘takeʼ and ‘payedʼ to ‘payʼ . So, As opposed to \\nstemming, lemmatization does not simply chop off inflections. Instead it \\nuses lexical knowledge bases to get the correct base forms of words.\\nLemmatization\\nimport nltk\\nfrom nltk.stem import WordNetLemmatizer\\n# Download the necessary NLTK resources (if you haven\\'t already)\\nnltk.download(\\'wordnet\\')\\nnltk.download(\\'omw-1.4\\')\\n# Initialize the WordNet Lemmatizer\\nlemmatizer = WordNetLemmatizer()\\n# Example words\\nwords = [\"running\", \"runner\", \"ran\", \"easily\", \"fairly\"]'),\n",
              " Document(metadata={'source': '/content/NLPconceptsResources.pdf', 'page': 21}, page_content='Basic NLP Concept s :\\n22\\n# Apply lemmatization\\nlemmatized_words = [lemmatizer.lemmatize(word) for word in words]\\nprint(\"Lemmatized Words:\", lemmatized_words)\\nKey Differences:\\nStemming may produce non-meaningful roots (\"easily\" to \"easili\"), whereas lemmatization \\nproduces actual words.\\nLemmatization considers the context and part of speech \\ue081POS\\ue082 of a word, while stemming \\ndoes not.\\nStemming is faster but less accurate; lemmatization is slower but more accurate and \\nmeaningful.\\nText Normalization\\nDefinition\\n\\ue092 The process of converting text to a standard format ( lowercasing, removing punctuation).\\nWhen to Use\\n\\ue092 Early in the preprocessing pipeline to ensure consistency.\\nAdvantages\\nReduces variability in text data.\\nHelps in improving model performance by standardizing input.\\nDisadvantages\\nMay remove important features ( capitalization for proper nouns).\\nRequires careful consideration of what to normalize.\\nConvert all characters into lowercase —'),\n",
              " Document(metadata={'source': '/content/NLPconceptsResources.pdf', 'page': 22}, page_content='Basic NLP Concept s :\\n23\\nConvert all characters into lowercase\\nHere\\'s a paragraph you can use for converting characters into lowercase:\\nparagraph = \"This is an Example Paragraph with Mixed CASE letters.\"\\nlowercase_paragraph = paragraph.lower()\\nprint(lowercase_paragraph)\\n1. Lowercasing\\nLowercasing involves converting all characters in the text to lowercase. This helps in \\nstandardizing the text and reducing the complexity of further text processing steps.\\nExample:\\ntext = \"Natural Language Processing is an Interesting Field.\"\\n# Convert to lowercase\\nlowercased_text = text.lower()'),\n",
              " Document(metadata={'source': '/content/NLPconceptsResources.pdf', 'page': 23}, page_content='Basic NLP Concept s :\\n2 4\\nprint(\"Lowercased Text:\", lowercased_text)\\nOutput:\\nsqlCopy code\\nLowercased Text: natural language processing is an interesting field.\\n4. Text Segmentation\\nText segmentation involves breaking down a text into meaningful chunks, such as sentences or \\nphrases.\\nExample:\\npythonCopy code\\nimport nltk\\nnltk.download(\\'punkt\\')\\nfrom nltk.tokenize import sent_tokenize, word_tokenize\\n# Sample text\\ntext = \"Natural language processing is an interesting field. It has man\\ny applications in AI.\"\\n# Sentence segmentation\\nsentences = sent_tokenize(text)\\n# Word segmentation\\nwords = word_tokenize(text)\\nprint(\"Sentences:\", sentences)\\nprint(\"Words:\", words)\\nSummary\\nLowercasing standardizes text by converting it to lowercase.\\nStemming reduces words to their base form, often resulting in non-words.'),\n",
              " Document(metadata={'source': '/content/NLPconceptsResources.pdf', 'page': 24}, page_content='Basic NLP Concept s :\\n25\\nLemmatization reduces words to their dictionary form, producing actual words.\\nText Segmentation breaks down text into meaningful chunks, such as sentences and \\nwords.\\nPart-of-Speech Tagging (POS Tagging)\\nDefinition\\n\\ue092 Categorizes each word in a sentence into its grammatical function (nouns, verbs, adjectives, \\netc.).\\nWhen to Use\\n\\ue092 After tokenization to understand the grammatical structure of the text.\\nAdvantages\\nEnhances understanding of the text\\'s meaning and structure.\\nUseful for tasks like parsing and NER.\\nDisadvantage\\nRequires accurate models, which can be language-dependent.\\nMay struggle with ambiguous words based on context.\\n. Part of Speech (POS) Tagging\\nPOS tagging is the process of marking up a word in a text as corresponding to a particular part \\nof speech, based on both its definition and context.\\nimport nltk\\nfrom nltk.tokenize import word_tokenize\\n# Download necessary NLTK resources (if you haven\\'t already)\\nnltk.download(\\'punkt\\')\\nnltk.download(\\'averaged_perceptron_tagger\\')\\n# Example sentence\\nsentence = \"The quick brown fox jumps over the lazy dog.\"'),\n",
              " Document(metadata={'source': '/content/NLPconceptsResources.pdf', 'page': 25}, page_content='Basic NLP Concept s :\\n26\\n# Tokenize the sentence\\ntokens = word_tokenize(sentence)\\n# Perform POS tagging\\npos_tags = nltk.pos_tag(tokens)\\nprint(\"POS Tags:\", pos_tags)\\nOutput:\\nIn the output:\\nDT \\ue09b Determiner\\nJJ \\ue09b Adjective\\nNN \\ue09b Noun\\nVBZ \\ue09b Verb \\ue0813rd person singular present)\\nIN \\ue09b Preposition\\nNamed Entity Recognition (NER)\\nDefinition\\n\\ue092 Identifies and classifies named entities ( names, locations, organizations) in text.\\nWhen to Use\\n\\ue092 After tokenization and POS tagging to extract structured information.\\nAdvantages\\n:\\nHelps in extracting valuable information from unstructured text.\\nFacilitates tasks like information retrieval and question answering.\\nDisadvantages\\n:\\nCan be sensitive to the quality of training data.\\nMay misclassify entities in ambiguous contexts.\\n2. Named Entity Recognition (NER)'),\n",
              " Document(metadata={'source': '/content/NLPconceptsResources.pdf', 'page': 26}, page_content='Basic NLP Concept s :\\n27\\nNER is the process of identifying entities such as names of persons, organizations, locations, \\nexpressions of times, quantities, monetary values, percentages, etc., in text.\\npythonCopy code\\nimport nltk\\nfrom nltk import word_tokenize, pos_tag, ne_chunk\\n# Download necessary NLTK resources (if you haven\\'t already)\\nnltk.download(\\'maxent_ne_chunker\\')\\nnltk.download(\\'words\\')\\n# Example sentence\\nsentence = \"Apple is looking at buying U.K. startup for $1 billion.\"\\n# Tokenize the sentence\\ntokens = word_tokenize(sentence)\\n# Perform POS tagging\\npos_tags = pos_tag(tokens)\\n# Perform Named Entity Recognition\\nnamed_entities = ne_chunk(pos_tags)\\nprint(\"Named Entities:\", named_entities)\\nDetailed Explanation\\nPOS Tagging\\ue092 Each word in the sentence is tagged with its corresponding part of speech.\\nNER\\ue092 The ne_chunk function in NLTK takes POS-tagged tokens and identifies named entities, \\nwhich could be organizations, people, locations, etc.\\nDetailed Explanation:'),\n",
              " Document(metadata={'source': '/content/NLPconceptsResources.pdf', 'page': 27}, page_content='Basic NLP Concept s :\\n28\\n1. One-Hot Encoding\\nDescription:\\nOne-Hot Encoding represents each word in the text as a binary vector. In this vector, each \\ndimensabulary, with a 1 in the position of the word and 0s elsewhere.\\nWhy Use It:\\nSimple Representation: It provides a straightforward method for converting text into \\nnumerical form.\\nPreprocessing Step: Useful for creating input features for machine learning models that \\nrequire numerical input.\\nWhen to Use:\\nSmall Vocabulary: Suitable when dealing with a limited number of unique words.\\nInitial Model Building: Often used as a starting point in text classification tasks.\\nAdvantages:\\nSimplicity: Easy to understand and implement.\\nClear Mapping: Directly maps each word to a unique position in the vector space.\\nDisadvantages:\\nHigh Dimensionality: Vocabulary size directly affects vector size, leading to high-\\ndimensional vectors if the vocabulary is large.\\nSparsity: Most of the vector elements are zero, resulting in sparse representations.\\nNo Semantic Meaning: Fails to capture any meaning or relationships between words.'),\n",
              " Document(metadata={'source': '/content/NLPconceptsResources.pdf', 'page': 28}, page_content='Basic NLP Concept s :\\n29\\nout of vocabulary\\nnew data we cannot add or extra data we cannot handle \\nExample:\\nVocabulary:\\nvocabulary = [\"I\", \"love\", \"machine\", \"learning\"]\\nOne-Hot Encoding for \"machine\":\\nimport numpy as np\\ndef one_hot_encode(word, vocab):\\n    vector = np.zeros(len(vocab))\\n    vector[vocab.index(word)] = 1\\n    return vector\\nvocab = [\"I\", \"love\", \"machine\", \"learning\"]\\none_hot = one_hot_encode(\"machine\", vocab)\\nprint(one_hot)  # Output: [0. 0. 1. 0.]\\nExample: One-Hot Encoding with Pandas\\nDataset:\\nLet\\'s start with a simple dataset containing a categorical column:\\npythonCopy code\\nimport pandas as pd\\n# Sample dataset with a categorical column\\ndata = {\\'Color\\': [\\'Red\\', \\'Blue\\', \\'Green\\', \\'Red\\', \\'Green\\']}\\ndf = pd.DataFrame(data)\\nprint(\"Original DataFrame:\")\\nprint(df)\\nPerforming One-Hot Encoding:'),\n",
              " Document(metadata={'source': '/content/NLPconceptsResources.pdf', 'page': 29}, page_content='Basic NLP Concept s :\\n30\\nUsing Pandas, you can perform one-hot encoding with the pd.get_dummies() function:\\npythonCopy code\\n# Perform one-hot encoding using Pandas\\none_hot_encoded = pd.get_dummies(df, columns=[\\'Color\\'])\\nprint(\"\\\\nOne-Hot Encoded DataFrame:\")\\nprint(one_hot_encoded)\\nCode Example:\\npythonCopy code\\nfrom sklearn.preprocessing import OneHotEncoder\\nimport numpy as np\\n# Define the vocabulary\\nvocabulary = [\"apple\", \"banana\", \"orange\"]\\n# Convert the vocabulary into a format suitable for OneHotEncoder\\nvocab_array = np.array(vocabulary).reshape(-1, 1)\\n# Initialize the OneHotEncoder\\none_hot_encoder = OneHotEncoder(sparse=False)\\n# Fit and transform the vocabulary\\none_hot_encoded = one_hot_encoder.fit_transform(vocab_array)\\n# Display the One-Hot Encoded vectors\\nfor word, vec in zip(vocabulary, one_hot_encoded):\\n    print(f\"{word}: {vec}\")\\nBag-of-Words (BOW) Model: Detailed Explanation'),\n",
              " Document(metadata={'source': '/content/NLPconceptsResources.pdf', 'page': 30}, page_content='Basic NLP Concept s :\\n31\\nWhat is Bag-of-Words?\\nThe Bag-of-Words \\ue081BOW\\ue082 model is a method for representing text data in Natural Language \\nProcessing \\ue081NLP\\ue082. It transforms text into a numerical format by counting the frequency of each \\nword in a document, disregarding grammar and word order.\\nKey Characteristics:\\nUnordered: The model does not consider the sequence or structure of words in the text.\\nFrequency-Based: It captures how often each word appears in the document.\\nWhy Use Bag-of-Words?\\nSimplicity: BOW is straightforward to implement and understand, making it a good starting \\npoint for text representation.\\nEffectiveness: Despite its simplicity, BOW can perform well for various NLP tasks, such as \\ntext classification and sentiment analysis.\\nCompatibility: The BOW representation can be easily used with many machine learning \\nalgorithms.'),\n",
              " Document(metadata={'source': '/content/NLPconceptsResources.pdf', 'page': 31}, page_content='Basic NLP Concept s :\\n32\\nWhen to Use Bag-of-Words?\\nInitial Text Representation: BOW is often used as an initial method for text representation \\nbefore applying more complex models.\\nText Classification: It is suitable for tasks like spam detection, sentiment analysis, and \\ntopic classification.\\nSmall to Medium-Sized Datasets: BOW works well when the dataset is not too large, as it \\ncan become computationally expensive with larger vocabularies.\\nImportance of Bag-of-Words\\nFeature Extraction: BOW transforms text into a numerical format that can be fed into \\nmachine learning models.\\nBaseline Comparison: It serves as a baseline for evaluating the performance of more \\nsophisticated models like TF\\ue088IDF or word embeddings.\\nHow to Implement Bag-of-Words\\n\\ue072\\ue094\\x00Text Preprocessing:\\nTokenization: Split text into words.\\nNormalization: Convert text to lowercase and remove stop words.\\n\\ue073\\ue094\\x00Vocabulary Collection:\\nCreate a set of unique words from the text.\\n\\ue074\\ue094\\x00Vectorization:\\nConvert text into numerical vectors based on word frequency.\\nDisadvantages:\\n\\ue072\\ue094\\x00sparsity \\n\\ue073\\ue094\\x00ordering \\n\\ue074\\ue094\\x00out of vocabulary issue \\nExample of Bag-of-Words\\nDocuments:\\n\\ue072\\ue094\\x00Document 1\\ue092 \"I love machine learning\"\\n\\ue073\\ue094\\x00Document 2\\ue092 \"Machine learning is fun\"\\nStep 1\\ue092 Text Preprocessing'),\n",
              " Document(metadata={'source': '/content/NLPconceptsResources.pdf', 'page': 32}, page_content='Basic NLP Concept s :\\n33\\nTokenization:\\nDoc 1\\ue092 \\ue083\"I\", \"love\", \"machine\", \"learning\"]\\nDoc 2\\ue092 \\ue083\"Machine\", \"learning\", \"is\", \"fun\"]\\nNormalization:\\nDoc 1\\ue092 \\ue083\"love\", \"machine\", \"learning\"]\\nDoc 2\\ue092 \\ue083\"machine\", \"learning\", \"fun\"]\\nStep 2\\ue092 Vocabulary Collection\\nThe vocabulary from the documents is:\\npythonCopy code\\n[\"love\", \"machine\", \"learning\", \"is\", \"fun\"]\\nStep 3\\ue092 Vectorization\\nCreate a document-term matrix based on word frequency:\\nDocument love machine learning is fun\\nDocument 11 1 1 0 0\\nDocument 20 1 1 1 1\\nCode Implementation\\nHere\\'s how to implement the Bag-of-Words model using Python with the scikit-learn library:\\nfrom sklearn.feature_extraction.text import CountVectorizer\\nimport pandas as pd\\n# Sample documents\\ncorpus = [\\n    \"I love machine learning\",\\n    \"Machine learning is fun\"\\n]\\n# Create the Bag-of-Words vectorizer\\nvectorizer = CountVectorizer()\\n# Fit and transform the corpus\\nX = vectorizer.fit_transform(corpus)'),\n",
              " Document(metadata={'source': '/content/NLPconceptsResources.pdf', 'page': 33}, page_content='Basic NLP Concept s :\\n34\\n# Get feature names (words)\\nfeature_names = vectorizer.get_feature_names_out()\\n# Convert the Bag-of-Words matrix to a dense format and display it\\ndense = X.todense()\\ndenselist = dense.tolist()\\n# Display the Bag-of-Words representation\\ndf_bow = pd.DataFrame(denselist, columns=feature_names)\\nprint(df_bow)\\nTF -IDF :\\nStandard TF-IDF'),\n",
              " Document(metadata={'source': '/content/NLPconceptsResources.pdf', 'page': 34}, page_content='Basic NLP Concept s :\\n35\\nItʼs obtained by combining two terms:\\nImage credit: Author\\nwhere\\xa0Term Frequency\\xa0\\ue081TF\\ue082 is the frequency of the word\\xa0t\\xa0within the document d. In other \\nwords, itʼs the ratio between the count of the word within the document and the total number of \\nwords:\\nAs we said before, the term frequency is not enough to provide efficient measures. We also \\nneed to combine it with another term, called\\xa0Inverse Document Frequency. Itʼs a logarithmic \\ntransformation of a fraction, calculated as the total number of documents in the corpus divided \\nby the number of documents containing the word.\\nWhat is TF-IDF?\\nTF\\ue088IDF stands for Term Frequency-Inverse Document Frequency. It is a numerical statistic \\nused in information retrieval and text mining to evaluate the importance of a term (word or \\nphrase) in a document relative to a collection of documents (corpus). TF\\ue088IDF helps identify \\nterms that are significant in specific documents while discounting terms that appear frequently \\nacross many documents.\\nComponents of TF-IDF\\n\\ue072\\ue094\\x00Term Frequency \\ue081TF\\ue082:\\nDefinition\\ue092 Measures how frequently a term appears in a document.\\nCalculation:'),\n",
              " Document(metadata={'source': '/content/NLPconceptsResources.pdf', 'page': 35}, page_content='Basic NLP Concept s :\\n36\\nRaw Count\\ue092 Number of times the term appears in the document.\\nBoolean Frequency\\ue092 1 if the term appears, 0 otherwise.\\n\\ue073\\ue094\\x00Inverse Document Frequency \\ue081IDF\\ue082:\\nDefinition\\ue092 Measures how common or rare a term is across all documents in the \\ncorpus.\\n\\ue074\\ue094\\x00TF\\ue088IDF Score:\\nDefinition\\ue092 Combines TF and IDF to provide a measure of a termʼs importance in a \\ndocument relative to the entire corpus.\\nWhen to Use TF-IDF\\nDocument Classification\\ue092 To determine the importance of terms in classifying text \\ndocuments.\\nInformation Retrieval\\ue092 To rank documents based on their relevance to a search query.\\nText Mining\\ue092 To extract meaningful terms for further analysis or modeling.\\nimportance of TF-IDF\\nHighlighting Unique Words\\ue092 TF\\ue088IDF helps identify words that are significant in a specific \\ndocument but not common across all documents. This is particularly useful for information \\nretrieval and text mining.\\nFeature Representation\\ue092 It provides a way to convert text into a numerical format that can \\nbe used in machine learning algorithms.\\nExample of TF-IDF Calculation\\nCorpus\\n\\ue072\\ue094\\x00Document 1: \"I love machine learning\"\\n\\ue073\\ue094\\x00Document 2: \"Machine learning is fun\"\\n\\ue074\\ue094\\x00Document 3: \"I love programming\"\\nCalculate TF-IDF for the Term \"machine\" in Document 1\\nStep 1\\ue092 Calculate Term Frequency \\ue081TF\\ue082'),\n",
              " Document(metadata={'source': '/content/NLPconceptsResources.pdf', 'page': 36}, page_content='Basic NLP Concept s :\\n37\\nTerm Frequency \\ue081TF\\ue082\\ue092 TF for \"machine\" in Document 1 \\ue09b \\ue081Number of times \"machine\" \\nappears in Document 1\\ue082 / \\ue081Total number of words in Document 1\\ue082\\nDocument 1\\ue092 \"I love machine learning\" \\ue081Total words \\ue09b 4\\ue082\\n\"Machine\" appears 1 time in Document 1.\\nTF\\ue081\"machine\", Doc1\\ue082 \\ue09b 1 / 4 \\ue09b 0.25\\nStep 2\\ue092 Calculate Inverse Document Frequency \\ue081IDF\\ue082\\nIDF\\ue092 IDF for \"machine\" = log(Total number of documents / Number of documents \\ncontaining the term)\\n\"Machine\" appears in 2 documents out of 3.\\nIDF\\ue081\"machine\") = log(3 / 2\\ue082 \\ue0a2 0.176\\nStep 3\\ue092 Calculate TF\\ue088IDF\\nTF\\ue088IDF\\ue092 TF\\ue088IDF\\ue081\"machine\", Doc1\\ue082 \\ue09b TF * IDF \\ue09b 0.25 \\ue0a4 0.176 \\ue0a2 0.044\\nCorrected Calculation\\nTerm Frequency \\ue081TF\\ue082\\ue092\\nDocument 1: \"I love machine learning\" \\ue081Total words \\ue09b 4\\ue082\\nTF\\ue081\"machine\", Doc1\\ue082 \\ue09b 1 / 4 \\ue09b 0.25\\nInverse Document Frequency \\ue081IDF\\ue082\\ue092\\nIDF\\ue081\"machine\") = log(3 / 2\\ue082 \\ue0a2 0.176\\nTF\\ue088IDF Calculation:\\nTF\\ue088IDF\\ue081\"machine\", Doc1\\ue082 \\ue09b 0.25 \\ue0a4 0.176 \\ue0a2 0.044\\nUpdated Python Code\\nHere\\'s the corrected code implementation to reflect these calculations:\\nfrom sklearn.feature_extraction.text import TfidfVectorizer\\nimport pandas as pd\\n# Sample corpus\\ncorpus = [\\n    \"I love machine learning\",\\n    \"Machine learning is fun\",\\n    \"I love programming\"\\n]'),\n",
              " Document(metadata={'source': '/content/NLPconceptsResources.pdf', 'page': 37}, page_content='Basic NLP Concept s :\\n38\\n# Create the TF-IDF vectorizer\\nvectorizer = TfidfVectorizer()\\n# Fit and transform the corpus\\ntfidf_matrix = vectorizer.fit_transform(corpus)\\n# Get feature names (words)\\nfeature_names = vectorizer.get_feature_names_out()\\n# Convert the TF-IDF matrix to a dense format and display it\\ndense = tfidf_matrix.todense()\\ndenselist = dense.tolist()\\n# Create a DataFrame for better readability\\ndf_tfidf = pd.DataFrame(denselist, columns=feature_names)\\nprint(df_tfidf)\\nAdvantages of TF-IDF\\n\\ue072\\ue094\\x00Highlights Unique Words\\ue092 TF\\ue088IDF helps identify words that are significant in a specific \\ndocument but not common across all documents. This is particularly useful for information \\nretrieval and text mining.\\n\\ue073\\ue094\\x00Provides Feature Representation\\ue092 It converts text into a numerical format that can be used \\nin machine learning algorithms.\\n\\ue074\\ue094\\x00Effective for Text Classification\\ue092 When building models to classify documents based on \\ntheir content, TF\\ue088IDF can be used as a feature representation.\\n\\ue075\\ue094\\x00Useful for Information Retrieval\\ue092 In search engines, TF\\ue088IDF helps rank documents based \\non their relevance to a query.\\n\\ue076\\ue094\\x00Identifies Key Terms for Topic Modeling\\ue092 TF\\ue088IDF can help identify the main topics of a \\ndocument by highlighting the most significant terms.'),\n",
              " Document(metadata={'source': '/content/NLPconceptsResources.pdf', 'page': 38}, page_content=\"Basic NLP Concept s :\\n39\\nDisadvantages of TF-IDF\\n\\ue072\\ue094\\x00Ignores Word Order and Grammar\\ue092 TF\\ue088IDF represents text as a bag of words, disregarding \\nthe order of words and the grammar of the text.\\n\\ue073\\ue094\\x00Sensitive to Document Length\\ue092 TF\\ue088IDF scores can be biased towards longer documents, \\nas they tend to have more unique words.\\n\\ue074\\ue094\\x00Sparsity in High-Dimensional Spaces\\ue092 When dealing with a large vocabulary, the TF\\ue088IDF \\nmatrix becomes very sparse, which can lead to computational inefficiencies.\\n\\ue075\\ue094\\x00Lack of Semantic Understanding\\ue092 TF\\ue088IDF does not capture the semantic relationships \\nbetween words, which can be important for certain NLP tasks.\\n\\ue076\\ue094\\x00Requires Careful Handling of Rare Words\\ue092 Rare words can have high TF\\ue088IDF scores, which \\nmay not always be meaningful. Proper handling of rare words is necessary to avoid \\noverfitting.\\nHow to Check Which Word is More Important\\nImportance in a Document\\ue092 The word with the highest TF\\ue088IDF score in a document is the \\nmost important word in that document.\\nCross-Document Importance\\ue092 If a word consistently has a high TF\\ue088IDF score across \\nmultiple documents, it could be important across the entire corpus.\\nWhat is Important and What TF-IDF is Showing\\nTF\\ue088IDF Scores\\ue092 The TF\\ue088IDF score of a word in a document tells you how relevant that word \\nis to the document, considering the word's frequency in other documents.\\nHigh TF\\ue088IDF\\ue092 Indicates the word is relatively unique to the document.\\nLow TF\\ue088IDF\\ue092 Indicates the word is common across many documents in the corpus, \\nhence less significant.\"),\n",
              " Document(metadata={'source': '/content/NLPconceptsResources.pdf', 'page': 39}, page_content='Basic NLP Concept s :\\n40\\nWord2Vec: An Overview\\nKing    -    Man    +    Woman    =    Queen\\n[5,3]   -    [2,1]  +    [3, 2]   =    [6,4]\\nWord2Vec is a technique used in natural language processing \\ue081NLP\\ue082 to convert words into \\ndense vectors of numbers. These vectors (or embeddings) capture the semantic meaning of \\nwords, meaning that words with similar meanings have similar vector representations.\\nWhy Do We Need Word2Vec?'),\n",
              " Document(metadata={'source': '/content/NLPconceptsResources.pdf', 'page': 40}, page_content='Basic NLP Concept s :\\n4 1\\nTraditional methods of representing words, like one-hot encoding, suffer from several \\nlimitations:\\nNo Semantic Information\\ue092 Each word is treated as a distinct entity with no relationship to \\nother words. For example, \"king\" and \"queen\" would be entirely different, with no inherent \\nsimilarity.\\nHigh Dimensionality\\ue092 The vector size is equal to the size of the vocabulary, which can be \\nenormous, leading to high-dimensional sparse vectors.\\nLack of Generalization\\ue092 Since each word is represented independently, these \\nrepresentations don\\'t generalize well to unseen data.\\nWord2Vec addresses these issues by capturing semantic relationships between words in a \\ncontinuous vector space, where similar words are closer together.\\nImportance and Applications of Word2Vec\\nWord2Vec embeddings have numerous applications in NLP\\ue092\\nText Classification\\ue092 Improve the accuracy of models by providing meaningful word \\nrepresentations.\\nSentiment Analysis\\ue092 Capture the sentiment of words based on their context.\\nMachine Translation\\ue092 Assist in mapping words from one language to another by capturing \\ntheir meaning.\\nInformation Retrieval\\ue092 Improve search engines by understanding the context and meaning \\nof queries.\\nSimilarity Measures\\ue092 Identify words or documents that are similar in meaning.\\nHow Word2Vec Works: \\nWord2Vec uses two main model architectures to generate word embeddings:\\n\\ue072\\ue094\\x00Continuous Bag of Words \\ue081CBOW\\ue082\\n\\ue073\\ue094\\x00Skip-Gram\\nBoth architectures involve a neural network that learns word representations based on the \\ncontext in which words appear.\\nContinuous Bag of Words (CBOW) Model\\nHow CBOW Works:\\nObjective\\ue092 Given context words, predict the target word (center word).'),\n",
              " Document(metadata={'source': '/content/NLPconceptsResources.pdf', 'page': 41}, page_content='Basic NLP Concept s :\\n42\\nArchitecture:\\nInput\\ue092 Context words around a target word.\\nEmbedding Layer\\ue092 The context words are passed through an embedding layer, which \\nconverts them into dense vectors.\\nAveraging\\ue092 The vectors are averaged to create a single vector representation.\\nSoftMax Layer\\ue092 This averaged vector is passed through a SoftMax layer to predict the \\ntarget word.\\nExample:\\nConsider the sentence: \"The cat sat on the mat.\"\\nIf the context window size is 2, and the target word is \"sat\", the context words would be [\"The\", \\n\"cat\", \"on\", \"the\"].\\nThe CBOW model will take these context words as input and try to predict the word \"sat\".\\nCode Example:\\nfrom gensim.models import Word2Vec\\n# Example sentence\\nsentences = [[\"the\", \"cat\", \"sat\", \"on\", \"the\", \"mat\"],\\n             [\"the\", \"dog\", \"barked\", \"at\", \"the\", \"cat\"]]\\n# Train the CBOW model\\ncbow_model = Word2Vec(sentences, vector_size=100, window=2, min_count=\\n1, sg=0)\\n# Get the embedding for a word\\nprint(cbow_model.wv[\\'sat\\'])\\nIf sg=1, the model uses Skip-Gram.\\nIf sg=0, the model uses CBOW.'),\n",
              " Document(metadata={'source': '/content/NLPconceptsResources.pdf', 'page': 42}, page_content='Basic NLP Concept s :\\n43\\nSkip-Gram Model\\nHow Skip-Gram Works:\\nObjective\\ue092 Given a target word (center word), predict the context words around it.\\nArchitecture:\\nInput\\ue092 The target word.\\nEmbedding Layer\\ue092 The target word is passed through an embedding layer, converting it \\ninto a dense vector.\\nOutput\\ue092 The model tries to predict the surrounding context words using this vector.\\nExample:\\nUsing the same sentence, \"The cat sat on the mat\", if \"sat\" is the target word, the model will try \\nto predict the context words [\"The\", \"cat\", \"on\", \"the\"].\\nCode Example:\\npythonCopy code\\nfrom gensim.models import Word2Vec\\n# Example sentence\\nsentences = [[\"the\", \"cat\", \"sat\", \"on\", \"the\", \"mat\"],\\n             [\"the\", \"dog\", \"barked\", \"at\", \"the\", \"cat\"]]\\n# Train the Skip-Gram model\\nskipgram_model = Word2Vec(sentences, vector_size=100, window=2, min_cou\\nnt=1, sg=1)\\n# Get the embedding for a word\\nprint(skipgram_model.wv[\\'sat\\'])\\nIf sg=1, the model uses Skip-Gram.\\nIf sg=0, the model uses CBOW.\\nAdvantages of Word2Vec\\n\\ue072\\ue094\\x00Captures Semantic Relationships\\ue092 Words with similar meanings are closer in vector space.'),\n",
              " Document(metadata={'source': '/content/NLPconceptsResources.pdf', 'page': 43}, page_content='Basic NLP Concept s :\\n44\\n\\ue073\\ue094\\x00Efficient and Scalable\\ue092 Word2Vec can handle large datasets and produce meaningful \\nembeddings quickly.\\n\\ue074\\ue094\\x00Versatile\\ue092 Can be used in various NLP tasks, from text classification to machine translation.\\n\\ue075\\ue094\\x00Generalizes Well\\ue092 Can produce embeddings for words not seen during training through \\nsimilarity with known words.\\nDisadvantages of Word2Vec\\n\\ue072\\ue094\\x00Context Ignorance\\ue092 Word2Vec does not consider the order of words beyond a fixed \\nwindow size.\\n\\ue073\\ue094\\x00Fixed Embeddings\\ue092 Each word has a single embedding, regardless of its meaning in \\ndifferent contexts.\\n\\ue074\\ue094\\x00Requires Large Corpus\\ue092 To generate good-quality embeddings, a large amount of text data \\nis needed.\\n\\ue075\\ue094\\x00Computational Complexity\\ue092 Training can be computationally expensive for very large \\ndatasets.\\nWord Embedding :  (Summary Format )\\nEvolution of Word Embedding Techniques: A Sequential Overview\\n1. One-Hot Encoding (OHE)\\nWhat It Is:\\nRepresents each word as a unique binary vector where only one element is \"1\" (indicating \\nthe presence of the word), and all others are \"0\".\\nAdvantages:\\nSimple to implement and understand.\\nUnambiguous representation of words.\\nDisadvantages:'),\n",
              " Document(metadata={'source': '/content/NLPconceptsResources.pdf', 'page': 44}, page_content='Basic NLP Concept s :\\n45\\nHigh Dimensionality: The size of vectors equals the vocabulary size, leading to sparse and \\nhigh-dimensional vectors.\\nNo Semantic Information: No information about the meaning or relationships between \\nwords is captured (e.g., \"king\" and \"queen\" are as different as \"king\" and \"car\").\\nNo Context Awareness: Every occurrence of a word is treated the same, regardless of its \\ncontext.\\nWhy It Was Improved:\\nThe inability to capture semantic meaning or relationships between words led to the need \\nfor more sophisticated embeddings.\\n2. Bag of Words (BoW)\\nWhat It Is:\\nRepresents a text (, sentence, document) as a vector of word counts or binary indicators, \\nignoring word order.\\nAdvantages:\\nSimpler and faster to implement for small texts.\\nCaptures word frequency, which can be important for certain tasks.\\nDisadvantages:\\nIgnores Word Order: Completely disregards the sequence of words, losing valuable \\nsyntactic information.\\nHigh Dimensionality: Similar to OHE, leading to sparse representations.\\nNo Context Awareness: Fails to capture the meaning of words based on their context.\\nWhy It Was Improved:\\nBoW doesnʼt consider word order or context, which are essential for understanding \\nnuanced meanings in text.\\n3. Term Frequency-Inverse Document Frequency (TF-IDF)\\nWhat It Is:\\nWeights words based on their frequency in a document and their rarity across the entire \\ncorpus. The more unique a word is to a document, the higher its weight.\\nAdvantages:\\nEmphasizes Rare but Important Words: Important for distinguishing documents in tasks \\nlike document classification.'),\n",
              " Document(metadata={'source': '/content/NLPconceptsResources.pdf', 'page': 45}, page_content='Basic NLP Concept s :\\n46\\nReduces the Influence of Common Words: More effective than BoW in highlighting the \\nsignificance of less frequent terms.\\nDisadvantages:\\nNo Semantic Meaning: Still doesnʼt capture word relationships or context.\\nSparse Vectors: High-dimensional and sparse like BoW and OHE.\\nNo Word Order Information: Similar to BoW, word order is ignored.\\nWhy It Was Improved:\\nTF\\ue088IDF improves upon BoW by considering word importance but still lacks semantic \\nunderstanding and context-awareness.\\n4. Word2Vec\\nWhat It Is:\\nA predictive model that creates dense, low-dimensional vectors for words based on their \\ncontext. Two architectures are used: Continuous Bag of Words \\ue081CBOW\\ue082 and Skip-Gram.\\nAdvantages:\\nCaptures Semantic Relationships: Words with similar meanings have similar vectors (e.g., \\n\"king\" and \"queen\").\\nDense Vectors: More compact and informative than sparse representations.\\nContextual Awareness: Word2Vec captures the context of a word in a sentence, leading to \\nbetter semantic understanding.\\nDisadvantages:\\nSingle Sense per Word: It assigns one vector per word, which doesnʼt account for \\npolysemy (e.g., \"bank\" as a financial institution vs. riverbank).\\nNo Global Context: Word2Vec focuses on local context within a window size and might \\nmiss out on broader document-level information.\\nWhy It Was Improved:\\nWord2Vec was a breakthrough in capturing semantic meanings but lacked the ability to \\nconsider polysemy and broader context.\\n5. GloVe (Global Vectors for Word Representation)\\nWhat It Is:\\nCombines the strengths of both matrix factorization and predictive methods by creating \\nword vectors based on the co-occurrence matrix of words in a corpus.'),\n",
              " Document(metadata={'source': '/content/NLPconceptsResources.pdf', 'page': 46}, page_content='Basic NLP Concept s :\\n47\\nAdvantages:\\nGlobal Context: Captures global statistical information from the corpus, giving a more \\nholistic understanding of word relationships.\\nDense Vectors: Similar to Word2Vec, produces compact and meaningful vectors.\\nCaptures Semantic and Syntactic Relationships: Effectively captures relationships like \\nanalogies (e.g., \"king\" is to \"queen\" as \"man\" is to \"woman\").\\nDisadvantages:\\nRequires Large Corpus: GloVe needs a substantial amount of text data for effective \\ntraining.\\nStatic Embeddings: Like Word2Vec, GloVe generates one embedding per word, ignoring \\nmultiple meanings.\\nWhy It Was Improved:\\nDespite its advantages, GloVe couldnʼt handle polysemy or dynamically change word \\nmeanings based on context.\\n6. Contextual Embeddings (e.g., BERT, ELMo)\\nWhat It Is:\\nModels like BERT \\ue081Bidirectional Encoder Representations from Transformers) and ELMo \\n\\ue081Embeddings from Language Models) create dynamic word embeddings that change based \\non the context in which a word appears.\\nAdvantages:\\nContextual Understanding: Words are represented differently depending on their \\nsurrounding context (e.g., \"bank\" in \"river bank\" vs. \"financial bank\").\\nHandles Polysemy: Addresses the limitation of static embeddings by creating context-\\nsensitive vectors.\\nPre-trained Models: Large pre-trained models are available, which can be fine-tuned for \\nspecific tasks, saving time and resources.\\nDisadvantages:\\nComputationally Intensive: Training and using these models require significant \\ncomputational resources.\\nComplexity: These models are complex and require a deeper understanding of NLP and \\ndeep learning to implement effectively.\\nWhy It Was Improved:'),\n",
              " Document(metadata={'source': '/content/NLPconceptsResources.pdf', 'page': 47}, page_content=\"Basic NLP Concept s :\\n48\\nTo overcome the limitations of static embeddings and enhance the model's ability to \\nunderstand nuanced meanings in varied contexts.\")]"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "loaded_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "id": "AiyJUss0-p9s",
        "outputId": "04d12aa9-5aef-4269-c672-6f67585b51f6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Basic NLP Concept s :\\n1\\nBasic NLP Concepts : \\nWhy We Need Text Preprocessing:\\n\\ue072\\ue094\\x00Improve Data Quality: Cleans and removes noise from raw text data, making it more \\nconsistent.\\n\\ue073\\ue094\\x00Reduce Dimensionality: Simplifies the text, making it easier for models to process and \\nanalyze.\\n\\ue074\\ue094\\x00Enhance Model Accuracy: Helps models focus on relevant information, improving \\nprediction accuracy.\\n\\ue075\\ue094\\x00Standardize Input: Ensures uniformity in text (e.g., lowercase, base forms), reducing \\nvariability.\\nWhen to Perform Text Preprocessing:\\n\\ue072\\ue094\\x00Before Model Training: Prepares text data to be fed into machine learning models.\\n\\ue073\\ue094\\x00Before Feature Extraction: Ensures that extracted features (like word vectors) are \\nmeaningful.\\n\\ue074\\ue094\\x00In Real-Time Systems: Preprocesses text on-the-fly in applications like chatbots.\\n\\ue075\\ue094\\x00During Data Exploration: Cleans text data before analysis to gain better insights.\\nNatural Language Processing (NLP): A Comprehensive Overview\\nCommon NLP Use Cases\\n\\ue072\\ue094\\x00Language Translation\\ue092 Automatically translating text from one language to another.\\n\\ue073\\ue094\\x00Speech Recognition\\ue092 Converting spoken language into text.\\n\\ue074\\ue094\\x00Hiring and Recruitment\\ue092 Analyzing resumes and job descriptions.\\n\\ue075\\ue094\\x00Chatbots\\ue092 Facilitating automated conversations with users.\\n\\ue076\\ue094\\x00Sentiment Analysis\\ue092 Determining the sentiment (positive, negative, neutral) expressed in \\ntext.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "loaded_data[0].page_content"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(loaded_data[0].page_content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s-n3IDRRCvek",
        "outputId": "6063d3da-2c25-4a7b-a747-06a59657ed6c"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Basic NLP Concept s :\n",
            "1\n",
            "Basic NLP Concepts : \n",
            "Why We Need Text Preprocessing:\n",
            "\u0000Improve Data Quality: Cleans and removes noise from raw text data, making it more \n",
            "consistent.\n",
            "\u0000Reduce Dimensionality: Simplifies the text, making it easier for models to process and \n",
            "analyze.\n",
            "\u0000Enhance Model Accuracy: Helps models focus on relevant information, improving \n",
            "prediction accuracy.\n",
            "\u0000Standardize Input: Ensures uniformity in text (e.g., lowercase, base forms), reducing \n",
            "variability.\n",
            "When to Perform Text Preprocessing:\n",
            "\u0000Before Model Training: Prepares text data to be fed into machine learning models.\n",
            "\u0000Before Feature Extraction: Ensures that extracted features (like word vectors) are \n",
            "meaningful.\n",
            "\u0000In Real-Time Systems: Preprocesses text on-the-fly in applications like chatbots.\n",
            "\u0000During Data Exploration: Cleans text data before analysis to gain better insights.\n",
            "Natural Language Processing (NLP): A Comprehensive Overview\n",
            "Common NLP Use Cases\n",
            "\u0000Language Translation Automatically translating text from one language to another.\n",
            "\u0000Speech Recognition Converting spoken language into text.\n",
            "\u0000Hiring and Recruitment Analyzing resumes and job descriptions.\n",
            "\u0000Chatbots Facilitating automated conversations with users.\n",
            "\u0000Sentiment Analysis Determining the sentiment (positive, negative, neutral) expressed in \n",
            "text.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DijH9PXH-p9t",
        "outputId": "f3b58d24-65a5-43a2-c597-5db7311fef7d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wikipedia\n",
            "  Downloading wikipedia-1.4.0.tar.gz (27 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from wikipedia) (4.12.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wikipedia) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2024.12.14)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->wikipedia) (2.6)\n",
            "Building wheels for collected packages: wikipedia\n",
            "  Building wheel for wikipedia (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wikipedia: filename=wikipedia-1.4.0-py3-none-any.whl size=11679 sha256=50e76b2a19ee3c970b3bedd47cc590db7ffed58c7eeda39df94aa735874bcb00\n",
            "  Stored in directory: /root/.cache/pip/wheels/5e/b6/c5/93f3dec388ae76edc830cb42901bb0232504dfc0df02fc50de\n",
            "Successfully built wikipedia\n",
            "Installing collected packages: wikipedia\n",
            "Successfully installed wikipedia-1.4.0\n"
          ]
        }
      ],
      "source": [
        "%pip install wikipedia"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eE0kt1B0-p9u"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IbhjGncO-p9v"
      },
      "source": [
        "#  WIKIPEDIA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "kXUdTXm5-p9v"
      },
      "outputs": [],
      "source": [
        "from langchain_community.document_loaders import WikipediaLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "id": "2DqmQW7v-p9v",
        "outputId": "16d4c43b-9469-4d10-9901-a49af047bb94"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/wikipedia/wikipedia.py:389: GuessedAtParserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
            "\n",
            "The code that caused this warning is on line 389 of the file /usr/local/lib/python3.10/dist-packages/wikipedia/wikipedia.py. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n",
            "\n",
            "  lis = BeautifulSoup(html).find_all('li')\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "list index out of range",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-33d6baa2a42a>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWikipediaLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Tesla\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mload_max_docs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mloaded_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpage_content\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m: list index out of range"
          ]
        }
      ],
      "source": [
        "loader = WikipediaLoader(query=\"Tesla\", load_max_docs=1)\n",
        "\n",
        "loaded_data = loader.load()[0].page_content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k0K1MffD-p9w",
        "outputId": "e363932c-e7c5-4af9-d92d-c56dd5579c6a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(metadata={'source': '/content/NLPconceptsResources.pdf', 'page': 0}, page_content='Basic NLP Concept s :\\n1\\nBasic NLP Concepts : \\nWhy We Need Text Preprocessing:\\n\\ue072\\ue094\\x00Improve Data Quality: Cleans and removes noise from raw text data, making it more \\nconsistent.\\n\\ue073\\ue094\\x00Reduce Dimensionality: Simplifies the text, making it easier for models to process and \\nanalyze.\\n\\ue074\\ue094\\x00Enhance Model Accuracy: Helps models focus on relevant information, improving \\nprediction accuracy.\\n\\ue075\\ue094\\x00Standardize Input: Ensures uniformity in text (e.g., lowercase, base forms), reducing \\nvariability.\\nWhen to Perform Text Preprocessing:\\n\\ue072\\ue094\\x00Before Model Training: Prepares text data to be fed into machine learning models.\\n\\ue073\\ue094\\x00Before Feature Extraction: Ensures that extracted features (like word vectors) are \\nmeaningful.\\n\\ue074\\ue094\\x00In Real-Time Systems: Preprocesses text on-the-fly in applications like chatbots.\\n\\ue075\\ue094\\x00During Data Exploration: Cleans text data before analysis to gain better insights.\\nNatural Language Processing (NLP): A Comprehensive Overview\\nCommon NLP Use Cases\\n\\ue072\\ue094\\x00Language Translation\\ue092 Automatically translating text from one language to another.\\n\\ue073\\ue094\\x00Speech Recognition\\ue092 Converting spoken language into text.\\n\\ue074\\ue094\\x00Hiring and Recruitment\\ue092 Analyzing resumes and job descriptions.\\n\\ue075\\ue094\\x00Chatbots\\ue092 Facilitating automated conversations with users.\\n\\ue076\\ue094\\x00Sentiment Analysis\\ue092 Determining the sentiment (positive, negative, neutral) expressed in \\ntext.'),\n",
              " Document(metadata={'source': '/content/NLPconceptsResources.pdf', 'page': 1}, page_content=\"Basic NLP Concept s :\\n2\\nSearch Engine Results\\nSearch engines use NLP to suggest relevant results based on user behavior and intent. For \\nexample, Google predicts what you'll type and shows relevant outcomes like a calculator for \\nmath equations or flight status for flight numbers.\\nLanguage Translation\\nNLP powers translation tools that convert text and voice formats between languages, with \\nroots going back to early machine translation in the 1950s.\\nSemantic Search\\nNLP-driven semantic search improves customer experience by understanding the context of \\nqueries and suggesting relevant products.\"),\n",
              " Document(metadata={'source': '/content/NLPconceptsResources.pdf', 'page': 2}, page_content='Basic NLP Concept s :\\n3\\nSentiment Analysis\\nOftentimes, when businesses need help understanding their customer needs, they turn \\nto\\xa0sentiment analysis.\\nSentiment analysis (also known as opinion mining) is an NLP strategy that can determine \\nwhether the meaning behind data is positive, negative, or neutral. For instance, if an unhappy \\nclient sends an email which mentions the terms “errorˮ and “not worth the priceˮ, then their \\nopinion would be automatically tagged as one with negative sentiment.\\nAutocomplete & Autocorrect\\nAutocomplete and autocorrect use NLP to predict and correct text as you type, improving \\ntyping accuracy and speed.\\nSpellcheck\\nSpellcheck notifies users of spelling errors and corrects them automatically, a common NLP \\napplication.\\nEmail Filters'),\n",
              " Document(metadata={'source': '/content/NLPconceptsResources.pdf', 'page': 3}, page_content='Basic NLP Concept s :\\n4\\nNLP enhances email filters, categorizing emails into primary, social, or promotions folders \\nbased on content.\\nChatbots\\nNLP-powered chatbots provide quick, automated responses in customer service, helping \\nbusinesses manage inquiries efficiently.\\nSmart Assistants\\nSmart assistants like Siri and Alexa use NLP for voice recognition and respond to everyday \\nqueries, even assisting with shopping.'),\n",
              " Document(metadata={'source': '/content/NLPconceptsResources.pdf', 'page': 4}, page_content='Basic NLP Concept s :\\n5\\nSocial Media Monitoring\\nNLP helps monitor social media by filtering comments and analyzing sentiment to understand \\ncustomer reactions.\\nCustomer Service Automation\\nNLP automates customer service by responding to simple questions and routing support \\ntickets to the right agents.\\nOptical Character Recognition (OCR)\\nOCR uses NLP to convert text from scanned documents or images into machine-readable \\nformats, useful for tasks like translation.'),\n",
              " Document(metadata={'source': '/content/NLPconceptsResources.pdf', 'page': 5}, page_content='Basic NLP Concept s :\\n6\\nSpeech Recognition\\nSpeech recognition converts human speech into text, enabling voice search and other \\napplications.\\nNatural Language Generation\\nNLP generates natural-sounding text or speech from data, used in smart assistants and \\ncustomer service bots.\\nSyllabus : \\nText Cleaning\\nRemoving Punctuation\\nRemoving Special Characters\\nRemoving Numbers\\nRemoving Extra Whitespaces\\nLowercasing Text\\nTokenization'),\n",
              " Document(metadata={'source': '/content/NLPconceptsResources.pdf', 'page': 6}, page_content='Basic NLP Concept s :\\n7\\nWord Tokenization\\nSentence Tokenization\\nSubword Tokenization\\nCharacter Tokenization\\nStop Words Removal\\nIdentifying Stop Words\\nRemoving Stop Words from Tokenized Text\\nStemming and Lemmatization\\nStemming \\nLemmatization\\nText Normalization\\nConverting Text to Lowercase\\nRemoving Accents and Diacritics\\nText Segmentation \\ue081Optional)\\nSentence Segmentation\\nTopic Segmentation\\nChunking\\nN\\ue088Gram Generation \\ue081Optional)\\nUnigrams\\nBigrams\\nTrigrams\\nHigher-order N\\ue088Grams\\nVectorization \\ue081Text to Numerical Form)\\nOHE (one Hot encoding )\\nBag of Words \\ue081BoW\\ue082\\nTF\\ue088IDF \\ue081Term Frequency-Inverse Document Frequency)\\nWord Embedding\\nWord2Vec'),\n",
              " Document(metadata={'source': '/content/NLPconceptsResources.pdf', 'page': 7}, page_content='Basic NLP Concept s :\\n8\\nGloVe \\ue081Global Vectors for Word Representation)\\nWhy We Need Text Preprocessing:\\n\\ue072\\ue094\\x00Improve Data Quality: Cleans and removes noise from raw text data, making it more \\nconsistent.\\n\\ue073\\ue094\\x00Reduce Dimensionality: Simplifies the text, making it easier for models to process and \\nanalyze.\\n\\ue074\\ue094\\x00Enhance Model Accuracy: Helps models focus on relevant information, improving \\nprediction accuracy.\\n\\ue075\\ue094\\x00Standardize Input: Ensures uniformity in text (e.g., lowercase, base forms), reducing \\nvariability.\\nWhen to Perform Text Preprocessing:\\n\\ue072\\ue094\\x00Before Model Training: Prepares text data to be fed into machine learning models.\\n\\ue073\\ue094\\x00Before Feature Extraction: Ensures that extracted features (like word vectors) are \\nmeaningful.\\n\\ue074\\ue094\\x00In Real-Time Systems: Preprocesses text on-the-fly in applications like chatbots.\\n\\ue075\\ue094\\x00During Data Exploration: Cleans text data before analysis to gain better insights.\\nKey NLP Terminology\\nCorpus:\\nExplanation: A large collection of text documents used for training NLP models. Itʼs \\nessentially the dataset containing all the text that will be analyzed.\\nDocument:'),\n",
              " Document(metadata={'source': '/content/NLPconceptsResources.pdf', 'page': 8}, page_content='Basic NLP Concept s :\\n9\\nExplanation: A single piece of text within a corpus. It can be an article, a paragraph, or \\neven a sentence, depending on how the data is structured.\\nVocabulary:\\nExplanation: The set of all unique words present in the corpus. Itʼs the dictionary of \\nterms that the model will learn from.\\nToken:\\nExplanation: The smallest unit of text, usually a word or punctuation mark, that results \\nfrom the tokenization process. Tokens are the building blocks for text analysis.\\nFrequency:\\nExplanation: The number of times a particular token (word) appears in a document or \\ncorpus. Itʼs used to understand how common or rare certain words are in the text.\\nN\\ue088Grams:\\nExplanation: A sequence of \\'n\\' consecutive words or tokens in a text. For example, in \\nthe sentence \"I love NLP,\" bigrams \\ue0812-grams) would be \"I love\" and \"love NLP.\"\\nUnigram:\\nExplanation: A single word or token in a text. For example, in \"I love NLP,\" the unigrams \\nare \"I,\" \"love,\" and \"NLP.\"\\nBigram:\\nExplanation: A pair of consecutive words or tokens. For example, in \"I love NLP,\" the \\nbigrams are \"I love\" and \"love NLP.\"\\nTrigram:\\nExplanation: A sequence of three consecutive words or tokens. For example, in \"I love \\nlearning NLP,\" the trigram is \"I love learning.\"'),\n",
              " Document(metadata={'source': '/content/NLPconceptsResources.pdf', 'page': 9}, page_content=\"Basic NLP Concept s :\\n10\\n1. Punctuation Removal\\nWhy Itʼs Important:\\nPunctuation marks like commas, periods, and exclamation points often don't add much \\nvalue in text analysis, especially when we're only interested in the content words. \\nRemoving them helps in standardizing the text and reducing noise.\\nPython Code:\\nThe issue in your code arises because string.punctuation contains individual punctuation \\ncharacters (e.g., ., ,, !), and you need to check if each character in a word is a punctuation \\nmark. If your goal is to remove punctuation from each word in filtered_tokens, you need to \\niterate over each word and remove punctuation from it. Here's how you can do it:\\nimport string\\n# Example filtered tokens\\nfiltered_tokens = ['Hello!', 'This', 'is', 'a', 'test.', 'Let\\\\'s', 'rem\\nove', 'punctuation!']\\n# Remove punctuation from each word\\npunctuation_removed = [''.join(char for char in word if char not in str\\ning.punctuation) for word in filtered_tokens]\"),\n",
              " Document(metadata={'source': '/content/NLPconceptsResources.pdf', 'page': 10}, page_content='Basic NLP Concept s :\\n11\\n# Remove empty strings if any word consisted only of punctuation\\npunctuation_removed = [word for word in punctuation_removed if word]\\nprint(\"Without Punctuation:\", punctuation_removed)\\nRemoving all irrelevant characters (Numbers and Punctuation) \\nRemove numbers if they are not relevant to your analyses \\ue0810\\ue0899\\ue082. And punctuation also will be \\nremove. Punctuation is basically the set of symbols \\ue083!ˮ#$%&ʼ()*+,-./:;\\ue1de?\\ue087\\ue083\\\\]^_`\\ue085|\\ue086\\ue0a3\\ue084\\ue092\\nResult: All numeric and punctuation has been replaced with space ʼ ‘ .\\nimport re\\ndef clean_text(text):\\n    # Remove all characters except alphabets and spaces\\n    cleaned_text = re.sub(r\\'[^A-Za-z\\\\s]\\', \\'\\', text)\\n    return cleaned_text'),\n",
              " Document(metadata={'source': '/content/NLPconceptsResources.pdf', 'page': 11}, page_content='Basic NLP Concept s :\\n12\\nparagraph = \"Here\\'s a paragraph with numbers 123 and special character\\ns! Like @#$%.\"\\ncleaned_paragraph = clean_text(paragraph)\\nprint(cleaned_paragraph)\\n3. Whitespace Removal\\nWhy Itʼs Important:\\nExtra spaces and tabs can create inconsistencies in text processing. Removing them \\nensures that the text is clean and uniform, preventing potential errors in analysis.\\nPython Code:\\npythonCopy code\\ntext_with_whitespace = \"  This   is   an example     sentence with    e\\nxtra spaces.  \"\\n# Remove extra whitespace\\ncleaned_text = \" \".join(text_with_whitespace.split())\\nprint(\"Cleaned Text:\", cleaned_text)\\nOutput:\\nvbnetCopy code\\nCleaned Text: This is an example sentence with extra spaces.\\nSummary\\nStop Words Removal: Reduces data size and enhances focus on meaningful words.\\nPunctuation Removal: Eliminates noise, making the text more consistent.\\nWhitespace Removal: Ensures clean and uniform text, preventing processing errors.'),\n",
              " Document(metadata={'source': '/content/NLPconceptsResources.pdf', 'page': 12}, page_content='Basic NLP Concept s :\\n13\\n Tokenization —\\nTokenization is the process of splitting the given text into smaller pieces called tokens. Words, \\nnumbers, punctuation marks, and others can be considered as tokens. We will use Natural \\nlanguage tool kit (nltk) library for tokenization.\\n1. Tokenization\\nDefinition\\n\\ue092 The process of breaking down text into smaller units called tokens (words, phrases, or \\nsymbols).\\nWhen to Use\\n\\ue092 At the beginning of the preprocessing pipeline to facilitate further analysis.\\nAdvantages\\nSimplifies text analysis by creating manageable units.\\nEssential for subsequent steps like POS tagging and NER.\\nDisadvantages:\\nCan lead to loss of context if not done carefully ( splitting contractions).\\nMay not handle multilingual text effectively.'),\n",
              " Document(metadata={'source': '/content/NLPconceptsResources.pdf', 'page': 13}, page_content='Basic NLP Concept s :\\n14\\nResult: As we can see the string has been changed into tokens, that has \\nbeen stored in the form of ‘list of stringʼ .\\nimport nltk\\nfrom nltk.tokenize import word_tokenize\\n# Download the necessary NLTK resources (if you haven\\'t already)\\nnltk.download(\\'punkt\\')\\n# Example sentence\\nsentence = \"Natural Language Processing with NLTK is fun and exciting!\"\\n# Tokenize the sentence\\ntokens = word_tokenize(sentence)\\n# Print the tokens'),\n",
              " Document(metadata={'source': '/content/NLPconceptsResources.pdf', 'page': 14}, page_content='Basic NLP Concept s :\\n15\\nprint(tokens)\\n1. Word Tokenization\\nDefinition: Splits text into individual words.\\nExample: \"Hello, world!\" → [\"Hello\", \"world\"]\\nCode Example \\ue081Python):\\nfrom nltk.tokenize import word_tokenize\\ntext = \"Hello, world!\"\\ntokens = word_tokenize(text)\\nprint(tokens)  # Output: [\\'Hello\\', \\',\\', \\'world\\', \\'!\\']\\n2. Sentence Tokenization\\nDefinition: Splits text into sentences.\\nExample: \"Hello, world! How are you?\" → [\"Hello, world!\", \"How are you?\"]\\nCode Example \\ue081Python):\\nfrom nltk.tokenize import sent_tokenize\\ntext = \"Hello, world! How are you?\"\\nsentences = sent_tokenize(text)\\nprint(sentences)  # Output: [\\'Hello, world!\\', \\'How are you?\\']\\n3. Character Tokenization\\nDefinition: Splits text into individual characters.\\nExample: \"Hello\" → [\"H\", \"e\", \"l\", \"l\", \"o\"]'),\n",
              " Document(metadata={'source': '/content/NLPconceptsResources.pdf', 'page': 15}, page_content='Basic NLP Concept s :\\n16\\nCode Example \\ue081Python):\\npythonCopy code\\ntext = \"Hello\"\\ntokens = list(text)\\nprint(tokens)  # Output: [\\'H\\', \\'e\\', \\'l\\', \\'l\\', \\'o\\']\\n4. Subword Tokenization\\nDefinition: Splits text into subword units, useful for handling rare or unknown words.\\nExample: \"unbelievable\" → [\"un\", \"believable\"]\\nCode Example \\ue081Python with SentencePiece):\\nimport sentencepiece as spm\\nsp = spm.SentencePieceProcessor(model_file=\\'m.model\\')\\ntext = \"unbelievable\"\\ntokens = sp.encode_as_pieces(text)\\nprint(tokens)  # Output example: [\\' ▁ un\\', \\'believable\\']\\n5. Whitespace Tokenization\\nDefinition: Splits text based on whitespace.\\nExample: \"Hello world\" → [\"Hello\", \"world\"]\\nCode Example \\ue081Python):\\ntext = \"Hello world\"\\ntokens = text.split()\\nprint(tokens)  # Output: [\\'Hello\\', \\'world\\']\\nRemoving Stopwords'),\n",
              " Document(metadata={'source': '/content/NLPconceptsResources.pdf', 'page': 16}, page_content='Basic NLP Concept s :\\n17\\n“Stopwordsˮ are the most common words in a language like “theˮ, “aˮ, “meˮ, “isˮ, “toˮ, “allˮ,. \\nThese words do not carry important meaning and are usually removed from texts. It is possible \\nto remove stopwords using Natural Language Toolkit (nltk). You also may check the list of \\nstopwords by using following code.\\nStop Words Removal\\nDefinition\\n\\ue092 The process of removing common words that do not add significant meaning to the text (e.g., \\n\"the,\" \"is,\" \"and\").\\nWhen to Use\\n\\ue092 After tokenization to reduce noise in the data.\\nAdvantages\\n:\\nReduces dimensionality of the dataset.\\nEnhances the focus on meaningful words.\\nDisadvantages\\n:\\nMay remove important context in certain applications (e.g., sentiment analysis).\\nRequires customization based on the specific use case and language.'),\n",
              " Document(metadata={'source': '/content/NLPconceptsResources.pdf', 'page': 17}, page_content='Basic NLP Concept s :\\n18\\n1. Stop Words Removal\\nWhy Itʼs Important:\\nStop words like \"is,\" \"the,\" and \"and\" are common words that usually don\\'t contribute much \\nto the meaning of the text. Removing them reduces the size of the data and helps the \\nmodel focus on more significant words.\\nPython Code:\\nimport nltk\\nfrom nltk.corpus import stopwords\\nfrom nltk.tokenize import word_tokenize\\n# Sample text\\ntext = \"This is an example sentence, demonstrating the removal of stop \\nwords.\"\\n# Tokenize the text\\ntokens = word_tokenize(text)\\n# Remove stop words\\nstop_words = set(stopwords.words(\\'english\\'))\\nfiltered_tokens = [word for word in tokens if word.lower() not in stop_\\nwords]\\nprint(\"Filtered Tokens:\", filtered_tokens)\\n Stemming and Lemmatization :\\nStemming\\xa0usually refers to a crude process that chops off the ends of words in the hope of \\nachieving this goal correctly most of the time, and often includes the removal of derivational \\nunits (the obtained element is known as the stem).\\n\\xa0lemmatization\\xa0consists in doing things properly with the use of a vocabulary and \\nmorphological analysis of words, to return the base or dictionary form of a word, which is \\nknown as the lemma.'),\n",
              " Document(metadata={'source': '/content/NLPconceptsResources.pdf', 'page': 18}, page_content='Basic NLP Concept s :\\n19\\nStemming\\nDefinition\\n\\ue092 Reduces words to their root form by removing suffixes ( \"running\" to \"run\").\\nWhen to Use\\n\\ue092 When a rough approximation of word forms is sufficient.\\nAdvantages\\nSimple and computationally efficient.\\nHelps in standardizing words to a common base form.\\nDisadvantages\\n:\\nCan produce non-words ( \"universe\" to \"univers\").\\nMay lose some meaning or context.'),\n",
              " Document(metadata={'source': '/content/NLPconceptsResources.pdf', 'page': 19}, page_content='Basic NLP Concept s :\\n20\\nStemming\\nimport nltk\\nfrom nltk.stem import PorterStemmer\\n# Initialize the Porter Stemmer\\nstemmer = PorterStemmer()\\n# Example words\\nwords = [\"running\", \"runner\", \"ran\", \"easily\", \"fairly\"]\\n# Apply stemming\\nstemmed_words = [stemmer.stem(word) for word in words]\\nprint(\"Stemmed Words:\", stemmed_words)\\nLemmatization\\nDefinition:\\nReduces words to their base or dictionary form, considering the context (e.g., \"better\" to \\n\"good\").\\nWhen to Use\\n\\ue092 When accurate word forms are required for analysis.\\nAdvantages\\n:\\nProvides more meaningful and contextually appropriate results than stemming.\\nPreserves grammatical correctness.\\nDisadvantages\\n:'),\n",
              " Document(metadata={'source': '/content/NLPconceptsResources.pdf', 'page': 20}, page_content='Basic NLP Concept s :\\n21\\nMore computationally intensive than stemming.\\nRequires a comprehensive dictionary and context understanding.\\nLemmatization:\\nResult: Now here we can see it finds the root word, like ‘troublingʼ to \\n‘troubleʼ, ‘tookʼ to ‘takeʼ and ‘payedʼ to ‘payʼ . So, As opposed to \\nstemming, lemmatization does not simply chop off inflections. Instead it \\nuses lexical knowledge bases to get the correct base forms of words.\\nLemmatization\\nimport nltk\\nfrom nltk.stem import WordNetLemmatizer\\n# Download the necessary NLTK resources (if you haven\\'t already)\\nnltk.download(\\'wordnet\\')\\nnltk.download(\\'omw-1.4\\')\\n# Initialize the WordNet Lemmatizer\\nlemmatizer = WordNetLemmatizer()\\n# Example words\\nwords = [\"running\", \"runner\", \"ran\", \"easily\", \"fairly\"]'),\n",
              " Document(metadata={'source': '/content/NLPconceptsResources.pdf', 'page': 21}, page_content='Basic NLP Concept s :\\n22\\n# Apply lemmatization\\nlemmatized_words = [lemmatizer.lemmatize(word) for word in words]\\nprint(\"Lemmatized Words:\", lemmatized_words)\\nKey Differences:\\nStemming may produce non-meaningful roots (\"easily\" to \"easili\"), whereas lemmatization \\nproduces actual words.\\nLemmatization considers the context and part of speech \\ue081POS\\ue082 of a word, while stemming \\ndoes not.\\nStemming is faster but less accurate; lemmatization is slower but more accurate and \\nmeaningful.\\nText Normalization\\nDefinition\\n\\ue092 The process of converting text to a standard format ( lowercasing, removing punctuation).\\nWhen to Use\\n\\ue092 Early in the preprocessing pipeline to ensure consistency.\\nAdvantages\\nReduces variability in text data.\\nHelps in improving model performance by standardizing input.\\nDisadvantages\\nMay remove important features ( capitalization for proper nouns).\\nRequires careful consideration of what to normalize.\\nConvert all characters into lowercase —'),\n",
              " Document(metadata={'source': '/content/NLPconceptsResources.pdf', 'page': 22}, page_content='Basic NLP Concept s :\\n23\\nConvert all characters into lowercase\\nHere\\'s a paragraph you can use for converting characters into lowercase:\\nparagraph = \"This is an Example Paragraph with Mixed CASE letters.\"\\nlowercase_paragraph = paragraph.lower()\\nprint(lowercase_paragraph)\\n1. Lowercasing\\nLowercasing involves converting all characters in the text to lowercase. This helps in \\nstandardizing the text and reducing the complexity of further text processing steps.\\nExample:\\ntext = \"Natural Language Processing is an Interesting Field.\"\\n# Convert to lowercase\\nlowercased_text = text.lower()'),\n",
              " Document(metadata={'source': '/content/NLPconceptsResources.pdf', 'page': 23}, page_content='Basic NLP Concept s :\\n2 4\\nprint(\"Lowercased Text:\", lowercased_text)\\nOutput:\\nsqlCopy code\\nLowercased Text: natural language processing is an interesting field.\\n4. Text Segmentation\\nText segmentation involves breaking down a text into meaningful chunks, such as sentences or \\nphrases.\\nExample:\\npythonCopy code\\nimport nltk\\nnltk.download(\\'punkt\\')\\nfrom nltk.tokenize import sent_tokenize, word_tokenize\\n# Sample text\\ntext = \"Natural language processing is an interesting field. It has man\\ny applications in AI.\"\\n# Sentence segmentation\\nsentences = sent_tokenize(text)\\n# Word segmentation\\nwords = word_tokenize(text)\\nprint(\"Sentences:\", sentences)\\nprint(\"Words:\", words)\\nSummary\\nLowercasing standardizes text by converting it to lowercase.\\nStemming reduces words to their base form, often resulting in non-words.'),\n",
              " Document(metadata={'source': '/content/NLPconceptsResources.pdf', 'page': 24}, page_content='Basic NLP Concept s :\\n25\\nLemmatization reduces words to their dictionary form, producing actual words.\\nText Segmentation breaks down text into meaningful chunks, such as sentences and \\nwords.\\nPart-of-Speech Tagging (POS Tagging)\\nDefinition\\n\\ue092 Categorizes each word in a sentence into its grammatical function (nouns, verbs, adjectives, \\netc.).\\nWhen to Use\\n\\ue092 After tokenization to understand the grammatical structure of the text.\\nAdvantages\\nEnhances understanding of the text\\'s meaning and structure.\\nUseful for tasks like parsing and NER.\\nDisadvantage\\nRequires accurate models, which can be language-dependent.\\nMay struggle with ambiguous words based on context.\\n. Part of Speech (POS) Tagging\\nPOS tagging is the process of marking up a word in a text as corresponding to a particular part \\nof speech, based on both its definition and context.\\nimport nltk\\nfrom nltk.tokenize import word_tokenize\\n# Download necessary NLTK resources (if you haven\\'t already)\\nnltk.download(\\'punkt\\')\\nnltk.download(\\'averaged_perceptron_tagger\\')\\n# Example sentence\\nsentence = \"The quick brown fox jumps over the lazy dog.\"'),\n",
              " Document(metadata={'source': '/content/NLPconceptsResources.pdf', 'page': 25}, page_content='Basic NLP Concept s :\\n26\\n# Tokenize the sentence\\ntokens = word_tokenize(sentence)\\n# Perform POS tagging\\npos_tags = nltk.pos_tag(tokens)\\nprint(\"POS Tags:\", pos_tags)\\nOutput:\\nIn the output:\\nDT \\ue09b Determiner\\nJJ \\ue09b Adjective\\nNN \\ue09b Noun\\nVBZ \\ue09b Verb \\ue0813rd person singular present)\\nIN \\ue09b Preposition\\nNamed Entity Recognition (NER)\\nDefinition\\n\\ue092 Identifies and classifies named entities ( names, locations, organizations) in text.\\nWhen to Use\\n\\ue092 After tokenization and POS tagging to extract structured information.\\nAdvantages\\n:\\nHelps in extracting valuable information from unstructured text.\\nFacilitates tasks like information retrieval and question answering.\\nDisadvantages\\n:\\nCan be sensitive to the quality of training data.\\nMay misclassify entities in ambiguous contexts.\\n2. Named Entity Recognition (NER)'),\n",
              " Document(metadata={'source': '/content/NLPconceptsResources.pdf', 'page': 26}, page_content='Basic NLP Concept s :\\n27\\nNER is the process of identifying entities such as names of persons, organizations, locations, \\nexpressions of times, quantities, monetary values, percentages, etc., in text.\\npythonCopy code\\nimport nltk\\nfrom nltk import word_tokenize, pos_tag, ne_chunk\\n# Download necessary NLTK resources (if you haven\\'t already)\\nnltk.download(\\'maxent_ne_chunker\\')\\nnltk.download(\\'words\\')\\n# Example sentence\\nsentence = \"Apple is looking at buying U.K. startup for $1 billion.\"\\n# Tokenize the sentence\\ntokens = word_tokenize(sentence)\\n# Perform POS tagging\\npos_tags = pos_tag(tokens)\\n# Perform Named Entity Recognition\\nnamed_entities = ne_chunk(pos_tags)\\nprint(\"Named Entities:\", named_entities)\\nDetailed Explanation\\nPOS Tagging\\ue092 Each word in the sentence is tagged with its corresponding part of speech.\\nNER\\ue092 The ne_chunk function in NLTK takes POS-tagged tokens and identifies named entities, \\nwhich could be organizations, people, locations, etc.\\nDetailed Explanation:'),\n",
              " Document(metadata={'source': '/content/NLPconceptsResources.pdf', 'page': 27}, page_content='Basic NLP Concept s :\\n28\\n1. One-Hot Encoding\\nDescription:\\nOne-Hot Encoding represents each word in the text as a binary vector. In this vector, each \\ndimensabulary, with a 1 in the position of the word and 0s elsewhere.\\nWhy Use It:\\nSimple Representation: It provides a straightforward method for converting text into \\nnumerical form.\\nPreprocessing Step: Useful for creating input features for machine learning models that \\nrequire numerical input.\\nWhen to Use:\\nSmall Vocabulary: Suitable when dealing with a limited number of unique words.\\nInitial Model Building: Often used as a starting point in text classification tasks.\\nAdvantages:\\nSimplicity: Easy to understand and implement.\\nClear Mapping: Directly maps each word to a unique position in the vector space.\\nDisadvantages:\\nHigh Dimensionality: Vocabulary size directly affects vector size, leading to high-\\ndimensional vectors if the vocabulary is large.\\nSparsity: Most of the vector elements are zero, resulting in sparse representations.\\nNo Semantic Meaning: Fails to capture any meaning or relationships between words.'),\n",
              " Document(metadata={'source': '/content/NLPconceptsResources.pdf', 'page': 28}, page_content='Basic NLP Concept s :\\n29\\nout of vocabulary\\nnew data we cannot add or extra data we cannot handle \\nExample:\\nVocabulary:\\nvocabulary = [\"I\", \"love\", \"machine\", \"learning\"]\\nOne-Hot Encoding for \"machine\":\\nimport numpy as np\\ndef one_hot_encode(word, vocab):\\n    vector = np.zeros(len(vocab))\\n    vector[vocab.index(word)] = 1\\n    return vector\\nvocab = [\"I\", \"love\", \"machine\", \"learning\"]\\none_hot = one_hot_encode(\"machine\", vocab)\\nprint(one_hot)  # Output: [0. 0. 1. 0.]\\nExample: One-Hot Encoding with Pandas\\nDataset:\\nLet\\'s start with a simple dataset containing a categorical column:\\npythonCopy code\\nimport pandas as pd\\n# Sample dataset with a categorical column\\ndata = {\\'Color\\': [\\'Red\\', \\'Blue\\', \\'Green\\', \\'Red\\', \\'Green\\']}\\ndf = pd.DataFrame(data)\\nprint(\"Original DataFrame:\")\\nprint(df)\\nPerforming One-Hot Encoding:'),\n",
              " Document(metadata={'source': '/content/NLPconceptsResources.pdf', 'page': 29}, page_content='Basic NLP Concept s :\\n30\\nUsing Pandas, you can perform one-hot encoding with the pd.get_dummies() function:\\npythonCopy code\\n# Perform one-hot encoding using Pandas\\none_hot_encoded = pd.get_dummies(df, columns=[\\'Color\\'])\\nprint(\"\\\\nOne-Hot Encoded DataFrame:\")\\nprint(one_hot_encoded)\\nCode Example:\\npythonCopy code\\nfrom sklearn.preprocessing import OneHotEncoder\\nimport numpy as np\\n# Define the vocabulary\\nvocabulary = [\"apple\", \"banana\", \"orange\"]\\n# Convert the vocabulary into a format suitable for OneHotEncoder\\nvocab_array = np.array(vocabulary).reshape(-1, 1)\\n# Initialize the OneHotEncoder\\none_hot_encoder = OneHotEncoder(sparse=False)\\n# Fit and transform the vocabulary\\none_hot_encoded = one_hot_encoder.fit_transform(vocab_array)\\n# Display the One-Hot Encoded vectors\\nfor word, vec in zip(vocabulary, one_hot_encoded):\\n    print(f\"{word}: {vec}\")\\nBag-of-Words (BOW) Model: Detailed Explanation'),\n",
              " Document(metadata={'source': '/content/NLPconceptsResources.pdf', 'page': 30}, page_content='Basic NLP Concept s :\\n31\\nWhat is Bag-of-Words?\\nThe Bag-of-Words \\ue081BOW\\ue082 model is a method for representing text data in Natural Language \\nProcessing \\ue081NLP\\ue082. It transforms text into a numerical format by counting the frequency of each \\nword in a document, disregarding grammar and word order.\\nKey Characteristics:\\nUnordered: The model does not consider the sequence or structure of words in the text.\\nFrequency-Based: It captures how often each word appears in the document.\\nWhy Use Bag-of-Words?\\nSimplicity: BOW is straightforward to implement and understand, making it a good starting \\npoint for text representation.\\nEffectiveness: Despite its simplicity, BOW can perform well for various NLP tasks, such as \\ntext classification and sentiment analysis.\\nCompatibility: The BOW representation can be easily used with many machine learning \\nalgorithms.'),\n",
              " Document(metadata={'source': '/content/NLPconceptsResources.pdf', 'page': 31}, page_content='Basic NLP Concept s :\\n32\\nWhen to Use Bag-of-Words?\\nInitial Text Representation: BOW is often used as an initial method for text representation \\nbefore applying more complex models.\\nText Classification: It is suitable for tasks like spam detection, sentiment analysis, and \\ntopic classification.\\nSmall to Medium-Sized Datasets: BOW works well when the dataset is not too large, as it \\ncan become computationally expensive with larger vocabularies.\\nImportance of Bag-of-Words\\nFeature Extraction: BOW transforms text into a numerical format that can be fed into \\nmachine learning models.\\nBaseline Comparison: It serves as a baseline for evaluating the performance of more \\nsophisticated models like TF\\ue088IDF or word embeddings.\\nHow to Implement Bag-of-Words\\n\\ue072\\ue094\\x00Text Preprocessing:\\nTokenization: Split text into words.\\nNormalization: Convert text to lowercase and remove stop words.\\n\\ue073\\ue094\\x00Vocabulary Collection:\\nCreate a set of unique words from the text.\\n\\ue074\\ue094\\x00Vectorization:\\nConvert text into numerical vectors based on word frequency.\\nDisadvantages:\\n\\ue072\\ue094\\x00sparsity \\n\\ue073\\ue094\\x00ordering \\n\\ue074\\ue094\\x00out of vocabulary issue \\nExample of Bag-of-Words\\nDocuments:\\n\\ue072\\ue094\\x00Document 1\\ue092 \"I love machine learning\"\\n\\ue073\\ue094\\x00Document 2\\ue092 \"Machine learning is fun\"\\nStep 1\\ue092 Text Preprocessing'),\n",
              " Document(metadata={'source': '/content/NLPconceptsResources.pdf', 'page': 32}, page_content='Basic NLP Concept s :\\n33\\nTokenization:\\nDoc 1\\ue092 \\ue083\"I\", \"love\", \"machine\", \"learning\"]\\nDoc 2\\ue092 \\ue083\"Machine\", \"learning\", \"is\", \"fun\"]\\nNormalization:\\nDoc 1\\ue092 \\ue083\"love\", \"machine\", \"learning\"]\\nDoc 2\\ue092 \\ue083\"machine\", \"learning\", \"fun\"]\\nStep 2\\ue092 Vocabulary Collection\\nThe vocabulary from the documents is:\\npythonCopy code\\n[\"love\", \"machine\", \"learning\", \"is\", \"fun\"]\\nStep 3\\ue092 Vectorization\\nCreate a document-term matrix based on word frequency:\\nDocument love machine learning is fun\\nDocument 11 1 1 0 0\\nDocument 20 1 1 1 1\\nCode Implementation\\nHere\\'s how to implement the Bag-of-Words model using Python with the scikit-learn library:\\nfrom sklearn.feature_extraction.text import CountVectorizer\\nimport pandas as pd\\n# Sample documents\\ncorpus = [\\n    \"I love machine learning\",\\n    \"Machine learning is fun\"\\n]\\n# Create the Bag-of-Words vectorizer\\nvectorizer = CountVectorizer()\\n# Fit and transform the corpus\\nX = vectorizer.fit_transform(corpus)'),\n",
              " Document(metadata={'source': '/content/NLPconceptsResources.pdf', 'page': 33}, page_content='Basic NLP Concept s :\\n34\\n# Get feature names (words)\\nfeature_names = vectorizer.get_feature_names_out()\\n# Convert the Bag-of-Words matrix to a dense format and display it\\ndense = X.todense()\\ndenselist = dense.tolist()\\n# Display the Bag-of-Words representation\\ndf_bow = pd.DataFrame(denselist, columns=feature_names)\\nprint(df_bow)\\nTF -IDF :\\nStandard TF-IDF'),\n",
              " Document(metadata={'source': '/content/NLPconceptsResources.pdf', 'page': 34}, page_content='Basic NLP Concept s :\\n35\\nItʼs obtained by combining two terms:\\nImage credit: Author\\nwhere\\xa0Term Frequency\\xa0\\ue081TF\\ue082 is the frequency of the word\\xa0t\\xa0within the document d. In other \\nwords, itʼs the ratio between the count of the word within the document and the total number of \\nwords:\\nAs we said before, the term frequency is not enough to provide efficient measures. We also \\nneed to combine it with another term, called\\xa0Inverse Document Frequency. Itʼs a logarithmic \\ntransformation of a fraction, calculated as the total number of documents in the corpus divided \\nby the number of documents containing the word.\\nWhat is TF-IDF?\\nTF\\ue088IDF stands for Term Frequency-Inverse Document Frequency. It is a numerical statistic \\nused in information retrieval and text mining to evaluate the importance of a term (word or \\nphrase) in a document relative to a collection of documents (corpus). TF\\ue088IDF helps identify \\nterms that are significant in specific documents while discounting terms that appear frequently \\nacross many documents.\\nComponents of TF-IDF\\n\\ue072\\ue094\\x00Term Frequency \\ue081TF\\ue082:\\nDefinition\\ue092 Measures how frequently a term appears in a document.\\nCalculation:'),\n",
              " Document(metadata={'source': '/content/NLPconceptsResources.pdf', 'page': 35}, page_content='Basic NLP Concept s :\\n36\\nRaw Count\\ue092 Number of times the term appears in the document.\\nBoolean Frequency\\ue092 1 if the term appears, 0 otherwise.\\n\\ue073\\ue094\\x00Inverse Document Frequency \\ue081IDF\\ue082:\\nDefinition\\ue092 Measures how common or rare a term is across all documents in the \\ncorpus.\\n\\ue074\\ue094\\x00TF\\ue088IDF Score:\\nDefinition\\ue092 Combines TF and IDF to provide a measure of a termʼs importance in a \\ndocument relative to the entire corpus.\\nWhen to Use TF-IDF\\nDocument Classification\\ue092 To determine the importance of terms in classifying text \\ndocuments.\\nInformation Retrieval\\ue092 To rank documents based on their relevance to a search query.\\nText Mining\\ue092 To extract meaningful terms for further analysis or modeling.\\nimportance of TF-IDF\\nHighlighting Unique Words\\ue092 TF\\ue088IDF helps identify words that are significant in a specific \\ndocument but not common across all documents. This is particularly useful for information \\nretrieval and text mining.\\nFeature Representation\\ue092 It provides a way to convert text into a numerical format that can \\nbe used in machine learning algorithms.\\nExample of TF-IDF Calculation\\nCorpus\\n\\ue072\\ue094\\x00Document 1: \"I love machine learning\"\\n\\ue073\\ue094\\x00Document 2: \"Machine learning is fun\"\\n\\ue074\\ue094\\x00Document 3: \"I love programming\"\\nCalculate TF-IDF for the Term \"machine\" in Document 1\\nStep 1\\ue092 Calculate Term Frequency \\ue081TF\\ue082'),\n",
              " Document(metadata={'source': '/content/NLPconceptsResources.pdf', 'page': 36}, page_content='Basic NLP Concept s :\\n37\\nTerm Frequency \\ue081TF\\ue082\\ue092 TF for \"machine\" in Document 1 \\ue09b \\ue081Number of times \"machine\" \\nappears in Document 1\\ue082 / \\ue081Total number of words in Document 1\\ue082\\nDocument 1\\ue092 \"I love machine learning\" \\ue081Total words \\ue09b 4\\ue082\\n\"Machine\" appears 1 time in Document 1.\\nTF\\ue081\"machine\", Doc1\\ue082 \\ue09b 1 / 4 \\ue09b 0.25\\nStep 2\\ue092 Calculate Inverse Document Frequency \\ue081IDF\\ue082\\nIDF\\ue092 IDF for \"machine\" = log(Total number of documents / Number of documents \\ncontaining the term)\\n\"Machine\" appears in 2 documents out of 3.\\nIDF\\ue081\"machine\") = log(3 / 2\\ue082 \\ue0a2 0.176\\nStep 3\\ue092 Calculate TF\\ue088IDF\\nTF\\ue088IDF\\ue092 TF\\ue088IDF\\ue081\"machine\", Doc1\\ue082 \\ue09b TF * IDF \\ue09b 0.25 \\ue0a4 0.176 \\ue0a2 0.044\\nCorrected Calculation\\nTerm Frequency \\ue081TF\\ue082\\ue092\\nDocument 1: \"I love machine learning\" \\ue081Total words \\ue09b 4\\ue082\\nTF\\ue081\"machine\", Doc1\\ue082 \\ue09b 1 / 4 \\ue09b 0.25\\nInverse Document Frequency \\ue081IDF\\ue082\\ue092\\nIDF\\ue081\"machine\") = log(3 / 2\\ue082 \\ue0a2 0.176\\nTF\\ue088IDF Calculation:\\nTF\\ue088IDF\\ue081\"machine\", Doc1\\ue082 \\ue09b 0.25 \\ue0a4 0.176 \\ue0a2 0.044\\nUpdated Python Code\\nHere\\'s the corrected code implementation to reflect these calculations:\\nfrom sklearn.feature_extraction.text import TfidfVectorizer\\nimport pandas as pd\\n# Sample corpus\\ncorpus = [\\n    \"I love machine learning\",\\n    \"Machine learning is fun\",\\n    \"I love programming\"\\n]'),\n",
              " Document(metadata={'source': '/content/NLPconceptsResources.pdf', 'page': 37}, page_content='Basic NLP Concept s :\\n38\\n# Create the TF-IDF vectorizer\\nvectorizer = TfidfVectorizer()\\n# Fit and transform the corpus\\ntfidf_matrix = vectorizer.fit_transform(corpus)\\n# Get feature names (words)\\nfeature_names = vectorizer.get_feature_names_out()\\n# Convert the TF-IDF matrix to a dense format and display it\\ndense = tfidf_matrix.todense()\\ndenselist = dense.tolist()\\n# Create a DataFrame for better readability\\ndf_tfidf = pd.DataFrame(denselist, columns=feature_names)\\nprint(df_tfidf)\\nAdvantages of TF-IDF\\n\\ue072\\ue094\\x00Highlights Unique Words\\ue092 TF\\ue088IDF helps identify words that are significant in a specific \\ndocument but not common across all documents. This is particularly useful for information \\nretrieval and text mining.\\n\\ue073\\ue094\\x00Provides Feature Representation\\ue092 It converts text into a numerical format that can be used \\nin machine learning algorithms.\\n\\ue074\\ue094\\x00Effective for Text Classification\\ue092 When building models to classify documents based on \\ntheir content, TF\\ue088IDF can be used as a feature representation.\\n\\ue075\\ue094\\x00Useful for Information Retrieval\\ue092 In search engines, TF\\ue088IDF helps rank documents based \\non their relevance to a query.\\n\\ue076\\ue094\\x00Identifies Key Terms for Topic Modeling\\ue092 TF\\ue088IDF can help identify the main topics of a \\ndocument by highlighting the most significant terms.'),\n",
              " Document(metadata={'source': '/content/NLPconceptsResources.pdf', 'page': 38}, page_content=\"Basic NLP Concept s :\\n39\\nDisadvantages of TF-IDF\\n\\ue072\\ue094\\x00Ignores Word Order and Grammar\\ue092 TF\\ue088IDF represents text as a bag of words, disregarding \\nthe order of words and the grammar of the text.\\n\\ue073\\ue094\\x00Sensitive to Document Length\\ue092 TF\\ue088IDF scores can be biased towards longer documents, \\nas they tend to have more unique words.\\n\\ue074\\ue094\\x00Sparsity in High-Dimensional Spaces\\ue092 When dealing with a large vocabulary, the TF\\ue088IDF \\nmatrix becomes very sparse, which can lead to computational inefficiencies.\\n\\ue075\\ue094\\x00Lack of Semantic Understanding\\ue092 TF\\ue088IDF does not capture the semantic relationships \\nbetween words, which can be important for certain NLP tasks.\\n\\ue076\\ue094\\x00Requires Careful Handling of Rare Words\\ue092 Rare words can have high TF\\ue088IDF scores, which \\nmay not always be meaningful. Proper handling of rare words is necessary to avoid \\noverfitting.\\nHow to Check Which Word is More Important\\nImportance in a Document\\ue092 The word with the highest TF\\ue088IDF score in a document is the \\nmost important word in that document.\\nCross-Document Importance\\ue092 If a word consistently has a high TF\\ue088IDF score across \\nmultiple documents, it could be important across the entire corpus.\\nWhat is Important and What TF-IDF is Showing\\nTF\\ue088IDF Scores\\ue092 The TF\\ue088IDF score of a word in a document tells you how relevant that word \\nis to the document, considering the word's frequency in other documents.\\nHigh TF\\ue088IDF\\ue092 Indicates the word is relatively unique to the document.\\nLow TF\\ue088IDF\\ue092 Indicates the word is common across many documents in the corpus, \\nhence less significant.\"),\n",
              " Document(metadata={'source': '/content/NLPconceptsResources.pdf', 'page': 39}, page_content='Basic NLP Concept s :\\n40\\nWord2Vec: An Overview\\nKing    -    Man    +    Woman    =    Queen\\n[5,3]   -    [2,1]  +    [3, 2]   =    [6,4]\\nWord2Vec is a technique used in natural language processing \\ue081NLP\\ue082 to convert words into \\ndense vectors of numbers. These vectors (or embeddings) capture the semantic meaning of \\nwords, meaning that words with similar meanings have similar vector representations.\\nWhy Do We Need Word2Vec?'),\n",
              " Document(metadata={'source': '/content/NLPconceptsResources.pdf', 'page': 40}, page_content='Basic NLP Concept s :\\n4 1\\nTraditional methods of representing words, like one-hot encoding, suffer from several \\nlimitations:\\nNo Semantic Information\\ue092 Each word is treated as a distinct entity with no relationship to \\nother words. For example, \"king\" and \"queen\" would be entirely different, with no inherent \\nsimilarity.\\nHigh Dimensionality\\ue092 The vector size is equal to the size of the vocabulary, which can be \\nenormous, leading to high-dimensional sparse vectors.\\nLack of Generalization\\ue092 Since each word is represented independently, these \\nrepresentations don\\'t generalize well to unseen data.\\nWord2Vec addresses these issues by capturing semantic relationships between words in a \\ncontinuous vector space, where similar words are closer together.\\nImportance and Applications of Word2Vec\\nWord2Vec embeddings have numerous applications in NLP\\ue092\\nText Classification\\ue092 Improve the accuracy of models by providing meaningful word \\nrepresentations.\\nSentiment Analysis\\ue092 Capture the sentiment of words based on their context.\\nMachine Translation\\ue092 Assist in mapping words from one language to another by capturing \\ntheir meaning.\\nInformation Retrieval\\ue092 Improve search engines by understanding the context and meaning \\nof queries.\\nSimilarity Measures\\ue092 Identify words or documents that are similar in meaning.\\nHow Word2Vec Works: \\nWord2Vec uses two main model architectures to generate word embeddings:\\n\\ue072\\ue094\\x00Continuous Bag of Words \\ue081CBOW\\ue082\\n\\ue073\\ue094\\x00Skip-Gram\\nBoth architectures involve a neural network that learns word representations based on the \\ncontext in which words appear.\\nContinuous Bag of Words (CBOW) Model\\nHow CBOW Works:\\nObjective\\ue092 Given context words, predict the target word (center word).'),\n",
              " Document(metadata={'source': '/content/NLPconceptsResources.pdf', 'page': 41}, page_content='Basic NLP Concept s :\\n42\\nArchitecture:\\nInput\\ue092 Context words around a target word.\\nEmbedding Layer\\ue092 The context words are passed through an embedding layer, which \\nconverts them into dense vectors.\\nAveraging\\ue092 The vectors are averaged to create a single vector representation.\\nSoftMax Layer\\ue092 This averaged vector is passed through a SoftMax layer to predict the \\ntarget word.\\nExample:\\nConsider the sentence: \"The cat sat on the mat.\"\\nIf the context window size is 2, and the target word is \"sat\", the context words would be [\"The\", \\n\"cat\", \"on\", \"the\"].\\nThe CBOW model will take these context words as input and try to predict the word \"sat\".\\nCode Example:\\nfrom gensim.models import Word2Vec\\n# Example sentence\\nsentences = [[\"the\", \"cat\", \"sat\", \"on\", \"the\", \"mat\"],\\n             [\"the\", \"dog\", \"barked\", \"at\", \"the\", \"cat\"]]\\n# Train the CBOW model\\ncbow_model = Word2Vec(sentences, vector_size=100, window=2, min_count=\\n1, sg=0)\\n# Get the embedding for a word\\nprint(cbow_model.wv[\\'sat\\'])\\nIf sg=1, the model uses Skip-Gram.\\nIf sg=0, the model uses CBOW.'),\n",
              " Document(metadata={'source': '/content/NLPconceptsResources.pdf', 'page': 42}, page_content='Basic NLP Concept s :\\n43\\nSkip-Gram Model\\nHow Skip-Gram Works:\\nObjective\\ue092 Given a target word (center word), predict the context words around it.\\nArchitecture:\\nInput\\ue092 The target word.\\nEmbedding Layer\\ue092 The target word is passed through an embedding layer, converting it \\ninto a dense vector.\\nOutput\\ue092 The model tries to predict the surrounding context words using this vector.\\nExample:\\nUsing the same sentence, \"The cat sat on the mat\", if \"sat\" is the target word, the model will try \\nto predict the context words [\"The\", \"cat\", \"on\", \"the\"].\\nCode Example:\\npythonCopy code\\nfrom gensim.models import Word2Vec\\n# Example sentence\\nsentences = [[\"the\", \"cat\", \"sat\", \"on\", \"the\", \"mat\"],\\n             [\"the\", \"dog\", \"barked\", \"at\", \"the\", \"cat\"]]\\n# Train the Skip-Gram model\\nskipgram_model = Word2Vec(sentences, vector_size=100, window=2, min_cou\\nnt=1, sg=1)\\n# Get the embedding for a word\\nprint(skipgram_model.wv[\\'sat\\'])\\nIf sg=1, the model uses Skip-Gram.\\nIf sg=0, the model uses CBOW.\\nAdvantages of Word2Vec\\n\\ue072\\ue094\\x00Captures Semantic Relationships\\ue092 Words with similar meanings are closer in vector space.'),\n",
              " Document(metadata={'source': '/content/NLPconceptsResources.pdf', 'page': 43}, page_content='Basic NLP Concept s :\\n44\\n\\ue073\\ue094\\x00Efficient and Scalable\\ue092 Word2Vec can handle large datasets and produce meaningful \\nembeddings quickly.\\n\\ue074\\ue094\\x00Versatile\\ue092 Can be used in various NLP tasks, from text classification to machine translation.\\n\\ue075\\ue094\\x00Generalizes Well\\ue092 Can produce embeddings for words not seen during training through \\nsimilarity with known words.\\nDisadvantages of Word2Vec\\n\\ue072\\ue094\\x00Context Ignorance\\ue092 Word2Vec does not consider the order of words beyond a fixed \\nwindow size.\\n\\ue073\\ue094\\x00Fixed Embeddings\\ue092 Each word has a single embedding, regardless of its meaning in \\ndifferent contexts.\\n\\ue074\\ue094\\x00Requires Large Corpus\\ue092 To generate good-quality embeddings, a large amount of text data \\nis needed.\\n\\ue075\\ue094\\x00Computational Complexity\\ue092 Training can be computationally expensive for very large \\ndatasets.\\nWord Embedding :  (Summary Format )\\nEvolution of Word Embedding Techniques: A Sequential Overview\\n1. One-Hot Encoding (OHE)\\nWhat It Is:\\nRepresents each word as a unique binary vector where only one element is \"1\" (indicating \\nthe presence of the word), and all others are \"0\".\\nAdvantages:\\nSimple to implement and understand.\\nUnambiguous representation of words.\\nDisadvantages:'),\n",
              " Document(metadata={'source': '/content/NLPconceptsResources.pdf', 'page': 44}, page_content='Basic NLP Concept s :\\n45\\nHigh Dimensionality: The size of vectors equals the vocabulary size, leading to sparse and \\nhigh-dimensional vectors.\\nNo Semantic Information: No information about the meaning or relationships between \\nwords is captured (e.g., \"king\" and \"queen\" are as different as \"king\" and \"car\").\\nNo Context Awareness: Every occurrence of a word is treated the same, regardless of its \\ncontext.\\nWhy It Was Improved:\\nThe inability to capture semantic meaning or relationships between words led to the need \\nfor more sophisticated embeddings.\\n2. Bag of Words (BoW)\\nWhat It Is:\\nRepresents a text (, sentence, document) as a vector of word counts or binary indicators, \\nignoring word order.\\nAdvantages:\\nSimpler and faster to implement for small texts.\\nCaptures word frequency, which can be important for certain tasks.\\nDisadvantages:\\nIgnores Word Order: Completely disregards the sequence of words, losing valuable \\nsyntactic information.\\nHigh Dimensionality: Similar to OHE, leading to sparse representations.\\nNo Context Awareness: Fails to capture the meaning of words based on their context.\\nWhy It Was Improved:\\nBoW doesnʼt consider word order or context, which are essential for understanding \\nnuanced meanings in text.\\n3. Term Frequency-Inverse Document Frequency (TF-IDF)\\nWhat It Is:\\nWeights words based on their frequency in a document and their rarity across the entire \\ncorpus. The more unique a word is to a document, the higher its weight.\\nAdvantages:\\nEmphasizes Rare but Important Words: Important for distinguishing documents in tasks \\nlike document classification.'),\n",
              " Document(metadata={'source': '/content/NLPconceptsResources.pdf', 'page': 45}, page_content='Basic NLP Concept s :\\n46\\nReduces the Influence of Common Words: More effective than BoW in highlighting the \\nsignificance of less frequent terms.\\nDisadvantages:\\nNo Semantic Meaning: Still doesnʼt capture word relationships or context.\\nSparse Vectors: High-dimensional and sparse like BoW and OHE.\\nNo Word Order Information: Similar to BoW, word order is ignored.\\nWhy It Was Improved:\\nTF\\ue088IDF improves upon BoW by considering word importance but still lacks semantic \\nunderstanding and context-awareness.\\n4. Word2Vec\\nWhat It Is:\\nA predictive model that creates dense, low-dimensional vectors for words based on their \\ncontext. Two architectures are used: Continuous Bag of Words \\ue081CBOW\\ue082 and Skip-Gram.\\nAdvantages:\\nCaptures Semantic Relationships: Words with similar meanings have similar vectors (e.g., \\n\"king\" and \"queen\").\\nDense Vectors: More compact and informative than sparse representations.\\nContextual Awareness: Word2Vec captures the context of a word in a sentence, leading to \\nbetter semantic understanding.\\nDisadvantages:\\nSingle Sense per Word: It assigns one vector per word, which doesnʼt account for \\npolysemy (e.g., \"bank\" as a financial institution vs. riverbank).\\nNo Global Context: Word2Vec focuses on local context within a window size and might \\nmiss out on broader document-level information.\\nWhy It Was Improved:\\nWord2Vec was a breakthrough in capturing semantic meanings but lacked the ability to \\nconsider polysemy and broader context.\\n5. GloVe (Global Vectors for Word Representation)\\nWhat It Is:\\nCombines the strengths of both matrix factorization and predictive methods by creating \\nword vectors based on the co-occurrence matrix of words in a corpus.'),\n",
              " Document(metadata={'source': '/content/NLPconceptsResources.pdf', 'page': 46}, page_content='Basic NLP Concept s :\\n47\\nAdvantages:\\nGlobal Context: Captures global statistical information from the corpus, giving a more \\nholistic understanding of word relationships.\\nDense Vectors: Similar to Word2Vec, produces compact and meaningful vectors.\\nCaptures Semantic and Syntactic Relationships: Effectively captures relationships like \\nanalogies (e.g., \"king\" is to \"queen\" as \"man\" is to \"woman\").\\nDisadvantages:\\nRequires Large Corpus: GloVe needs a substantial amount of text data for effective \\ntraining.\\nStatic Embeddings: Like Word2Vec, GloVe generates one embedding per word, ignoring \\nmultiple meanings.\\nWhy It Was Improved:\\nDespite its advantages, GloVe couldnʼt handle polysemy or dynamically change word \\nmeanings based on context.\\n6. Contextual Embeddings (e.g., BERT, ELMo)\\nWhat It Is:\\nModels like BERT \\ue081Bidirectional Encoder Representations from Transformers) and ELMo \\n\\ue081Embeddings from Language Models) create dynamic word embeddings that change based \\non the context in which a word appears.\\nAdvantages:\\nContextual Understanding: Words are represented differently depending on their \\nsurrounding context (e.g., \"bank\" in \"river bank\" vs. \"financial bank\").\\nHandles Polysemy: Addresses the limitation of static embeddings by creating context-\\nsensitive vectors.\\nPre-trained Models: Large pre-trained models are available, which can be fine-tuned for \\nspecific tasks, saving time and resources.\\nDisadvantages:\\nComputationally Intensive: Training and using these models require significant \\ncomputational resources.\\nComplexity: These models are complex and require a deeper understanding of NLP and \\ndeep learning to implement effectively.\\nWhy It Was Improved:'),\n",
              " Document(metadata={'source': '/content/NLPconceptsResources.pdf', 'page': 47}, page_content=\"Basic NLP Concept s :\\n48\\nTo overcome the limitations of static embeddings and enhance the model's ability to \\nunderstand nuanced meanings in varied contexts.\")]"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "loaded_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P6IVFSMk-p9z"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PDFs from Direcrory"
      ],
      "metadata": {
        "id": "1Jhnmdu2C8IP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import PyPDFDirectoryLoader\n",
        "\n",
        "# Use the PyPDFDirectoryLoader to load and parse the PDFs from a directory\n",
        "loader = PyPDFDirectoryLoader(\"/content/pdf_files/\")\n",
        "docs = loader.load_and_split()\n",
        "\n",
        "print(len(docs))\n",
        "print(docs[0])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dnbac2y9DDil",
        "outputId": "2a2b9001-5edd-4989-c203-27cc5ef98c9c"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5\n",
            "page_content='ExpoHub\n",
            "123456789\n",
            " expohub street\n",
            "ABC\n",
            "123456\n",
            "India\n",
            "Billed To\n",
            "Rohan Verma.\n",
            ".\n",
            "Invoice Number\n",
            "873103.\n",
            "Amount Due (INR)\n",
            "Rs.79,250.00\n",
            "Date of Issue\n",
            "22/04/2020\n",
            "Due Date\n",
            "22/05/2020\n",
            "Description Rate Qty Line Total\n",
            "Shared Hosting Rs.78,000.00 1 Rs.78,000.00\n",
            "LateFee Rs.1,250.00 1 Rs.1,250.00\n",
            "Rs.0.00 1 Rs.0.00\n",
            "Subtotal\n",
            "Tax\n",
            "79,250.00\n",
            "0.00\n",
            "Total\n",
            "Amount Paid\n",
            "79,250.00\n",
            "0.00\n",
            "Amount Due (INR) Rs.79,250.00' metadata={'source': '/content/pdf_files/Invoice 873103.pdf', 'page': 0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(docs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V3Ra6h3DD0qv",
        "outputId": "fb306374-f6bb-4cd7-c300-573b72b32eb7"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Document(metadata={'source': '/content/pdf_files/Invoice 873103.pdf', 'page': 0}, page_content='ExpoHub\\n123456789\\n expohub street\\nABC\\n123456\\nIndia\\nBilled To\\nRohan Verma.\\n.\\nInvoice Number\\n873103.\\nAmount Due (INR)\\nRs.79,250.00\\nDate of Issue\\n22/04/2020\\nDue Date\\n22/05/2020\\nDescription Rate Qty Line Total\\nShared Hosting Rs.78,000.00 1 Rs.78,000.00\\nLateFee Rs.1,250.00 1 Rs.1,250.00\\nRs.0.00 1 Rs.0.00\\nSubtotal\\nTax\\n79,250.00\\n0.00\\nTotal\\nAmount Paid\\n79,250.00\\n0.00\\nAmount Due (INR) Rs.79,250.00'), Document(metadata={'source': '/content/pdf_files/Invoice 873104.pdf', 'page': 0}, page_content='ExpoHub\\n123456789\\n expohub street\\nABC\\n123456\\nIndia\\nBilled To\\nJoydeep Sahoo.\\n.\\nInvoice Number\\n873104.\\nAmount Due (INR)\\nRs.48,780.00\\nDate of Issue\\n22/04/2020\\nDue Date\\n22/05/2020\\nDescription Rate Qty Line Total\\nShared Hosting Rs.48,000.00 1 Rs.48,000.00\\nLateFee Rs.780.00 1 Rs.780.00\\nRs.0.00 1 Rs.0.00\\nSubtotal\\nTax\\n48,780.00\\n0.00\\nTotal\\nAmount Paid\\n48,780.00\\n0.00\\nAmount Due (INR) Rs.48,780.00'), Document(metadata={'source': '/content/pdf_files/Invoice 873102.pdf', 'page': 0}, page_content='ExpoHub\\n123456789\\n expohub street\\nABC\\n123456\\nIndia\\nBilled To\\nRahul Jignesh.\\nInvoice Number\\n873100.\\nAmount Due (INR)\\nRs.89,600.00\\nDate of Issue\\n22/04/2020\\nDue Date\\n22/05/2020\\nDescription Rate Qty Line Total\\nShared Hosting Rs.88,900.00 1 Rs.88,900.00\\nLateFee Rs.700.00 1 Rs.700.00\\nRs.0.00 1 Rs.0.00\\nSubtotal\\nTax\\n89,600.00\\n0.00\\nTotal\\nAmount Paid\\n89,600.00\\n0.00\\nAmount Due (INR) Rs.89,600.00'), Document(metadata={'source': '/content/pdf_files/Invoice 873105.pdf', 'page': 0}, page_content='ExpoHub\\n123456789\\n expohub street\\nABC\\n123456\\nIndia\\nBilled To\\nRam Godwale.\\n.\\nInvoice Number\\n873102.\\nAmount Due (INR)\\nRs.69,524.00\\nDate of Issue\\n22/04/2020\\nDue Date\\n22/05/2020\\nDescription Rate Qty Line Total\\nShared Hosting Rs.68,544.00 1 Rs.68,544.00\\nLateFee Rs.980.00 1 Rs.980.00\\nRs.0.00 1 Rs.0.00\\nSubtotal\\nTax\\n69,524.00\\n0.00\\nTotal\\nAmount Paid\\n69,524.00\\n0.00\\nAmount Due (INR) Rs.69,524.00'), Document(metadata={'source': '/content/pdf_files/Invoice 873101.pdf', 'page': 0}, page_content='ExpoHub\\n123456789\\n expohub street\\nABC\\n123456\\nIndia\\nBilled To\\nNagesh Reddy.\\nInvoice Number\\n873101.\\nAmount Due (INR)\\nRs.26,178.00\\nDate of Issue\\n22/04/2020\\nDue Date\\n22/05/2020\\nDescription Rate Qty Line Total\\nShared Hosting Rs.25,878.00 1 Rs.25,878.00\\nLateFee Rs.300.00 1 Rs.300.00\\nRs.0.00 1 Rs.0.00\\nSubtotal\\nTax\\n26,178.00\\n0.00\\nTotal\\nAmount Paid\\n26,178.00\\n0.00\\nAmount Due (INR) Rs.26,178.00')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "6VfjWp7kLy6-"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "we3_63FuMUf_"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}