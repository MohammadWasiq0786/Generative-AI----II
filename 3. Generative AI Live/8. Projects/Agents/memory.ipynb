{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())\n",
    "openai_api_key = os.environ[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "AI Message:  As an AI, I don't have access to personal data about individuals unless it has been shared with me in the course of our conversation.\n",
      "\n",
      "AI Message:  AI, or Artificial Intelligence, refers to the simulation of human intelligence processes by machines, especially computer systems, including learning, reasoning, problem-solving, perception, and language understanding.\n",
      "\n",
      "AI Message:  Nice to meet you, Prince Kstiayr! How can I assist you today?\n",
      "\n",
      "Message 1 - SYSTEM:  You are a helpful AI Assistant. Answer the User's queries succinctly in one sentence.\n",
      "\n",
      "Message 2 - HUMAN:  what is my name\n",
      "\n",
      "Message 3 - AI:  As an AI, I don't have access to personal data about individuals unless it has been shared with me in the course of our conversation.\n",
      "\n",
      "Message 4 - HUMAN:  what is ai \n",
      "\n",
      "Message 5 - AI:  AI, or Artificial Intelligence, refers to the simulation of human intelligence processes by machines, especially computer systems, including learning, reasoning, problem-solving, perception, and language understanding.\n",
      "\n",
      "Message 6 - HUMAN:  my name s prince kstiayr\n",
      "\n",
      "Message 7 - AI:  Nice to meet you, Prince Kstiayr! How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.7, model=\"gpt-4\")\n",
    "\n",
    "\n",
    "system_prompt = SystemMessage(\"You are a helpful AI Assistant. Answer the User's queries succinctly in one sentence.\")\n",
    "\n",
    "messages = [system_prompt]\n",
    "\n",
    "while True:\n",
    "\n",
    "    user_message = HumanMessage(input(\"\\nUser: \"))\n",
    "    \n",
    "    if user_message.content.lower() == \"exit\":\n",
    "        break\n",
    "\n",
    "    else:\n",
    "\n",
    "        messages.append(user_message)\n",
    "\n",
    "\n",
    "    response = llm.invoke(messages)\n",
    "    \n",
    "    print(\"\\nAI Message: \", response.content)\n",
    "\n",
    "\n",
    "    messages.append(response)\n",
    "\n",
    "\n",
    "for i in range(len(messages)):\n",
    "    print(f\"\\nMessage {i+1} - {messages[i].type.upper()}: \", messages[i].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# episodic "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "AI Message:  AI, or Artificial Intelligence, is a branch of computer science that aims to create systems capable of performing tasks that normally require human intelligence such as understanding natural language, recognizing patterns, solving problems, and learning.\n",
      "\n",
      "AI Message:  Prince was an iconic American musician known for his eclectic work, flamboyant stage presence, extravagant fashion sense, and wide vocal range.\n",
      "\n",
      "AI Message:  I'm sorry, but I couldn't find specific information on \"Prince Katiyar\".\n",
      "\n",
      "AI Message:  Python is a high-level, interpreted, and general-purpose programming language that emphasizes readability with its notable use of significant whitespace.\n",
      "\n",
      "Full Conversation History (with Episodic Memory):\n",
      "\n",
      "Message 1 - SYSTEM:  You are a helpful AI Assistant. Answer the User's queries succinctly in one sentence.\n",
      "\n",
      "Message 2 - HUMAN:  what si ai\n",
      "\n",
      "Message 3 - AI:  AI, or Artificial Intelligence, is a branch of computer science that aims to create systems capable of performing tasks that normally require human intelligence such as understanding natural language, recognizing patterns, solving problems, and learning.\n",
      "\n",
      "Message 4 - HUMAN:  who is prince\n",
      "\n",
      "Message 5 - AI:  Prince was an iconic American musician known for his eclectic work, flamboyant stage presence, extravagant fashion sense, and wide vocal range.\n",
      "\n",
      "Message 6 - HUMAN:  who is prince katiyar\n",
      "\n",
      "Message 7 - AI:  I'm sorry, but I couldn't find specific information on \"Prince Katiyar\".\n",
      "\n",
      "Message 8 - HUMAN:  what is python\n",
      "\n",
      "Message 9 - AI:  Python is a high-level, interpreted, and general-purpose programming language that emphasizes readability with its notable use of significant whitespace.\n",
      "\n",
      "Episodic Memory After All Conversations:\n",
      "[\n",
      "  {\n",
      "    \"conversation\": {\n",
      "      \"user_message\": \"what si ai\",\n",
      "      \"ai_response\": \"AI, or Artificial Intelligence, is a branch of computer science that aims to create systems capable of performing tasks that normally require human intelligence such as understanding natural language, recognizing patterns, solving problems, and learning.\"\n",
      "    },\n",
      "    \"context_tags\": [\n",
      "      \"general_conversation\"\n",
      "    ],\n",
      "    \"conversation_summary\": \"User asked: what si ai, AI responded: AI, or Artificial Intelligence, is a branch of computer science that aims to create systems capable of performing tasks that normally require human intelligence such as understanding natural language, recognizing patterns, solving problems, and learning.\",\n",
      "    \"what_worked\": \"AI answered the user's query clearly and concisely.\",\n",
      "    \"what_to_avoid\": \"AI should avoid repeating the same responses.\"\n",
      "  },\n",
      "  {\n",
      "    \"conversation\": {\n",
      "      \"user_message\": \"who is prince\",\n",
      "      \"ai_response\": \"Prince was an iconic American musician known for his eclectic work, flamboyant stage presence, extravagant fashion sense, and wide vocal range.\"\n",
      "    },\n",
      "    \"context_tags\": [\n",
      "      \"general_conversation\"\n",
      "    ],\n",
      "    \"conversation_summary\": \"User asked: who is prince, AI responded: Prince was an iconic American musician known for his eclectic work, flamboyant stage presence, extravagant fashion sense, and wide vocal range.\",\n",
      "    \"what_worked\": \"AI answered the user's query clearly and concisely.\",\n",
      "    \"what_to_avoid\": \"AI should avoid repeating the same responses.\"\n",
      "  },\n",
      "  {\n",
      "    \"conversation\": {\n",
      "      \"user_message\": \"who is prince katiyar\",\n",
      "      \"ai_response\": \"I'm sorry, but I couldn't find specific information on \\\"Prince Katiyar\\\".\"\n",
      "    },\n",
      "    \"context_tags\": [\n",
      "      \"general_conversation\"\n",
      "    ],\n",
      "    \"conversation_summary\": \"User asked: who is prince katiyar, AI responded: I'm sorry, but I couldn't find specific information on \\\"Prince Katiyar\\\".\",\n",
      "    \"what_worked\": \"AI answered the user's query clearly and concisely.\",\n",
      "    \"what_to_avoid\": \"AI should avoid repeating the same responses.\"\n",
      "  },\n",
      "  {\n",
      "    \"conversation\": {\n",
      "      \"user_message\": \"what is python\",\n",
      "      \"ai_response\": \"Python is a high-level, interpreted, and general-purpose programming language that emphasizes readability with its notable use of significant whitespace.\"\n",
      "    },\n",
      "    \"context_tags\": [\n",
      "      \"general_conversation\"\n",
      "    ],\n",
      "    \"conversation_summary\": \"User asked: what is python, AI responded: Python is a high-level, interpreted, and general-purpose programming language that emphasizes readability with its notable use of significant whitespace.\",\n",
      "    \"what_worked\": \"AI answered the user's query clearly and concisely.\",\n",
      "    \"what_to_avoid\": \"AI should avoid repeating the same responses.\"\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "import json\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.7, model=\"gpt-4\")\n",
    "\n",
    "system_prompt = SystemMessage(content=\"You are a helpful AI Assistant. Answer the User's queries succinctly in one sentence.\")\n",
    "\n",
    "messages = [system_prompt]\n",
    "episodic_memory = []\n",
    "\n",
    "def store_conversation(user_message, ai_response):\n",
    "    \"\"\"Store the conversation in episodic memory with reflection.\"\"\"\n",
    "    \n",
    "    context_tags = [\"general_conversation\"]\n",
    "    summary = f\"User asked: {user_message.content}, AI responded: {ai_response.content}\"\n",
    "    what_worked = \"AI answered the user's query clearly and concisely.\"\n",
    "    what_to_avoid = \"AI should avoid repeating the same responses.\"\n",
    "    \n",
    "    memory = {\n",
    "        \"conversation\": {\n",
    "            \"user_message\": user_message.content,\n",
    "            \"ai_response\": ai_response.content\n",
    "        },\n",
    "        \"context_tags\": context_tags,\n",
    "        \"conversation_summary\": summary,\n",
    "        \"what_worked\": what_worked,\n",
    "        \"what_to_avoid\": what_to_avoid\n",
    "    }\n",
    "    \n",
    "    episodic_memory.append(memory)\n",
    "\n",
    "def get_episodic_memory():\n",
    "    \"\"\"Retrieve the last 3 interactions from episodic memory for context.\"\"\"\n",
    "    context = \"\"\n",
    "    for memory in reversed(episodic_memory[-3:]):  # Retrieve the last 3 conversations\n",
    "        context += f\"User: {memory['conversation']['user_message']}\\nAI: {memory['conversation']['ai_response']}\\n\"\n",
    "    return context\n",
    "\n",
    "while True:\n",
    "    user_message = HumanMessage(content=input(\"\\nUser: \").strip())\n",
    "    \n",
    "    if user_message.content.lower() == \"exit\":\n",
    "        break\n",
    "    \n",
    "\n",
    "    messages.append(user_message)\n",
    "\n",
    " \n",
    "    memory_context = get_episodic_memory()\n",
    "    \n",
    "\n",
    "    messages_with_memory = messages + [HumanMessage(content=memory_context)]\n",
    "\n",
    "\n",
    "    response = llm.invoke(messages_with_memory)\n",
    "\n",
    "    print(\"\\nAI Message: \", response.content)\n",
    "\n",
    "\n",
    "    store_conversation(user_message, response)\n",
    "\n",
    "    messages.append(response)\n",
    "\n",
    "\n",
    "print(\"\\nFull Conversation History (with Episodic Memory):\")\n",
    "for i, message in enumerate(messages):\n",
    "    print(f\"\\nMessage {i+1} - {message.type.upper()}: \", message.content)\n",
    "\n",
    "\n",
    "print(\"\\nEpisodic Memory After All Conversations:\")\n",
    "print(json.dumps(episodic_memory, indent=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# semantic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "AI Message:  As an AI assistant, I'm designed to provide information on a wide range of topics. Could you please specify your question or topic? I'll do my best to provide a helpful answer.\n",
      "\n",
      "AI Message:  Yes, that's correct. Python is a high-level programming language that's interpreted, meaning it doesn't need to be compiled before it's run. Its design philosophy emphasizes code readability and its syntax allows programmers to express concepts in fewer lines of code than might be possible in languages such as C++ or Java. Python supports multiple programming paradigms, including procedural, object-oriented, and functional programming.\n",
      "\n",
      "Dynamic typing means that the type is checked at runtime, not in advance, and type information can be changed. This feature makes Python very flexible in assigning data types; it's why you can assign any kind of data to any variable.\n",
      "\n",
      "Garbage collection refers to automatic memory management. Python's garbage collector runs during program execution and is triggered when an object's reference count drops to zero. An object's reference count changes as the number of aliases that point to it changes.\n",
      "\n",
      "Knowledge Base After All Conversations:\n",
      "{\n",
      "  \"python\": \"Python is a high-level, interpreted programming language with dynamic typing and garbage collection.\",\n",
      "  \"machine_learning\": \"Machine learning is a branch of artificial intelligence that focuses on building systems that learn from data.\",\n",
      "  \"deep_learning\": \"Deep learning is a subset of machine learning that uses neural networks to model complex patterns in data.\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "import json\n",
    "\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.7, model=\"gpt-4\")\n",
    "\n",
    "\n",
    "system_prompt = SystemMessage(\"You are a helpful AI Assistant. Answer the User's queries with facts and relevant information from your knowledge base.\")\n",
    "\n",
    "\n",
    "knowledge_base = {\n",
    "    \"python\": \"Python is a high-level, interpreted programming language with dynamic typing and garbage collection.\",\n",
    "    \"machine_learning\": \"Machine learning is a branch of artificial intelligence that focuses on building systems that learn from data.\",\n",
    "    \"deep_learning\": \"Deep learning is a subset of machine learning that uses neural networks to model complex patterns in data.\"\n",
    "}\n",
    "\n",
    "def store_knowledge(concept, fact):\n",
    "    \"\"\" Store a concept and its associated fact in semantic memory \"\"\"\n",
    "    knowledge_base[concept.lower()] = fact\n",
    "\n",
    "def retrieve_knowledge(query):\n",
    "    \"\"\" Retrieve relevant knowledge from the semantic memory based on the user's query \"\"\"\n",
    "    query_lower = query.lower()\n",
    "    relevant_knowledge = \"\"\n",
    "    \n",
    " \n",
    "    for concept, fact in knowledge_base.items():\n",
    "        if concept in query_lower:\n",
    "            relevant_knowledge += f\"{concept.capitalize()}: {fact}\\n\"\n",
    "    \n",
    "    return relevant_knowledge if relevant_knowledge else \"Sorry, I don't have information on that topic.\"\n",
    "\n",
    "while True:\n",
    "\n",
    "    user_message = HumanMessage(input(\"\\nUser: \"))\n",
    "    \n",
    "    if user_message.content.lower() == \"exit\":\n",
    "        break\n",
    "\n",
    "    else:\n",
    "\n",
    "        knowledge_context = retrieve_knowledge(user_message.content)\n",
    "    \n",
    "        response = llm.invoke([system_prompt, HumanMessage(knowledge_context)])\n",
    "        \n",
    "        print(\"\\nAI Message: \", response.content)\n",
    "\n",
    "\n",
    "    if \"learn\" in user_message.content.lower():\n",
    "     \n",
    "        parts = user_message.content.split(\"is\")\n",
    "        if len(parts) == 2:\n",
    "            store_knowledge(parts[0].strip(), parts[1].strip())\n",
    "            print(\"\\nAI Message: I've learned something new!\")\n",
    "\n",
    "print(\"\\nKnowledge Base After All Conversations:\")\n",
    "print(json.dumps(knowledge_base, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# procedural "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myagents",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
